{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries ",
   "id": "7870155a64a40826"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:13:16.251839Z",
     "start_time": "2025-03-28T11:13:08.168371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Sampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm  \n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import ConcatDataset\n",
    "import random\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ],
   "id": "972f00a4cfa3d4db",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating dataset for multiple parts",
   "id": "ba51d4193a72afb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:13:34.091099Z",
     "start_time": "2025-03-28T11:13:29.707146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define file paths as constants\n",
    "CSV_FILE_PATH = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'\n",
    "ROOT_DIR_PATH = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset'  # Common parent directory\n",
    "root_dir = ROOT_DIR_PATH\n",
    "\n",
    "# Load data into a DataFrame for easier processing\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Filter the dataset to include images containing \"print24\", \"print131\", or \"print0\"\n",
    "pattern = 'print24|print131|print0|print46|print82|print111|print132|print122|print37'\n",
    "data_filtered = data[data.iloc[:, 0].str.contains(pattern, na=False)]\n",
    "\n",
    "# Update the first column to include both the print folder and the image filename.\n",
    "# The regex now captures the folder name (print24, print131, or print0) and the image filename.\n",
    "data_filtered.iloc[:, 0] = data_filtered.iloc[:, 0].str.replace(\n",
    "    r'.*?/(print24|print131|print0|print46|print82|print111|print132|print122|print37)/(image-\\d+\\.jpg)', \n",
    "    r'\\1/\\2', \n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"First rows of filtered DataFrame:\")\n",
    "print(data_filtered.head())\n",
    "\n",
    "print(\"\\nLast rows of filtered DataFrame:\")\n",
    "print(data_filtered.tail())"
   ],
   "id": "adb488900c4619ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of filtered DataFrame:\n",
      "              img_path               timestamp  flow_rate  feed_rate  \\\n",
      "0   print0/image-6.jpg  2020-10-08T13:12:50-34        100        100   \n",
      "1   print0/image-7.jpg  2020-10-08T13:12:50-80        100        100   \n",
      "2   print0/image-8.jpg  2020-10-08T13:12:51-27        100        100   \n",
      "3   print0/image-9.jpg  2020-10-08T13:12:51-74        100        100   \n",
      "4  print0/image-10.jpg  2020-10-08T13:12:52-20        100        100   \n",
      "\n",
      "   z_offset  target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  \\\n",
      "0       0.0          205.0  204.13  65.74           531           554   \n",
      "1       0.0          205.0  204.13  65.74           531           554   \n",
      "2       0.0          205.0  204.24  65.84           531           554   \n",
      "3       0.0          205.0  204.24  65.84           531           554   \n",
      "4       0.0          205.0  204.24  65.84           531           554   \n",
      "\n",
      "   img_num  print_id  flow_rate_class  feed_rate_class  z_offset_class  \\\n",
      "0        5         0                1                1               1   \n",
      "1        6         0                1                1               1   \n",
      "2        7         0                1                1               1   \n",
      "3        8         0                1                1               1   \n",
      "4        9         0                1                1               1   \n",
      "\n",
      "   hotend_class   img_mean    img_std  \n",
      "0             1  18.687230  13.809311  \n",
      "1             1  27.321104  22.875292  \n",
      "2             1  23.138174  17.933411  \n",
      "3             1  21.014212  17.120604  \n",
      "4             1  27.481729  15.091996  \n",
      "\n",
      "Last rows of filtered DataFrame:\n",
      "                        img_path               timestamp  flow_rate  \\\n",
      "707911  print132/image-28711.jpg  2020-10-06T13:00:16-61         53   \n",
      "707912  print132/image-28712.jpg  2020-10-06T13:00:17-07         53   \n",
      "707913  print132/image-28713.jpg  2020-10-06T13:00:17-54         53   \n",
      "707914  print132/image-28714.jpg  2020-10-06T13:00:18-00         53   \n",
      "707915  print132/image-28715.jpg  2020-10-06T13:00:18-46         53   \n",
      "\n",
      "        feed_rate  z_offset  target_hotend  hotend    bed  nozzle_tip_x  \\\n",
      "707911        199       0.0          209.0  216.41  65.17           580   \n",
      "707912        199       0.0          209.0  212.46  65.01           580   \n",
      "707913        199       0.0          209.0  212.46  65.01           580   \n",
      "707914        199       0.0          209.0  212.46  65.01           580   \n",
      "707915        199       0.0          209.0  212.46  65.01           580   \n",
      "\n",
      "        nozzle_tip_y  img_num  print_id  flow_rate_class  feed_rate_class  \\\n",
      "707911           501    28710       132                0                2   \n",
      "707912           501    28711       132                0                2   \n",
      "707913           501    28712       132                0                2   \n",
      "707914           501    28713       132                0                2   \n",
      "707915           501    28714       132                0                2   \n",
      "\n",
      "        z_offset_class  hotend_class   img_mean    img_std  \n",
      "707911               1             1  42.445140  43.999882  \n",
      "707912               1             1  45.420326  44.246413  \n",
      "707913               1             1  50.186357  48.417185  \n",
      "707914               1             1  51.183958  49.750333  \n",
      "707915               1             1  49.758112  46.374623  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysing the target hotend temperature column",
   "id": "b53bc861f34b792e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:02.225894Z",
     "start_time": "2025-03-28T11:14:02.068075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())\n",
    "\n",
    "if len(unique_temperatures) >= 60:\n",
    "    temperature_min = unique_temperatures[0]\n",
    "    temperature_max = unique_temperatures[-1]\n",
    "    remaining_temperatures = [temp for temp in unique_temperatures if temp not in [temperature_min, temperature_max]]\n",
    "    random_temperatures = random.sample(remaining_temperatures, 51)\n",
    "    temperature_sublist = sorted([temperature_min, temperature_max] + random_temperatures)\n",
    "    \n",
    "    # Split the temperature sublist into three groups (roughly equal thirds)\n",
    "    split_size = len(temperature_sublist) // 3\n",
    "    experience_1 = temperature_sublist[:split_size]\n",
    "    experience_2 = temperature_sublist[split_size:2*split_size]\n",
    "    experience_3 = temperature_sublist[2*split_size:]\n",
    "    \n",
    "    print(\"Temperature sublist:\", temperature_sublist)\n",
    "    print(\"\\nExperience Group 1:\", experience_1)\n",
    "    print(\"Experience Group 2:\", experience_2)\n",
    "    print(\"Experience Group 3:\", experience_3)\n",
    "else:\n",
    "    print(\"Not enough unique temperatures to select from.\")\n",
    "    experience_1 = experience_2 = experience_3 = []\n",
    "\n",
    "# Create a dictionary to store datasets for each experience (non-cumulative)\n",
    "experience_datasets = {}\n",
    "\n",
    "# Assign each experience a specific class: \n",
    "# Experience 1 -> class 0, Experience 2 -> class 1, Experience 3 -> class 2\n",
    "for exp_id, (experience_temps, target_class) in enumerate(zip([experience_1, experience_2, experience_3], [0, 1, 2]), start=1):\n",
    "    if not experience_temps:\n",
    "        print(f\"Skipping Experience {exp_id} due to insufficient temperatures.\")\n",
    "        continue\n",
    "    print(f\"\\nProcessing Experience {exp_id} for class {target_class} with temperatures: {experience_temps}...\")\n",
    "    \n",
    "    # Print initial class distribution for the experience's temperatures (before filtering by target_class)\n",
    "    exp_data_all = data_filtered[data_filtered['target_hotend'].isin(experience_temps)]\n",
    "    print(\"Initial class distribution in Experience\", exp_id, \":\")\n",
    "    for class_id in [0, 1, 2]:\n",
    "        class_count = len(exp_data_all[exp_data_all['hotend_class'] == class_id])\n",
    "        print(f\"Class {class_id}: {class_count}\")\n",
    "    \n",
    "    # Filter data for the current experience's temperatures and the target class\n",
    "    exp_data = exp_data_all[exp_data_all['hotend_class'] == target_class]\n",
    "    \n",
    "    if exp_data.empty:\n",
    "        print(f\"No data found for Experience {exp_id} with class {target_class}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Ensure each experience dataset has exactly x images.\n",
    "    # Enforce a fixed size for each experience:\n",
    "    desired_size = 20000\n",
    "    if len(exp_data) >= desired_size:\n",
    "        exp_data = exp_data.sample(n=desired_size, random_state=42)\n",
    "    else:\n",
    "        exp_data = exp_data.sample(n=desired_size, replace=True, random_state=42)\n",
    "    experience_datasets[exp_id] = exp_data\n",
    "    print(f\"Dataset size for Experience {exp_id} (class {target_class}): {len(exp_data)}\")"
   ],
   "id": "b8f64236c39dd314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature sublist: [180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 188.0, 189.0, 192.0, 193.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 217.0, 218.0, 219.0, 220.0, 221.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0]\n",
      "\n",
      "Experience Group 1: [180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 188.0, 189.0, 192.0, 193.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0]\n",
      "Experience Group 2: [201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 217.0, 218.0, 219.0]\n",
      "Experience Group 3: [220.0, 221.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0]\n",
      "\n",
      "Processing Experience 1 for class 0 with temperatures: [180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 188.0, 189.0, 192.0, 193.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0]...\n",
      "Initial class distribution in Experience 1 :\n",
      "Class 0: 39521\n",
      "Class 1: 15191\n",
      "Class 2: 469\n",
      "Dataset size for Experience 1 (class 0): 20000\n",
      "\n",
      "Processing Experience 2 for class 1 with temperatures: [201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 217.0, 218.0, 219.0]...\n",
      "Initial class distribution in Experience 2 :\n",
      "Class 0: 3618\n",
      "Class 1: 46914\n",
      "Class 2: 534\n",
      "Dataset size for Experience 2 (class 1): 20000\n",
      "\n",
      "Processing Experience 3 for class 2 with temperatures: [220.0, 221.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0]...\n",
      "Initial class distribution in Experience 3 :\n",
      "Class 0: 1450\n",
      "Class 1: 13652\n",
      "Class 2: 20054\n",
      "Dataset size for Experience 3 (class 2): 20000\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:03.958325Z",
     "start_time": "2025-03-28T11:14:03.882823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Define split proportions\n",
    "train_prop = 0.7\n",
    "valid_prop = 0.2\n",
    "# test_prop is computed as remainder\n",
    "\n",
    "def stratified_split(df, train_prop=0.7, valid_prop=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into stratified train, validation, and test sets based on 'hotend_class'.\n",
    "    \"\"\"\n",
    "    train_list, valid_list, test_list = [], [], []\n",
    "    \n",
    "    # Group the DataFrame by the class column\n",
    "    for cls, group in df.groupby('hotend_class'):\n",
    "        group_shuffled = group.sample(frac=1, random_state=random_state)\n",
    "        n = len(group_shuffled)\n",
    "        n_train = int(train_prop * n)\n",
    "        n_valid = int(valid_prop * n)\n",
    "        \n",
    "        train_list.append(group_shuffled.iloc[:n_train])\n",
    "        valid_list.append(group_shuffled.iloc[n_train:n_train+n_valid])\n",
    "        test_list.append(group_shuffled.iloc[n_train+n_valid:])\n",
    "    \n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    valid_df = pd.concat(valid_list).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# Combine experiences 1 and 2 into one dataset: experience_1_2\n",
    "combined_dataset = pd.DataFrame()\n",
    "for exp_id in [1, 2]:\n",
    "    if exp_id in experience_datasets:\n",
    "        # Use only the necessary columns from each experience\n",
    "        data = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "        combined_dataset = pd.concat([combined_dataset, data], ignore_index=True)\n",
    "\n",
    "# Use stratified splitting for Experience 1_2\n",
    "if not combined_dataset.empty:\n",
    "    experience_1_2_train, experience_1_2_valid, experience_1_2_test = stratified_split(combined_dataset, train_prop, valid_prop)\n",
    "    \n",
    "    total_images = len(combined_dataset)\n",
    "    print(\"\\n--- Experience 1_2 Stratified Splits ---\")\n",
    "    print(f\"Total images in Experience 1_2: {total_images}\")\n",
    "    print(f\"Train set size: {len(experience_1_2_train)}\")\n",
    "    print(f\"Validation set size: {len(experience_1_2_valid)}\")\n",
    "    print(f\"Test set size: {len(experience_1_2_test)}\")\n",
    "    print(\"Class distribution (combined):\", combined_dataset['hotend_class'].value_counts().to_dict())\n",
    "\n",
    "# Process experience 3 separately using the original non-stratified approach\n",
    "if 3 in experience_datasets:\n",
    "    dataset = experience_datasets[3][['img_path', 'hotend_class']]\n",
    "    indices = dataset.index.tolist()\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    total_images = len(indices)\n",
    "    train_count = int(train_prop * total_images)\n",
    "    valid_count = int(valid_prop * total_images)\n",
    "    # The test set gets the remaining images\n",
    "    test_count = total_images - train_count - valid_count\n",
    "    \n",
    "    experience_3_train = dataset.loc[indices[:train_count]].reset_index(drop=True)\n",
    "    experience_3_valid = dataset.loc[indices[train_count:train_count + valid_count]].reset_index(drop=True)\n",
    "    experience_3_test  = dataset.loc[indices[train_count + valid_count:]].reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n--- Experience 3 Splits ---\")\n",
    "    print(f\"Total images in Experience 3: {total_images}\")\n",
    "    print(f\"Train set size: {len(experience_3_train)}\")\n",
    "    print(f\"Validation set size: {len(experience_3_valid)}\")\n",
    "    print(f\"Test set size: {len(experience_3_test)}\")\n",
    "    print(\"Class distribution (Experience 3):\", dataset['hotend_class'].value_counts().to_dict())\n"
   ],
   "id": "31e7a38b483e34d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experience 1_2 Stratified Splits ---\n",
      "Total images in Experience 1_2: 40000\n",
      "Train set size: 28000\n",
      "Validation set size: 8000\n",
      "Test set size: 4000\n",
      "Class distribution (combined): {0: 20000, 1: 20000}\n",
      "\n",
      "--- Experience 3 Splits ---\n",
      "Total images in Experience 3: 20000\n",
      "Train set size: 14000\n",
      "Validation set size: 4000\n",
      "Test set size: 2000\n",
      "Class distribution (Experience 3): {2: 20000}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BalancedBatchSamplerClass",
   "id": "ae0b1b11498250dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:06.836025Z",
     "start_time": "2025-03-28T11:14:06.819200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BalancedBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_frame, batch_size=15, samples_per_class=5):\n",
    "        \"\"\"\n",
    "        data_frame: Pandas DataFrame with image paths and their respective class labels.\n",
    "        batch_size: Total batch size.\n",
    "        samples_per_class: Number of samples to draw from each class per batch.\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.batch_size = batch_size\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.num_classes = len(data_frame['hotend_class'].unique())\n",
    "        \n",
    "        if self.batch_size % self.num_classes != 0:\n",
    "            raise ValueError(\"Batch size must be divisible by the number of classes.\")\n",
    "\n",
    "        # Build a dictionary of indices per class.\n",
    "        self.class_indices = {\n",
    "            class_id: self.data_frame[self.data_frame['hotend_class'] == class_id].index.tolist()\n",
    "            for class_id in self.data_frame['hotend_class'].unique()\n",
    "        }\n",
    "        for class_id in self.class_indices:\n",
    "            random.shuffle(self.class_indices[class_id])\n",
    "        self.num_samples_per_epoch = sum(len(indices) for indices in self.class_indices.values())\n",
    "        self.indices_used = {class_id: [] for class_id in self.class_indices}\n",
    "    \n",
    "    def __iter__(self):\n",
    "        indices_used = {cid: self.class_indices[cid].copy() for cid in self.class_indices}\n",
    "        for indices in indices_used.values():\n",
    "            random.shuffle(indices)\n",
    "        \n",
    "        num_batches = min(len(indices) for indices in indices_used.values()) // self.samples_per_class\n",
    "        batches = []\n",
    "        for b in range(num_batches):\n",
    "            #print(f\"Before batch {b+1}, indices available per class:\")\n",
    "            #for cid in indices_used:\n",
    "                #print(f\"  Class {cid}: {len(indices_used[cid])} indices left\")\n",
    "            batch = []\n",
    "            for cid in self.class_indices:\n",
    "                batch.extend(indices_used[cid][:self.samples_per_class])\n",
    "                indices_used[cid] = indices_used[cid][self.samples_per_class:]\n",
    "            random.shuffle(batch)\n",
    "            batches.append(batch)\n",
    "        return iter(batches)\n",
    "\n",
    "\n",
    "# You can define __len__ to be a fixed number of batches per epoch if needed.\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(indices) for indices in self.class_indices.values()) // self.samples_per_class    "
   ],
   "id": "b69dbc1f5d8eb7de",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BalancedDataset Class",
   "id": "917a1dd3237b548b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:08.055041Z",
     "start_time": "2025-03-28T11:14:08.042823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BalancedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None, debug=False, max_retries=5):\n",
    "        self.debug = debug\n",
    "        self.root_dir = root_dir\n",
    "        # Reset index to ensure proper positional indexing.\n",
    "        self.data = data_frame.reset_index(drop=True)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.max_retries = max_retries\n",
    "        if self.debug:\n",
    "            print(f\"Dataset length (filtered): {len(self.data)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Use .iloc for positional indexing.\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = row.iloc[0].strip()  # e.g., \"print24/image-123.jpg\"\n",
    "        full_img_path = os.path.join(self.root_dir, img_path)\n",
    "        label = row.iloc[1]\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error loading image at index {idx} ({full_img_path}): {e}\")\n",
    "            # Instead of shifting the index, sample a replacement from the same class.\n",
    "            same_class_df = self.data[self.data.iloc[:, 1] == label]\n",
    "            if same_class_df.empty:\n",
    "                raise RuntimeError(f\"No replacement available for class {label}.\")\n",
    "            replacement_idx = random.choice(same_class_df.index.tolist())\n",
    "            # Try loading the replacement image.\n",
    "            row = self.data.iloc[replacement_idx]\n",
    "            img_path = row.iloc[0].strip()\n",
    "            full_img_path = os.path.join(self.root_dir, img_path)\n",
    "            try:\n",
    "                image = Image.open(full_img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, label\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load replacement image for index {idx} (class {label}): {e}\")"
   ],
   "id": "4a2bb378afc4313a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filter and reindex function",
   "id": "acb3a26d34d94dfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:09.236254Z",
     "start_time": "2025-03-28T11:14:09.225567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_and_reindex(data_frame, root_dir):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only rows with valid image paths\n",
    "    and then reindexes the DataFrame so that indices are contiguous.\n",
    "    \"\"\"\n",
    "    valid_indices = []\n",
    "    allowed_folders = {\"print24\", \"print131\", \"print0\", \"print46\",\"print82\",\"print111\",\"print132\", \"print37\",\"print122\"}\n",
    "    for idx in range(len(data_frame)):\n",
    "        img_path = data_frame.iloc[idx, 0].strip()\n",
    "        parts = img_path.split('/')\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        folder, file_name = parts[0], parts[1]\n",
    "        if folder not in allowed_folders:\n",
    "            continue\n",
    "        if not file_name.startswith(\"image-\"):\n",
    "            continue\n",
    "        full_img_path = os.path.join(root_dir, folder, file_name)\n",
    "        if os.path.exists(full_img_path):\n",
    "            valid_indices.append(idx)\n",
    "    filtered_df = data_frame.iloc[valid_indices].reset_index(drop=True)\n",
    "    return filtered_df"
   ],
   "id": "434dc1c60c10c06",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a Naive Class which inherits from AvalancheDataset and contains all the expected functions",
   "id": "6f07f8ff5e4c2d8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:18.599988Z",
     "start_time": "2025-03-28T11:14:10.084019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from avalanche.benchmarks.utils import AvalancheDataset, DataAttribute\n",
    "from avalanche.benchmarks.utils.transforms import TupleTransform\n",
    "\n",
    "class NaiveCompatibleBalancedDataset(AvalancheDataset):\n",
    "    def __init__(self, data_frame, root_dir=None, transform=None, task_label=0, indices=None):\n",
    "        \"\"\"\n",
    "        Custom dataset compatible with Naive that inherits from AvalancheDataset.\n",
    "        It loads images from disk, applies transforms, and provides sample-wise\n",
    "        attributes for targets and task labels.\n",
    "        \n",
    "        Args:\n",
    "            data_frame (pd.DataFrame or list): If a DataFrame, it must contain columns\n",
    "                'image_path' and 'hotend_class'. If a list, it is assumed to be a pre-built\n",
    "                list of datasets (used in subset calls).\n",
    "            root_dir (str, optional): Directory where images are stored. Must be provided if data_frame is a DataFrame.\n",
    "            transform (callable, optional): Transformations to apply.\n",
    "            task_label (int, optional): Task label for continual learning.\n",
    "            indices (Sequence[int], optional): Optional indices for subsetting.\n",
    "        \"\"\"\n",
    "        # If data_frame is a list, assume this is a call from subset() and forward the call.\n",
    "        if isinstance(data_frame, list):\n",
    "            super().__init__(data_frame, indices=indices)\n",
    "            return\n",
    "\n",
    "        # Otherwise, data_frame is a DataFrame. Ensure root_dir is provided.\n",
    "        if root_dir is None:\n",
    "            raise ValueError(\"root_dir must be provided when data_frame is a DataFrame\")\n",
    "        \n",
    "        # Reset DataFrame index for consistency.\n",
    "        self.data = data_frame.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.task_label = task_label\n",
    "\n",
    "        # Define a default transform if none provided.\n",
    "        default_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # Wrap the transform in TupleTransform so that it applies only to the image element.\n",
    "        self._transform_groups = {\n",
    "            \"train\": TupleTransform([transform or default_transform]),\n",
    "            \"eval\": TupleTransform([transform or default_transform])\n",
    "        }\n",
    "        \n",
    "        # Ensure required columns exist.\n",
    "        if 'hotend_class' not in self.data.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'hotend_class' for labels.\")\n",
    "        if 'image_path' not in self.data.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'image_path' for image paths.\")\n",
    "        \n",
    "        # Validate image paths and obtain valid indices.\n",
    "        valid_indices = self.get_valid_indices()\n",
    "        if len(valid_indices) == 0:\n",
    "            raise ValueError(\"No valid image paths found.\")\n",
    "        \n",
    "        # Compute targets and task labels for valid samples.\n",
    "        targets_data = torch.tensor(self.data.loc[valid_indices, 'hotend_class'].values)\n",
    "        targets_task_labels_data = torch.full_like(targets_data, self.task_label)\n",
    "        \n",
    "        # Prepare sample entries (one per valid image).\n",
    "        samples = []\n",
    "        for idx in valid_indices:\n",
    "            img_name = self.data.loc[idx, 'image_path'].strip()\n",
    "            full_img_path = os.path.join(self.root_dir, img_name)\n",
    "            label = int(self.data.loc[idx, 'hotend_class'])\n",
    "            samples.append({\n",
    "                \"img_path\": full_img_path,\n",
    "                \"label\": label,\n",
    "                \"task_label\": self.task_label\n",
    "            })\n",
    "        \n",
    "        # Define an internal basic dataset that loads images.\n",
    "        class BasicDataset(Dataset):\n",
    "            def __init__(self, samples):\n",
    "                self.samples = samples\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.samples)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                sample = self.samples[idx]\n",
    "                img_path = sample[\"img_path\"]\n",
    "                try:\n",
    "                    # Load the image (ensure it is a PIL image).\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "                    # If an error occurs, try the next sample.\n",
    "                    return self.__getitem__((idx + 1) % len(self.samples))\n",
    "                return image, sample[\"label\"], sample[\"task_label\"]\n",
    "        \n",
    "        basic_dataset = BasicDataset(samples)\n",
    "        \n",
    "        # Create data attributes.\n",
    "        data_attributes = [\n",
    "            DataAttribute(targets_data, name=\"targets\", use_in_getitem=True),\n",
    "            DataAttribute(targets_task_labels_data, name=\"targets_task_labels\", use_in_getitem=True)\n",
    "        ]\n",
    "        \n",
    "        # IMPORTANT: Pass the basic_dataset inside a list so that AvalancheDataset\n",
    "        # correctly sets up its internal flat data, and forward the indices parameter.\n",
    "        super().__init__(\n",
    "            [basic_dataset],\n",
    "            data_attributes=data_attributes,\n",
    "            transform_groups=self._transform_groups,\n",
    "            indices=indices\n",
    "        )\n",
    "    \n",
    "    def get_valid_indices(self):\n",
    "        \"\"\"Return indices for which the image file exists.\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "            img_name = self.data.loc[idx, 'image_path'].strip()\n",
    "            full_img_path = os.path.join(self.root_dir, img_name)\n",
    "            if os.path.exists(full_img_path):\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                print(f\"Image does not exist: {full_img_path}\")\n",
    "        print(f\"Total valid images: {len(valid_indices)}\")\n",
    "        return valid_indices"
   ],
   "id": "e4de91fbe3b7bc65",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating training, validation and testing datasets to implement EWC",
   "id": "cfa1e2e56975ab9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:44.745951Z",
     "start_time": "2025-03-28T11:14:22.758857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Define the transformation (e.g., normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Combine Experience 1 and 2\n",
    "# ---------------------------\n",
    "# Use the already split DataFrames from the combined dataset.\n",
    "# Here we filter and reindex these DataFrames and rename 'img_path' to 'image_path'.\n",
    "\n",
    "filtered_train_data_exp1_2 = filter_and_reindex(experience_1_2_train, root_dir).rename(\n",
    "    columns={'img_path': 'image_path'}\n",
    ")\n",
    "filtered_valid_data_exp1_2 = filter_and_reindex(experience_1_2_valid, root_dir).rename(\n",
    "    columns={'img_path': 'image_path'}\n",
    ")\n",
    "filtered_test_data_exp1_2 = filter_and_reindex(experience_1_2_test, root_dir).rename(\n",
    "    columns={'img_path': 'image_path'}\n",
    ")\n",
    "\n",
    "# Create dataset instances for combined Experience 1_2.\n",
    "train_dataset_exp1_2 = NaiveCompatibleBalancedDataset(\n",
    "    data_frame=filtered_train_data_exp1_2,\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "val_dataset_exp1_2 = NaiveCompatibleBalancedDataset(\n",
    "    data_frame=filtered_valid_data_exp1_2,\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "test_dataset_exp1_2 = NaiveCompatibleBalancedDataset(\n",
    "    data_frame=filtered_test_data_exp1_2,\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "# Process Experience 3\n",
    "# ------------------\n",
    "# Use the already split DataFrames for Experience 3.\n",
    "filtered_train_data_exp3 = filter_and_reindex(experience_3_train, root_dir).rename(\n",
    "    columns={'img_path': 'image_path'}\n",
    ")\n",
    "filtered_valid_data_exp3 = filter_and_reindex(experience_3_valid, root_dir).rename(\n",
    "    columns={'img_path': 'image_path'}\n",
    ")\n",
    "filtered_test_data_exp3 = filter_and_reindex(experience_3_test, root_dir).rename(\n",
    "    columns={'img_path': 'image_path'}\n",
    ")\n",
    "\n",
    "train_dataset_exp3 = NaiveCompatibleBalancedDataset(\n",
    "    data_frame=filtered_train_data_exp3,\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "val_dataset_exp3 = NaiveCompatibleBalancedDataset(\n",
    "    data_frame=filtered_valid_data_exp3,\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "test_dataset_exp3 = NaiveCompatibleBalancedDataset(\n",
    "    data_frame=filtered_test_data_exp3,\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")"
   ],
   "id": "7986d3634e8a788c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 28000/28000 [00:02<00:00, 13894.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 8000/8000 [00:00<00:00, 12912.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 4000/4000 [00:00<00:00, 12010.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 14000/14000 [00:00<00:00, 14439.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 4000/4000 [00:00<00:00, 12294.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 2000/2000 [00:00<00:00, 12594.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 2000\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating Dataloaders for more efficient data processing",
   "id": "5889010372b14f54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:48.764771Z",
     "start_time": "2025-03-28T11:14:48.711561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# Experience 1_2: Combined Data\n",
    "# ---------------------------\n",
    "train_sampler_exp1_2 = BalancedBatchSampler(\n",
    "    data_frame=filtered_train_data_exp1_2, \n",
    "    batch_size=10, \n",
    "    samples_per_class=5\n",
    ")\n",
    "val_sampler_exp1_2 = BalancedBatchSampler(\n",
    "    data_frame=filtered_valid_data_exp1_2, \n",
    "    batch_size=10, \n",
    "    samples_per_class=5\n",
    ")\n",
    "test_sampler_exp1_2 = BalancedBatchSampler(\n",
    "    data_frame=filtered_test_data_exp1_2, \n",
    "    batch_size=10, \n",
    "    samples_per_class=5\n",
    ")\n",
    "\n",
    "train_loader_exp1_2 = DataLoader(train_dataset_exp1_2, batch_sampler=train_sampler_exp1_2, shuffle=False)\n",
    "val_loader_exp1_2 = DataLoader(val_dataset_exp1_2, batch_sampler=val_sampler_exp1_2, shuffle=False)\n",
    "test_loader_exp1_2 = DataLoader(test_dataset_exp1_2, batch_sampler=test_sampler_exp1_2, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# Experience 3: Original Data\n",
    "# ---------------------------\n",
    "train_sampler_exp3 = BalancedBatchSampler(\n",
    "    data_frame=filtered_train_data_exp3, \n",
    "    batch_size=10, \n",
    "    samples_per_class=5\n",
    ")\n",
    "val_sampler_exp3 = BalancedBatchSampler(\n",
    "    data_frame=filtered_valid_data_exp3, \n",
    "    batch_size=10, \n",
    "    samples_per_class=5\n",
    ")\n",
    "test_sampler_exp3 = BalancedBatchSampler(\n",
    "    data_frame=filtered_test_data_exp3, \n",
    "    batch_size=10, \n",
    "    samples_per_class=5\n",
    ")\n",
    "\n",
    "train_loader_exp3 = DataLoader(train_dataset_exp3, batch_sampler=train_sampler_exp3, shuffle=False)\n",
    "val_loader_exp3 = DataLoader(val_dataset_exp3, batch_sampler=val_sampler_exp3, shuffle=False)\n",
    "test_loader_exp3 = DataLoader(test_dataset_exp3, batch_sampler=test_sampler_exp3, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders for all experiences created successfully!\")"
   ],
   "id": "6a9cb0dfaef9af15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders for all experiences created successfully!\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking the class distribution of random batches",
   "id": "24bbeee7d775115e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:14:51.729388Z",
     "start_time": "2025-03-28T11:14:49.601658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_random_batches_from_loader(loader, num_batches=3, dataset_name=\"Dataset\"):\n",
    "    print(f\"\\nRandom batches for {dataset_name}:\")\n",
    "    batch_iter = iter(loader)\n",
    "    for i in range(num_batches):\n",
    "        try:\n",
    "            batch = next(batch_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        # Assume that the labels are in the second position of the batch\n",
    "        labels = batch[1]\n",
    "        \n",
    "        # Convert labels to a NumPy array if needed.\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            labels_np = labels.cpu().numpy()\n",
    "        else:\n",
    "            labels_np = labels\n",
    "        \n",
    "        batch_dist = Counter(labels_np)\n",
    "        print(f\"Batch {i+1} class distribution: {batch_dist}\")\n",
    "\n",
    "# Example usage:\n",
    "print_random_batches_from_loader(train_loader_exp1_2, num_batches=3, dataset_name=\"Experience 1_2 Train\")\n",
    "print_random_batches_from_loader(val_loader_exp1_2, num_batches=3, dataset_name=\"Experience 1_2 Validation\")\n",
    "print_random_batches_from_loader(test_loader_exp1_2, num_batches=3, dataset_name=\"Experience 1_2 Test\")\n",
    "\n",
    "print_random_batches_from_loader(train_loader_exp3, num_batches=3, dataset_name=\"Experience 3 Train\")\n",
    "print_random_batches_from_loader(val_loader_exp3, num_batches=3, dataset_name=\"Experience 3 Validation\")\n",
    "print_random_batches_from_loader(test_loader_exp3, num_batches=3, dataset_name=\"Experience 3 Test\")"
   ],
   "id": "7430b031338aa46d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random batches for Experience 1_2 Train:\n",
      "Batch 1 class distribution: Counter({0: 5, 1: 5})\n",
      "Batch 2 class distribution: Counter({1: 5, 0: 5})\n",
      "Batch 3 class distribution: Counter({0: 5, 1: 5})\n",
      "\n",
      "Random batches for Experience 1_2 Validation:\n",
      "Batch 1 class distribution: Counter({1: 5, 0: 5})\n",
      "Batch 2 class distribution: Counter({1: 5, 0: 5})\n",
      "Batch 3 class distribution: Counter({1: 5, 0: 5})\n",
      "\n",
      "Random batches for Experience 1_2 Test:\n",
      "Batch 1 class distribution: Counter({1: 5, 0: 5})\n",
      "Batch 2 class distribution: Counter({0: 5, 1: 5})\n",
      "Batch 3 class distribution: Counter({1: 5, 0: 5})\n",
      "\n",
      "Random batches for Experience 3 Train:\n",
      "Batch 1 class distribution: Counter({2: 5})\n",
      "Batch 2 class distribution: Counter({2: 5})\n",
      "Batch 3 class distribution: Counter({2: 5})\n",
      "\n",
      "Random batches for Experience 3 Validation:\n",
      "Batch 1 class distribution: Counter({2: 5})\n",
      "Batch 2 class distribution: Counter({2: 5})\n",
      "Batch 3 class distribution: Counter({2: 5})\n",
      "\n",
      "Random batches for Experience 3 Test:\n",
      "Batch 1 class distribution: Counter({2: 5})\n",
      "Batch 2 class distribution: Counter({2: 5})\n",
      "Batch 3 class distribution: Counter({2: 5})\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking class distribution in each dataset",
   "id": "9b51c95f873a2933"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:16:02.818629Z",
     "start_time": "2025-03-28T11:16:02.740497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def count_classes(dataset):\n",
    "    # Convert the targets attribute into a list of values.\n",
    "    values = [x for x in dataset.targets]\n",
    "    # Convert the list of values to a tensor.\n",
    "    t = torch.tensor(values)\n",
    "    # Convert the tensor to a NumPy array and count the classes.\n",
    "    return Counter(t.numpy())\n",
    "\n",
    "print(\"Class distribution in Train Dataset (Experience 1_2):\", count_classes(train_dataset_exp1_2))\n",
    "print(\"Class distribution in Train Dataset (Experience 3):\", count_classes(train_dataset_exp3))\n",
    "\n",
    "print(\"Class distribution in Validation Dataset (Experience 1_2):\", count_classes(val_dataset_exp1_2))\n",
    "print(\"Class distribution in Validation Dataset (Experience 3):\", count_classes(val_dataset_exp3))\n",
    "\n",
    "print(\"Class distribution in Test Dataset (Experience 1_2):\", count_classes(test_dataset_exp1_2))\n",
    "print(\"Class distribution in Test Dataset (Experience 3):\", count_classes(test_dataset_exp3))"
   ],
   "id": "de58432abf1af70c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Train Dataset (Experience 1_2): Counter({0: 14000, 1: 14000})\n",
      "Class distribution in Train Dataset (Experience 3): Counter({2: 14000})\n",
      "Class distribution in Validation Dataset (Experience 1_2): Counter({0: 4000, 1: 4000})\n",
      "Class distribution in Validation Dataset (Experience 3): Counter({2: 4000})\n",
      "Class distribution in Test Dataset (Experience 1_2): Counter({0: 2000, 1: 2000})\n",
      "Class distribution in Test Dataset (Experience 3): Counter({2: 2000})\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking unique classes in each experience",
   "id": "b025b4df93557d61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T11:16:10.152145Z",
     "start_time": "2025-03-28T11:16:10.113437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from avalanche.benchmarks.utils import DataAttribute\n",
    "from avalanche.benchmarks import benchmark_from_datasets\n",
    "\n",
    "# Create the benchmark from your datasets using the combined experience_1_2 and experience_3.\n",
    "dataset_streams = {\n",
    "    \"train\": [train_dataset_exp1_2, train_dataset_exp3],\n",
    "    \"test\": [test_dataset_exp1_2, test_dataset_exp3]\n",
    "}\n",
    "\n",
    "benchmark = benchmark_from_datasets(**dataset_streams)\n",
    "\n",
    "for experience in benchmark.train_stream:\n",
    "    print(f\"Start of experience: {experience.current_experience}\")\n",
    "    \n",
    "    # Try to get the targets via the dynamic property.\n",
    "    try:\n",
    "        targets_data = experience.dataset.targets.data\n",
    "    except AttributeError:\n",
    "        # Fallback: access the internal _data_attributes dictionary.\n",
    "        targets_data = experience.dataset._data_attributes[\"targets\"].data\n",
    "\n",
    "    # If targets_data doesn't have 'tolist', assume it's already iterable.\n",
    "    if hasattr(targets_data, \"tolist\"):\n",
    "        unique_classes = set(targets_data.tolist())\n",
    "    else:\n",
    "        unique_classes = set(targets_data)\n",
    "        \n",
    "    print(f\"Classes in this experience: {unique_classes}\")"
   ],
   "id": "daf703114359b497",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of experience: 0\n",
      "Classes in this experience: {0, 1}\n",
      "Start of experience: 1\n",
      "Classes in this experience: {2}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementing Naive strategy using Avalanche - the end-to-end continual learning library",
   "id": "845a0d03b5690f43"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-28T11:16:11.293609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from avalanche.benchmarks import benchmark_from_datasets\n",
    "from avalanche.training import Naive\n",
    "from avalanche.training.plugins import EvaluationPlugin, LRSchedulerPlugin\n",
    "from avalanche.evaluation.metrics import (\n",
    "    accuracy_metrics,\n",
    "    loss_metrics,\n",
    "    timing_metrics,\n",
    "    cpu_usage_metrics,\n",
    "    forgetting_metrics,\n",
    "    StreamConfusionMatrix,\n",
    "    disk_usage_metrics\n",
    ")\n",
    "from avalanche.logging import TensorboardLogger, TextLogger, InteractiveLogger\n",
    "from avalanche.benchmarks.utils import DataAttribute\n",
    "from models.cnn_models import SimpleCNN, update_classifier  # Import both your CNN and update_classifier\n",
    "\n",
    "# -------------------------------\n",
    "# Create main folder for experiment outputs\n",
    "# -------------------------------\n",
    "MAIN_OUT_FOLDER = \"naive_experiment_new\"\n",
    "os.makedirs(MAIN_OUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper function: log metrics to CSV\n",
    "# -------------------------------\n",
    "def log_metrics(csv_file, experience_id, epoch, train_loss, train_acc, val_loss, val_acc):\n",
    "    with open(csv_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([experience_id, epoch, train_loss, train_acc, val_loss, val_acc])\n",
    "\n",
    "# -------------------------------\n",
    "# Helper function: plot metrics and save to folder \"loss_plots\"\n",
    "# -------------------------------\n",
    "def plot_metrics(epochs, train_vals, val_vals, ylabel, title, filename):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(epochs, train_vals, 'b-', label=f'Train {ylabel}')\n",
    "    plt.plot(epochs, val_vals, 'r-', label=f'Validation {ylabel}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Helper function: save a confusion matrix as an image.\n",
    "# -------------------------------\n",
    "def save_confusion_matrix(cm, title, filename):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = range(cm.shape[0])\n",
    "    plt.xticks(tick_marks)\n",
    "    plt.yticks(tick_marks)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > cm.max() / 2.0 else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------\n",
    "# Setup loggers and device\n",
    "# -------------------------------\n",
    "tb_logger = TensorboardLogger()\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "interactive_logger = InteractiveLogger()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -------------------------------\n",
    "# Setup benchmark and validation datasets\n",
    "# -------------------------------\n",
    "# Note: Here, we assume you have already created the following datasets:\n",
    "#   - train_dataset_exp1_2, val_dataset_exp1_2, test_dataset_exp1_2  (for combined experiences 1 & 2)\n",
    "#   - train_dataset_exp3,    val_dataset_exp3,    test_dataset_exp3       (for experience 3)\n",
    "\n",
    "dataset_streams = {\n",
    "    \"train\": [train_dataset_exp1_2, train_dataset_exp3],\n",
    "    \"test\": [test_dataset_exp1_2, test_dataset_exp3]\n",
    "}\n",
    "benchmark = benchmark_from_datasets(**dataset_streams)\n",
    "# Also store the validation datasets for later use.\n",
    "validation_datasets = [val_dataset_exp1_2, val_dataset_exp3]\n",
    "\n",
    "# -------------------------------\n",
    "# Set learning rate and prepare results summary\n",
    "# -------------------------------\n",
    "lr = 0.001\n",
    "results_summary = []\n",
    "\n",
    "# Create a folder for this hyperparameter configuration.\n",
    "config_folder = os.path.join(MAIN_OUT_FOLDER, f\"lr{lr}\")\n",
    "os.makedirs(config_folder, exist_ok=True)\n",
    "\n",
    "# Prepare a CSV file for summary metrics for this configuration.\n",
    "csv_file_path = os.path.join(config_folder, f\"summary_lr{lr}.csv\")\n",
    "with open(csv_file_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Experience\", \"Epoch\", \"TrainLoss\", \"TrainAcc\", \"ValLoss\", \"ValAcc\"])\n",
    "\n",
    "# We'll maintain the model between experiences.\n",
    "model = None\n",
    "\n",
    "# For each experience in the benchmark.\n",
    "for exp_idx, experience in enumerate(benchmark.train_stream):\n",
    "    print(f\"\\n=== Start of Experience {experience.current_experience} ===\")\n",
    "    \n",
    "    # Select the correct DataLoaders based on the experience.\n",
    "    # Experience 0: Combined experiences 1 & 2 (2 classes)\n",
    "    # Experience 1: Experience 3 (introduces the new class)\n",
    "    if experience.current_experience == 0:\n",
    "        current_train_loader = train_loader_exp1_2\n",
    "        current_val_loader   = val_loader_exp1_2\n",
    "        current_test_loader  = test_loader_exp1_2\n",
    "        # Instantiate model with 2 output classes.\n",
    "        model = SimpleCNN(num_classes=2).to(device)\n",
    "    elif experience.current_experience == 1:\n",
    "        current_train_loader = train_loader_exp3\n",
    "        current_val_loader   = val_loader_exp3\n",
    "        current_test_loader  = test_loader_exp3\n",
    "        # Update classifier to support 3 classes while preserving learned weights.\n",
    "        model = update_classifier(model, new_num_classes=3)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected experience id\")\n",
    "    \n",
    "    # Create a folder for this experience.\n",
    "    exp_folder = os.path.join(config_folder, f\"experience_{experience.current_experience}\")\n",
    "    os.makedirs(exp_folder, exist_ok=True)\n",
    "    \n",
    "    # Create a validation benchmark using the validation dataset.\n",
    "    val_benchmark = benchmark_from_datasets(\n",
    "        train=[current_val_loader.dataset],\n",
    "        test=[current_val_loader.dataset]\n",
    "    )\n",
    "    \n",
    "    # Set up criterion, optimizer, and learning rate scheduler.\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "    lr_plugin = LRSchedulerPlugin(scheduler)\n",
    "    \n",
    "    evaluator = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "        # Even though experience 0 has 2 classes, we set num_classes=3 overall.\n",
    "        StreamConfusionMatrix(num_classes=3, save_image=False),\n",
    "        loggers=[interactive_logger, text_logger, tb_logger]\n",
    "    )\n",
    "\n",
    "    # Instantiate the Naive strategy.\n",
    "    # Note: We set train_epochs=1 so we can loop for each epoch.\n",
    "    cl_strategy = Naive(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_mb_size=15,\n",
    "        train_epochs=1,\n",
    "        eval_mb_size=15,\n",
    "        evaluator=evaluator,\n",
    "        eval_every=-1,  # We'll perform our own per-epoch evaluation.\n",
    "        device=device,\n",
    "        plugins=[lr_plugin]\n",
    "    )\n",
    "    \n",
    "    # Lists to store per-epoch metrics.\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    num_epochs = 5  # Adjust this as needed.\n",
    "    \n",
    "    # Training and validation loop.\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch} for Experience {experience.current_experience} ...\")\n",
    "        train_res = cl_strategy.train(experience, train_loader=current_train_loader)\n",
    "        epoch_train_loss = train_res.get(\"Loss_Epoch/train_phase/train_stream\", None)\n",
    "        epoch_train_acc  = train_res.get(\"Top1_Acc_Epoch/train_phase/train_stream\", None)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        train_acc_history.append(epoch_train_acc)\n",
    "        \n",
    "        # Evaluate on the validation dataset.\n",
    "        val_res = cl_strategy.eval(val_benchmark.test_stream)\n",
    "        epoch_val_loss = val_res.get(\"Loss_Stream/eval_phase/test_stream\", None)\n",
    "        epoch_val_acc  = val_res.get(\"Top1_Acc_Stream/eval_phase/test_stream\", None)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_acc_history.append(epoch_val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss={epoch_train_loss:.4f}, Train Acc={epoch_train_acc:.4f} | Val Loss={epoch_val_loss:.4f}, Val Acc={epoch_val_acc:.4f}\")\n",
    "        \n",
    "        # Step the scheduler.\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log the epoch's metrics.\n",
    "        log_metrics(csv_file_path, experience.current_experience, epoch, epoch_train_loss, epoch_train_acc, epoch_val_loss, epoch_val_acc)\n",
    "    \n",
    "    # Plot losses and accuracies.\n",
    "    epochs_range = list(range(1, num_epochs + 1))\n",
    "    loss_title = f\"Exp {experience.current_experience}: lr={lr} (Loss)\"\n",
    "    loss_plot_path = os.path.join(exp_folder, f\"loss_plot_exp{experience.current_experience}.png\")\n",
    "    plot_metrics(epochs_range, train_loss_history, val_loss_history, \"Loss\", loss_title, loss_plot_path)\n",
    "    \n",
    "    acc_title = f\"Exp {experience.current_experience}: lr={lr} (Accuracy)\"\n",
    "    acc_plot_path = os.path.join(exp_folder, f\"acc_plot_exp{experience.current_experience}.png\")\n",
    "    plot_metrics(epochs_range, train_acc_history, val_acc_history, \"Accuracy\", acc_title, acc_plot_path)\n",
    "    \n",
    "    # --- End-of-Experience Testing ---\n",
    "    print(\"Testing on each test dataset for this experience...\")\n",
    "    # Evaluate on each test dataset in the benchmark.\n",
    "    for test_idx, test_dataset in enumerate(dataset_streams[\"test\"]):\n",
    "        test_benchmark = benchmark_from_datasets(\n",
    "            train=[test_dataset],\n",
    "            test=[test_dataset]\n",
    "        )\n",
    "        test_results = cl_strategy.eval(test_benchmark.test_stream)\n",
    "        test_cm = test_results.get(\"ConfusionMatrix_Stream/eval_phase/test_stream\", None)\n",
    "        if test_cm is not None and test_cm != \"No confusion matrix available\":\n",
    "            try:\n",
    "                cm_array = np.array(test_cm)\n",
    "                filename = os.path.join(exp_folder, f\"test_confusion_matrix_dataset_{test_idx}.png\")\n",
    "                title = f\"Experience {experience.current_experience} Test Confusion Matrix for Test Dataset {test_idx}\"\n",
    "                save_confusion_matrix(cm_array, title, filename)\n",
    "                print(f\"Saved confusion matrix for experience {experience.current_experience}, test dataset {test_idx} to {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not save confusion matrix for experience {experience.current_experience}, test dataset {test_idx}: {e}\")\n",
    "        else:\n",
    "            print(f\"No confusion matrix available for experience {experience.current_experience}, test dataset {test_idx}\")\n",
    "    \n",
    "    # Optionally, evaluate on the entire test stream.\n",
    "    print(\"Evaluating on the entire test stream...\")\n",
    "    test_res = cl_strategy.eval(benchmark.test_stream)\n",
    "    print(\"Test results:\", test_res)\n",
    "    \n",
    "    results_summary.append({\n",
    "        \"lr\": lr,\n",
    "        \"final_train_loss\": train_loss_history[-1],\n",
    "        \"final_val_loss\": val_loss_history[-1],\n",
    "        \"test_results\": test_res\n",
    "    })\n",
    "\n",
    "print(\"\\n=== Hyperparameter Search Summary ===\")\n",
    "for res in results_summary:\n",
    "    print(res)"
   ],
   "id": "5e3269bdfd9b0c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Start of Experience 0 ===\n",
      "Epoch 1 for Experience 0 ...\n",
      "-- >> Start of training phase << --\n",
      "  0%|          | 3/1867 [00:05<48:21,  1.56s/it]  "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "80cfcbb3c7c746d8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
