{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries",
   "id": "7a496f4799590725"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:11.065118Z",
     "start_time": "2025-03-11T11:17:10.978939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Sampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm  \n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import ConcatDataset\n",
    "import random\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ],
   "id": "39d4e05a5e4ad6a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating dataset for multiple parts",
   "id": "7786908805c83a6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:20.457400Z",
     "start_time": "2025-03-11T11:17:12.228634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define file paths as constants\n",
    "CSV_FILE_PATH = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'\n",
    "ROOT_DIR_PATH = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset'  # Common parent directory\n",
    "root_dir = ROOT_DIR_PATH\n",
    "\n",
    "# Load data into a DataFrame for easier processing\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Filter the dataset to include images containing \"print24\", \"print131\", or \"print0\"\n",
    "pattern = 'print24|print131|print0|print46|print82|print109|print111|print132|print171'\n",
    "data_filtered = data[data.iloc[:, 0].str.contains(pattern, na=False)]\n",
    "\n",
    "# Update the first column to include both the print folder and the image filename.\n",
    "# The regex now captures the folder name (print24, print131, or print0) and the image filename.\n",
    "data_filtered.iloc[:, 0] = data_filtered.iloc[:, 0].str.replace(\n",
    "    r'.*?/(print24|print131|print0|print46|print82|print109|print111|print132|print171)/(image-\\d+\\.jpg)', \n",
    "    r'\\1/\\2', \n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"First rows of filtered DataFrame:\")\n",
    "print(data_filtered.head())\n",
    "\n",
    "print(\"\\nLast rows of filtered DataFrame:\")\n",
    "print(data_filtered.tail())"
   ],
   "id": "e249d02789fd21d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of filtered DataFrame:\n",
      "              img_path               timestamp  flow_rate  feed_rate  \\\n",
      "0   print0/image-6.jpg  2020-10-08T13:12:50-34        100        100   \n",
      "1   print0/image-7.jpg  2020-10-08T13:12:50-80        100        100   \n",
      "2   print0/image-8.jpg  2020-10-08T13:12:51-27        100        100   \n",
      "3   print0/image-9.jpg  2020-10-08T13:12:51-74        100        100   \n",
      "4  print0/image-10.jpg  2020-10-08T13:12:52-20        100        100   \n",
      "\n",
      "   z_offset  target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  \\\n",
      "0       0.0          205.0  204.13  65.74           531           554   \n",
      "1       0.0          205.0  204.13  65.74           531           554   \n",
      "2       0.0          205.0  204.24  65.84           531           554   \n",
      "3       0.0          205.0  204.24  65.84           531           554   \n",
      "4       0.0          205.0  204.24  65.84           531           554   \n",
      "\n",
      "   img_num  print_id  flow_rate_class  feed_rate_class  z_offset_class  \\\n",
      "0        5         0                1                1               1   \n",
      "1        6         0                1                1               1   \n",
      "2        7         0                1                1               1   \n",
      "3        8         0                1                1               1   \n",
      "4        9         0                1                1               1   \n",
      "\n",
      "   hotend_class   img_mean    img_std  \n",
      "0             1  18.687230  13.809311  \n",
      "1             1  27.321104  22.875292  \n",
      "2             1  23.138174  17.933411  \n",
      "3             1  21.014212  17.120604  \n",
      "4             1  27.481729  15.091996  \n",
      "\n",
      "Last rows of filtered DataFrame:\n",
      "                        img_path               timestamp  flow_rate  \\\n",
      "932604  print171/image-25435.jpg  2020-10-09T20:05:52-31         94   \n",
      "932605  print171/image-25436.jpg  2020-10-09T20:05:52-78         94   \n",
      "932606  print171/image-25437.jpg  2020-10-09T20:05:53-24         94   \n",
      "932607  print171/image-25438.jpg  2020-10-09T20:05:53-71         94   \n",
      "932608  print171/image-25439.jpg  2020-10-09T20:05:54-17         94   \n",
      "\n",
      "        feed_rate  z_offset  target_hotend  hotend    bed  nozzle_tip_x  \\\n",
      "932604         83     -0.01          229.0  220.58  65.31           569   \n",
      "932605         83     -0.01          229.0  220.58  65.31           569   \n",
      "932606         83     -0.01          229.0  220.58  65.31           569   \n",
      "932607         83     -0.01          229.0  220.45  65.15           569   \n",
      "932608         83     -0.01          229.0  220.45  65.15           569   \n",
      "\n",
      "        nozzle_tip_y  img_num  print_id  flow_rate_class  feed_rate_class  \\\n",
      "932604           491    25434       171                1                0   \n",
      "932605           491    25435       171                1                0   \n",
      "932606           491    25436       171                1                0   \n",
      "932607           491    25437       171                1                0   \n",
      "932608           491    25438       171                1                0   \n",
      "\n",
      "        z_offset_class  hotend_class   img_mean    img_std  \n",
      "932604               1             2  51.364118  12.612697  \n",
      "932605               1             2  65.522158  17.575251  \n",
      "932606               1             2  50.552998  12.803905  \n",
      "932607               1             2  50.914915  12.440213  \n",
      "932608               1             2  49.718431  12.226354  \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysing the target hotend temperature column",
   "id": "5f5ef51c56f4d2f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:43.610178Z",
     "start_time": "2025-03-11T11:17:43.256657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())\n",
    "\n",
    "if len(unique_temperatures) >= 69:\n",
    "    temperature_min = unique_temperatures[0]\n",
    "    temperature_max = unique_temperatures[-1]\n",
    "    remaining_temperatures = [temp for temp in unique_temperatures if temp not in [temperature_min, temperature_max]]\n",
    "    random_temperatures = random.sample(remaining_temperatures, 50)\n",
    "    temperature_sublist = sorted([temperature_min, temperature_max] + random_temperatures)\n",
    "    \n",
    "    # Split the temperature sublist into three groups (roughly equal thirds)\n",
    "    split_size = len(temperature_sublist) // 3\n",
    "    experience_1 = temperature_sublist[:split_size]\n",
    "    experience_2 = temperature_sublist[split_size:2*split_size]\n",
    "    experience_3 = temperature_sublist[2*split_size:]\n",
    "    \n",
    "    print(\"Temperature sublist:\", temperature_sublist)\n",
    "    print(\"\\nExperience Group 1:\", experience_1)\n",
    "    print(\"Experience Group 2:\", experience_2)\n",
    "    print(\"Experience Group 3:\", experience_3)\n",
    "else:\n",
    "    print(\"Not enough unique temperatures to select from.\")\n",
    "    experience_1 = experience_2 = experience_3 = []\n",
    "\n",
    "# Create a dictionary to store balanced datasets (non-cumulative) for each experience\n",
    "experience_datasets = {}\n",
    "\n",
    "for exp_id, experience_temps in enumerate([experience_1, experience_2, experience_3], start=1):\n",
    "    if not experience_temps:\n",
    "        print(f\"Skipping Experience {exp_id} due to insufficient temperatures.\")\n",
    "        continue\n",
    "    print(f\"\\nProcessing Experience {exp_id} with temperatures: {experience_temps}...\")\n",
    "    \n",
    "    # Filter data for the current experience's temperatures\n",
    "    exp_data = data_filtered[data_filtered['target_hotend'].isin(experience_temps)]\n",
    "    if exp_data.empty:\n",
    "        print(f\"No data found for Experience {exp_id}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Create dictionary for each class (assumed classes: 0, 1, 2)\n",
    "    class_datasets = {}\n",
    "    for class_id in [0, 1, 2]:\n",
    "        class_data = exp_data[exp_data['hotend_class'] == class_id]\n",
    "        if class_data.empty:\n",
    "            print(f\"Warning: Class {class_id} in Experience {exp_id} has no data!\")\n",
    "        else:\n",
    "            class_datasets[class_id] = class_data\n",
    "    \n",
    "    if len(class_datasets) != 3:\n",
    "        print(f\"Skipping Experience {exp_id} because one or more classes are missing data!\")\n",
    "        continue\n",
    "    \n",
    "    # Balance by sampling the minimum available images per class\n",
    "    min_class_size = min(len(class_datasets[c]) for c in class_datasets)\n",
    "    print(f\"Smallest class size in Experience {exp_id}: {min_class_size}\")\n",
    "    \n",
    "    balanced_data = [class_datasets[c].sample(n=min_class_size, random_state=42) for c in class_datasets]\n",
    "    balanced_dataset = pd.concat(balanced_data).reset_index(drop=True)\n",
    "    balanced_dataset = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    experience_datasets[exp_id] = balanced_dataset\n",
    "    print(f\"Balanced dataset size for Experience {exp_id}: {len(balanced_dataset)}\")\n",
    "    for class_id in [0,1,2]:\n",
    "        count = len(balanced_dataset[balanced_dataset['hotend_class'] == class_id])\n",
    "        print(f\"Class {class_id} count: {count}\")"
   ],
   "id": "1b263bda9c6cfab4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature sublist: [180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 190.0, 192.0, 193.0, 196.0, 197.0, 199.0, 200.0, 202.0, 203.0, 204.0, 205.0, 207.0, 208.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 231.0, 233.0, 234.0, 236.0, 237.0, 238.0, 240.0, 241.0, 242.0, 244.0, 245.0, 248.0, 249.0, 250.0]\n",
      "\n",
      "Experience Group 1: [180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 190.0, 192.0, 193.0, 196.0, 197.0, 199.0, 200.0, 202.0]\n",
      "Experience Group 2: [203.0, 204.0, 205.0, 207.0, 208.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 222.0, 223.0, 224.0]\n",
      "Experience Group 3: [225.0, 226.0, 227.0, 228.0, 231.0, 233.0, 234.0, 236.0, 237.0, 238.0, 240.0, 241.0, 242.0, 244.0, 245.0, 248.0, 249.0, 250.0]\n",
      "\n",
      "Processing Experience 1 with temperatures: [180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 190.0, 192.0, 193.0, 196.0, 197.0, 199.0, 200.0, 202.0]...\n",
      "Smallest class size in Experience 1: 736\n",
      "Balanced dataset size for Experience 1: 2208\n",
      "Class 0 count: 736\n",
      "Class 1 count: 736\n",
      "Class 2 count: 736\n",
      "\n",
      "Processing Experience 2 with temperatures: [203.0, 204.0, 205.0, 207.0, 208.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 222.0, 223.0, 224.0]...\n",
      "Smallest class size in Experience 2: 2990\n",
      "Balanced dataset size for Experience 2: 8970\n",
      "Class 0 count: 2990\n",
      "Class 1 count: 2990\n",
      "Class 2 count: 2990\n",
      "\n",
      "Processing Experience 3 with temperatures: [225.0, 226.0, 227.0, 228.0, 231.0, 233.0, 234.0, 236.0, 237.0, 238.0, 240.0, 241.0, 242.0, 244.0, 245.0, 248.0, 249.0, 250.0]...\n",
      "Smallest class size in Experience 3: 624\n",
      "Balanced dataset size for Experience 3: 1872\n",
      "Class 0 count: 624\n",
      "Class 1 count: 624\n",
      "Class 2 count: 624\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:43.816880Z",
     "start_time": "2025-03-11T11:17:43.695200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine the overall minimum number of images per class across all experiences\n",
    "min_images_per_class_overall = min(\n",
    "    [min(experience_datasets[exp]['hotend_class'].value_counts()) for exp in experience_datasets]\n",
    ")\n",
    "print(\"Overall minimum images per class across experiences:\", min_images_per_class_overall)\n",
    "\n",
    "# Define split proportions\n",
    "train_prop = 0.7\n",
    "valid_prop = 0.15\n",
    "test_prop = 0.15\n",
    "\n",
    "samples_per_class_train = int(train_prop * min_images_per_class_overall)\n",
    "samples_per_class_valid = int(valid_prop * min_images_per_class_overall)\n",
    "# The test set gets the remaining images\n",
    "samples_per_class_test  = min_images_per_class_overall - samples_per_class_train - samples_per_class_valid\n",
    "\n",
    "print(\"Samples per class - Training:\", samples_per_class_train)\n",
    "print(\"Samples per class - Validation:\", samples_per_class_valid)\n",
    "print(\"Samples per class - Test:\", samples_per_class_test)\n",
    "\n",
    "# For each experience, re-sample the balanced dataset accordingly.\n",
    "for exp_id in [1, 2, 3]:\n",
    "    if exp_id not in experience_datasets:\n",
    "        continue\n",
    "    # Work only on the necessary columns\n",
    "    balanced_dataset_filtered = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "    \n",
    "    train_indices, valid_indices, test_indices = [], [], []\n",
    "    for class_label in [0, 1, 2]:\n",
    "        # Get indices for current class\n",
    "        class_indices = balanced_dataset_filtered[balanced_dataset_filtered['hotend_class'] == class_label].index.tolist()\n",
    "        random.shuffle(class_indices)\n",
    "        train_indices.extend(class_indices[:samples_per_class_train])\n",
    "        valid_indices.extend(class_indices[samples_per_class_train:samples_per_class_train + samples_per_class_valid])\n",
    "        test_indices.extend(class_indices[samples_per_class_train + samples_per_class_valid:\n",
    "                                           samples_per_class_train + samples_per_class_valid + samples_per_class_test])\n",
    "    \n",
    "    # Sort indices (optional, for consistency)\n",
    "    train_indices = sorted(train_indices)\n",
    "    valid_indices = sorted(valid_indices)\n",
    "    test_indices = sorted(test_indices)\n",
    "    \n",
    "    globals()[f'train_{exp_id}'] = balanced_dataset_filtered.loc[train_indices].reset_index(drop=True)\n",
    "    globals()[f'valid_{exp_id}'] = balanced_dataset_filtered.loc[valid_indices].reset_index(drop=True)\n",
    "    globals()[f'test_{exp_id}']  = balanced_dataset_filtered.loc[test_indices].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n--- Experience {exp_id} Splits ---\")\n",
    "    print(f\"Train set size: {len(globals()[f'train_{exp_id}'])} (Expected: {samples_per_class_train*3})\")\n",
    "    print(f\"Validation set size: {len(globals()[f'valid_{exp_id}'])} (Expected: {samples_per_class_valid*3})\")\n",
    "    print(f\"Test set size: {len(globals()[f'test_{exp_id}'])} (Expected: {samples_per_class_test*3})\")\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        df = globals()[f'{split}_{exp_id}']\n",
    "        counts = df['hotend_class'].value_counts().to_dict()\n",
    "        print(f\"{split.capitalize()} class distribution: {counts}\")"
   ],
   "id": "6dbe3f00b632beae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall minimum images per class across experiences: 624\n",
      "Samples per class - Training: 436\n",
      "Samples per class - Validation: 93\n",
      "Samples per class - Test: 95\n",
      "\n",
      "--- Experience 1 Splits ---\n",
      "Train set size: 1308 (Expected: 1308)\n",
      "Validation set size: 279 (Expected: 279)\n",
      "Test set size: 285 (Expected: 285)\n",
      "Train class distribution: {1: 436, 0: 436, 2: 436}\n",
      "Valid class distribution: {1: 93, 0: 93, 2: 93}\n",
      "Test class distribution: {2: 95, 0: 95, 1: 95}\n",
      "\n",
      "--- Experience 2 Splits ---\n",
      "Train set size: 1308 (Expected: 1308)\n",
      "Validation set size: 279 (Expected: 279)\n",
      "Test set size: 285 (Expected: 285)\n",
      "Train class distribution: {1: 436, 0: 436, 2: 436}\n",
      "Valid class distribution: {0: 93, 1: 93, 2: 93}\n",
      "Test class distribution: {0: 95, 1: 95, 2: 95}\n",
      "\n",
      "--- Experience 3 Splits ---\n",
      "Train set size: 1308 (Expected: 1308)\n",
      "Validation set size: 279 (Expected: 279)\n",
      "Test set size: 285 (Expected: 285)\n",
      "Train class distribution: {2: 436, 1: 436, 0: 436}\n",
      "Valid class distribution: {0: 93, 2: 93, 1: 93}\n",
      "Test class distribution: {0: 95, 2: 95, 1: 95}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BalancedBatchSampler Class",
   "id": "7d098e656c0caa12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:48.315234Z",
     "start_time": "2025-03-11T11:17:48.283195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BalancedBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_frame, batch_size=15, samples_per_class=5):\n",
    "        \"\"\"\n",
    "        data_frame: Pandas DataFrame with image paths and their respective class labels.\n",
    "        batch_size: Total batch size.\n",
    "        samples_per_class: Number of samples to draw from each class per batch.\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.batch_size = batch_size\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.num_classes = len(data_frame['hotend_class'].unique())\n",
    "        \n",
    "        if self.batch_size % self.num_classes != 0:\n",
    "            raise ValueError(\"Batch size must be divisible by the number of classes.\")\n",
    "\n",
    "        # Build a dictionary of indices per class.\n",
    "        self.class_indices = {\n",
    "            class_id: self.data_frame[self.data_frame['hotend_class'] == class_id].index.tolist()\n",
    "            for class_id in self.data_frame['hotend_class'].unique()\n",
    "        }\n",
    "        for class_id in self.class_indices:\n",
    "            random.shuffle(self.class_indices[class_id])\n",
    "        self.num_samples_per_epoch = sum(len(indices) for indices in self.class_indices.values())\n",
    "        self.indices_used = {class_id: [] for class_id in self.class_indices}\n",
    "    \n",
    "    def __iter__(self):\n",
    "        indices_used = {cid: self.class_indices[cid].copy() for cid in self.class_indices}\n",
    "        for indices in indices_used.values():\n",
    "            random.shuffle(indices)\n",
    "        \n",
    "        num_batches = min(len(indices) for indices in indices_used.values()) // self.samples_per_class\n",
    "        batches = []\n",
    "        for b in range(num_batches):\n",
    "            print(f\"Before batch {b+1}, indices available per class:\")\n",
    "            for cid in indices_used:\n",
    "                print(f\"  Class {cid}: {len(indices_used[cid])} indices left\")\n",
    "            batch = []\n",
    "            for cid in self.class_indices:\n",
    "                batch.extend(indices_used[cid][:self.samples_per_class])\n",
    "                indices_used[cid] = indices_used[cid][self.samples_per_class:]\n",
    "            random.shuffle(batch)\n",
    "            batches.append(batch)\n",
    "        return iter(batches)\n",
    "\n",
    "\n",
    "# You can define __len__ to be a fixed number of batches per epoch if needed.\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(indices) for indices in self.class_indices.values()) // self.samples_per_class    "
   ],
   "id": "e717b0e9e8624c04",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BalancedDataset Class",
   "id": "c47e5de4efb8cb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:49.004446Z",
     "start_time": "2025-03-11T11:17:48.977675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BalancedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None, debug=False, max_retries=5):\n",
    "        self.debug = debug\n",
    "        self.root_dir = root_dir\n",
    "        # Reset index to ensure proper positional indexing.\n",
    "        self.data = data_frame.reset_index(drop=True)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.max_retries = max_retries\n",
    "        if self.debug:\n",
    "            print(f\"Dataset length (filtered): {len(self.data)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Use .iloc for positional indexing.\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = row.iloc[0].strip()  # e.g., \"print24/image-123.jpg\"\n",
    "        full_img_path = os.path.join(self.root_dir, img_path)\n",
    "        label = row.iloc[1]\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error loading image at index {idx} ({full_img_path}): {e}\")\n",
    "            # Instead of shifting the index, sample a replacement from the same class.\n",
    "            same_class_df = self.data[self.data.iloc[:, 1] == label]\n",
    "            if same_class_df.empty:\n",
    "                raise RuntimeError(f\"No replacement available for class {label}.\")\n",
    "            replacement_idx = random.choice(same_class_df.index.tolist())\n",
    "            # Try loading the replacement image.\n",
    "            row = self.data.iloc[replacement_idx]\n",
    "            img_path = row.iloc[0].strip()\n",
    "            full_img_path = os.path.join(self.root_dir, img_path)\n",
    "            try:\n",
    "                image = Image.open(full_img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, label\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load replacement image for index {idx} (class {label}): {e}\")"
   ],
   "id": "607d9fe6c380f3b3",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build global indices function",
   "id": "ad25ec26ae660987"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:49.638522Z",
     "start_time": "2025-03-11T11:17:49.618660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_global_class_indices(concat_dataset):\n",
    "    \"\"\"\n",
    "    Builds a dictionary mapping each class (from 'hotend_class') to a list of global indices \n",
    "    for the given ConcatDataset.\n",
    "    \"\"\"\n",
    "    global_class_indices = {}\n",
    "    offset = 0\n",
    "    # Iterate over each sub-dataset in the ConcatDataset.\n",
    "    for ds in concat_dataset.datasets:\n",
    "        # Assume each sub-dataset has a 'data' attribute (a DataFrame).\n",
    "        df = ds.data.reset_index(drop=True) if hasattr(ds, 'data') else ds.reset_index(drop=True)\n",
    "        for cls in df['hotend_class'].unique():\n",
    "            if cls not in global_class_indices:\n",
    "                global_class_indices[cls] = []\n",
    "        # Add global indices (local index + offset)\n",
    "        for local_idx, row in df.iterrows():\n",
    "            cls = row['hotend_class']\n",
    "            global_class_indices[cls].append(offset + local_idx)\n",
    "        offset += len(df)\n",
    "    return global_class_indices\n",
    "\n",
    "class GlobalBalancedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    A sampler that yields balanced batches using global indices computed for a ConcatDataset.\n",
    "    It guarantees that each batch contains exactly `samples_per_class` samples from every class.\n",
    "    If a class does not have enough remaining indices in the current epoch, it will sample with replacement.\n",
    "    \"\"\"\n",
    "    def __init__(self, global_class_indices, batch_size=15, samples_per_class=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            global_class_indices (dict): Mapping from class label to list of global indices.\n",
    "            batch_size (int): Total batch size (must be divisible by the number of classes).\n",
    "            samples_per_class (int): Number of samples to draw per class in each batch.\n",
    "        \"\"\"\n",
    "        self.global_class_indices = global_class_indices\n",
    "        self.batch_size = batch_size\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.num_classes = len(global_class_indices)\n",
    "        if self.batch_size % self.num_classes != 0:\n",
    "            raise ValueError(\"Batch size must be divisible by the number of classes.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Make a local copy of the global indices for each class.\n",
    "        indices_used = {cls: self.global_class_indices[cls].copy() for cls in self.global_class_indices}\n",
    "        # Shuffle each list.\n",
    "        for cls in indices_used:\n",
    "            random.shuffle(indices_used[cls])\n",
    "        \n",
    "        # Compute the number of batches based on the minimum available indices.\n",
    "        # (This value may be less than what you want, but we will use replacement for classes that run low.)\n",
    "        num_batches = min(len(indices) for indices in indices_used.values()) // self.samples_per_class\n",
    "        \n",
    "        batches = []\n",
    "        # Always iterate over sorted keys to ensure a consistent order.\n",
    "        class_keys = sorted(self.global_class_indices.keys())\n",
    "        for _ in range(num_batches):\n",
    "            batch = []\n",
    "            for cls in class_keys:\n",
    "                # If there arenâ€™t enough remaining indices for this class, sample with replacement.\n",
    "                if len(indices_used[cls]) < self.samples_per_class:\n",
    "                    sampled = random.choices(self.global_class_indices[cls], k=self.samples_per_class)\n",
    "                else:\n",
    "                    sampled = indices_used[cls][:self.samples_per_class]\n",
    "                    indices_used[cls] = indices_used[cls][self.samples_per_class:]\n",
    "                batch.extend(sampled)\n",
    "            random.shuffle(batch)\n",
    "            batches.append(batch)\n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches computed from the minimum count (without replacement).\n",
    "        return min(len(indices) for indices in self.global_class_indices.values()) // self.samples_per_class"
   ],
   "id": "70dbb053eebb1f4e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating experience datasets",
   "id": "c446264dd5cf7b0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:50.291501Z",
     "start_time": "2025-03-11T11:17:50.240924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def wrap_dataset(ds, root_dir):\n",
    "    if hasattr(ds, 'data'):\n",
    "        df = ds.data.reset_index(drop=True)\n",
    "    else:\n",
    "        df = ds.reset_index(drop=True)\n",
    "    return BalancedDataset(df, root_dir, debug=False)\n",
    "\n",
    "\n",
    "# Experience 1 datasets (single datasets)\n",
    "exp1_train = globals()[\"train_1\"]\n",
    "exp1_valid = globals()[\"valid_1\"]\n",
    "exp1_test  = globals()[\"test_1\"]\n",
    "\n",
    "# For Experience 1_2, re-wrap the underlying DataFrames and then concatenate.\n",
    "exp1_2_train = ConcatDataset([\n",
    "    wrap_dataset(globals()[\"train_1\"], root_dir),\n",
    "    wrap_dataset(globals()[\"train_2\"], root_dir)\n",
    "])\n",
    "exp1_2_valid = ConcatDataset([\n",
    "    wrap_dataset(globals()[\"valid_1\"], root_dir),\n",
    "    wrap_dataset(globals()[\"valid_2\"], root_dir)\n",
    "])\n",
    "exp1_2_test = ConcatDataset([\n",
    "    wrap_dataset(globals()[\"test_1\"], root_dir),\n",
    "    wrap_dataset(globals()[\"test_2\"], root_dir)\n",
    "])\n",
    "\n",
    "# For Experience 1_2_3, re-wrap and concatenate datasets from experiences 1, 2, and 3.\n",
    "exp1_2_3_train = ConcatDataset([\n",
    "    wrap_dataset(globals()[\"train_1\"], root_dir),\n",
    "    wrap_dataset(globals()[\"train_2\"], root_dir),\n",
    "    wrap_dataset(globals()[\"train_3\"], root_dir)\n",
    "])\n",
    "exp1_2_3_valid = ConcatDataset([\n",
    "    wrap_dataset(globals()[\"valid_1\"], root_dir),\n",
    "    wrap_dataset(globals()[\"valid_2\"], root_dir),\n",
    "    wrap_dataset(globals()[\"valid_3\"], root_dir)\n",
    "])\n",
    "exp1_2_3_test = ConcatDataset([\n",
    "    wrap_dataset(globals()[\"test_1\"], root_dir),\n",
    "    wrap_dataset(globals()[\"test_2\"], root_dir),\n",
    "    wrap_dataset(globals()[\"test_3\"], root_dir)\n",
    "])"
   ],
   "id": "139a423ad28bd45",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:50.536390Z",
     "start_time": "2025-03-11T11:17:50.527290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_balanced_loader(dataset, root_dir, batch_size=15, samples_per_class=5):\n",
    "    \"\"\"\n",
    "    Given an experience dataset (plain DataFrame, BalancedDataset, or ConcatDataset),\n",
    "    create a DataLoader using a balanced batch sampler built from the underlying data.\n",
    "    \"\"\"\n",
    "    # If the dataset is a plain DataFrame, wrap it.\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        dataset = BalancedDataset(dataset, root_dir, debug=False)\n",
    "    \n",
    "    # If the dataset is a ConcatDataset, build global indices and use GlobalBalancedBatchSampler.\n",
    "    if isinstance(dataset, ConcatDataset):\n",
    "        global_class_indices = build_global_class_indices(dataset)\n",
    "        sampler = GlobalBalancedBatchSampler(global_class_indices, batch_size=batch_size, samples_per_class=samples_per_class)\n",
    "    elif hasattr(dataset, 'data'):\n",
    "        data_for_sampler = dataset.data.reset_index(drop=True)\n",
    "        sampler = BalancedBatchSampler(data_frame=data_for_sampler, batch_size=batch_size, samples_per_class=samples_per_class)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset type not recognized for sampler creation.\")\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_sampler=sampler)\n",
    "    return loader"
   ],
   "id": "37eb7dbadd12fd02",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:51.278206Z",
     "start_time": "2025-03-11T11:17:50.820848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# Create DataLoaders for each experience dataset.\n",
    "# =============================================================================\n",
    "\n",
    "exp1_train_loader    = create_balanced_loader(exp1_train, root_dir, batch_size=15, samples_per_class=5)\n",
    "exp1_valid_loader    = create_balanced_loader(exp1_valid, root_dir, batch_size=15, samples_per_class=5)\n",
    "exp1_test_loader     = create_balanced_loader(exp1_test,  root_dir, batch_size=15, samples_per_class=5)\n",
    "\n",
    "exp1_2_train_loader  = create_balanced_loader(exp1_2_train, root_dir, batch_size=15, samples_per_class=5)\n",
    "exp1_2_valid_loader  = create_balanced_loader(exp1_2_valid, root_dir, batch_size=15, samples_per_class=5)\n",
    "exp1_2_test_loader   = create_balanced_loader(exp1_2_test,  root_dir, batch_size=15, samples_per_class=5)\n",
    "\n",
    "exp1_2_3_train_loader = create_balanced_loader(exp1_2_3_train, root_dir, batch_size=15, samples_per_class=5)\n",
    "exp1_2_3_valid_loader = create_balanced_loader(exp1_2_3_valid, root_dir, batch_size=15, samples_per_class=5)\n",
    "exp1_2_3_test_loader  = create_balanced_loader(exp1_2_3_test,  root_dir, batch_size=15, samples_per_class=5)"
   ],
   "id": "1612d3dc194edd30",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:17:51.288692Z",
     "start_time": "2025-03-11T11:17:51.279689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_batches_from_loader(loader, num_batches=3, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Iterates through the given DataLoader and prints the label distribution\n",
    "    for the first `num_batches` batches.\n",
    "    \n",
    "    Args:\n",
    "        loader (DataLoader): The DataLoader to iterate over.\n",
    "        num_batches (int): Number of batches to print.\n",
    "        dataset_name (str): Name of the dataset (for printing purposes).\n",
    "    \"\"\"\n",
    "    print(f\"\\nBatch label distributions for {dataset_name}:\")\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "        # Ensure labels are on CPU and convert to a numpy array for counting.\n",
    "        label_counts = Counter(labels.cpu().numpy())\n",
    "        print(f\"Batch {batch_idx + 1} distribution: {label_counts}\")"
   ],
   "id": "1b9db86786709ace",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:18:10.408558Z",
     "start_time": "2025-03-11T11:17:51.408943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For Experience 1:\n",
    "print_batches_from_loader(exp1_train_loader, num_batches=3, dataset_name=\"Experience 1 Train\")\n",
    "print_batches_from_loader(exp1_valid_loader, num_batches=3, dataset_name=\"Experience 1 Validation\")\n",
    "print_batches_from_loader(exp1_test_loader,  num_batches=3, dataset_name=\"Experience 1 Test\")\n",
    "\n",
    "# For Experience 1_2:\n",
    "print_batches_from_loader(exp1_2_train_loader, num_batches=3, dataset_name=\"Experience 1_2 Train\")\n",
    "print_batches_from_loader(exp1_2_valid_loader, num_batches=3, dataset_name=\"Experience 1_2 Validation\")\n",
    "print_batches_from_loader(exp1_2_test_loader,  num_batches=3, dataset_name=\"Experience 1_2 Test\")\n",
    "\n",
    "# For Experience 1_2_3:\n",
    "print_batches_from_loader(exp1_2_3_train_loader, num_batches=3, dataset_name=\"Experience 1_2_3 Train\")\n",
    "print_batches_from_loader(exp1_2_3_valid_loader, num_batches=3, dataset_name=\"Experience 1_2_3 Validation\")\n",
    "print_batches_from_loader(exp1_2_3_test_loader,  num_batches=3, dataset_name=\"Experience 1_2_3 Test\")"
   ],
   "id": "766c1fdf324593f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch label distributions for Experience 1 Train:\n",
      "Before batch 1, indices available per class:\n",
      "  Class 1: 436 indices left\n",
      "  Class 0: 436 indices left\n",
      "  Class 2: 436 indices left\n",
      "Before batch 2, indices available per class:\n",
      "  Class 1: 431 indices left\n",
      "  Class 0: 431 indices left\n",
      "  Class 2: 431 indices left\n",
      "Before batch 3, indices available per class:\n",
      "  Class 1: 426 indices left\n",
      "  Class 0: 426 indices left\n",
      "  Class 2: 426 indices left\n",
      "Before batch 4, indices available per class:\n",
      "  Class 1: 421 indices left\n",
      "  Class 0: 421 indices left\n",
      "  Class 2: 421 indices left\n",
      "Before batch 5, indices available per class:\n",
      "  Class 1: 416 indices left\n",
      "  Class 0: 416 indices left\n",
      "  Class 2: 416 indices left\n",
      "Before batch 6, indices available per class:\n",
      "  Class 1: 411 indices left\n",
      "  Class 0: 411 indices left\n",
      "  Class 2: 411 indices left\n",
      "Before batch 7, indices available per class:\n",
      "  Class 1: 406 indices left\n",
      "  Class 0: 406 indices left\n",
      "  Class 2: 406 indices left\n",
      "Before batch 8, indices available per class:\n",
      "  Class 1: 401 indices left\n",
      "  Class 0: 401 indices left\n",
      "  Class 2: 401 indices left\n",
      "Before batch 9, indices available per class:\n",
      "  Class 1: 396 indices left\n",
      "  Class 0: 396 indices left\n",
      "  Class 2: 396 indices left\n",
      "Before batch 10, indices available per class:\n",
      "  Class 1: 391 indices left\n",
      "  Class 0: 391 indices left\n",
      "  Class 2: 391 indices left\n",
      "Before batch 11, indices available per class:\n",
      "  Class 1: 386 indices left\n",
      "  Class 0: 386 indices left\n",
      "  Class 2: 386 indices left\n",
      "Before batch 12, indices available per class:\n",
      "  Class 1: 381 indices left\n",
      "  Class 0: 381 indices left\n",
      "  Class 2: 381 indices left\n",
      "Before batch 13, indices available per class:\n",
      "  Class 1: 376 indices left\n",
      "  Class 0: 376 indices left\n",
      "  Class 2: 376 indices left\n",
      "Before batch 14, indices available per class:\n",
      "  Class 1: 371 indices left\n",
      "  Class 0: 371 indices left\n",
      "  Class 2: 371 indices left\n",
      "Before batch 15, indices available per class:\n",
      "  Class 1: 366 indices left\n",
      "  Class 0: 366 indices left\n",
      "  Class 2: 366 indices left\n",
      "Before batch 16, indices available per class:\n",
      "  Class 1: 361 indices left\n",
      "  Class 0: 361 indices left\n",
      "  Class 2: 361 indices left\n",
      "Before batch 17, indices available per class:\n",
      "  Class 1: 356 indices left\n",
      "  Class 0: 356 indices left\n",
      "  Class 2: 356 indices left\n",
      "Before batch 18, indices available per class:\n",
      "  Class 1: 351 indices left\n",
      "  Class 0: 351 indices left\n",
      "  Class 2: 351 indices left\n",
      "Before batch 19, indices available per class:\n",
      "  Class 1: 346 indices left\n",
      "  Class 0: 346 indices left\n",
      "  Class 2: 346 indices left\n",
      "Before batch 20, indices available per class:\n",
      "  Class 1: 341 indices left\n",
      "  Class 0: 341 indices left\n",
      "  Class 2: 341 indices left\n",
      "Before batch 21, indices available per class:\n",
      "  Class 1: 336 indices left\n",
      "  Class 0: 336 indices left\n",
      "  Class 2: 336 indices left\n",
      "Before batch 22, indices available per class:\n",
      "  Class 1: 331 indices left\n",
      "  Class 0: 331 indices left\n",
      "  Class 2: 331 indices left\n",
      "Before batch 23, indices available per class:\n",
      "  Class 1: 326 indices left\n",
      "  Class 0: 326 indices left\n",
      "  Class 2: 326 indices left\n",
      "Before batch 24, indices available per class:\n",
      "  Class 1: 321 indices left\n",
      "  Class 0: 321 indices left\n",
      "  Class 2: 321 indices left\n",
      "Before batch 25, indices available per class:\n",
      "  Class 1: 316 indices left\n",
      "  Class 0: 316 indices left\n",
      "  Class 2: 316 indices left\n",
      "Before batch 26, indices available per class:\n",
      "  Class 1: 311 indices left\n",
      "  Class 0: 311 indices left\n",
      "  Class 2: 311 indices left\n",
      "Before batch 27, indices available per class:\n",
      "  Class 1: 306 indices left\n",
      "  Class 0: 306 indices left\n",
      "  Class 2: 306 indices left\n",
      "Before batch 28, indices available per class:\n",
      "  Class 1: 301 indices left\n",
      "  Class 0: 301 indices left\n",
      "  Class 2: 301 indices left\n",
      "Before batch 29, indices available per class:\n",
      "  Class 1: 296 indices left\n",
      "  Class 0: 296 indices left\n",
      "  Class 2: 296 indices left\n",
      "Before batch 30, indices available per class:\n",
      "  Class 1: 291 indices left\n",
      "  Class 0: 291 indices left\n",
      "  Class 2: 291 indices left\n",
      "Before batch 31, indices available per class:\n",
      "  Class 1: 286 indices left\n",
      "  Class 0: 286 indices left\n",
      "  Class 2: 286 indices left\n",
      "Before batch 32, indices available per class:\n",
      "  Class 1: 281 indices left\n",
      "  Class 0: 281 indices left\n",
      "  Class 2: 281 indices left\n",
      "Before batch 33, indices available per class:\n",
      "  Class 1: 276 indices left\n",
      "  Class 0: 276 indices left\n",
      "  Class 2: 276 indices left\n",
      "Before batch 34, indices available per class:\n",
      "  Class 1: 271 indices left\n",
      "  Class 0: 271 indices left\n",
      "  Class 2: 271 indices left\n",
      "Before batch 35, indices available per class:\n",
      "  Class 1: 266 indices left\n",
      "  Class 0: 266 indices left\n",
      "  Class 2: 266 indices left\n",
      "Before batch 36, indices available per class:\n",
      "  Class 1: 261 indices left\n",
      "  Class 0: 261 indices left\n",
      "  Class 2: 261 indices left\n",
      "Before batch 37, indices available per class:\n",
      "  Class 1: 256 indices left\n",
      "  Class 0: 256 indices left\n",
      "  Class 2: 256 indices left\n",
      "Before batch 38, indices available per class:\n",
      "  Class 1: 251 indices left\n",
      "  Class 0: 251 indices left\n",
      "  Class 2: 251 indices left\n",
      "Before batch 39, indices available per class:\n",
      "  Class 1: 246 indices left\n",
      "  Class 0: 246 indices left\n",
      "  Class 2: 246 indices left\n",
      "Before batch 40, indices available per class:\n",
      "  Class 1: 241 indices left\n",
      "  Class 0: 241 indices left\n",
      "  Class 2: 241 indices left\n",
      "Before batch 41, indices available per class:\n",
      "  Class 1: 236 indices left\n",
      "  Class 0: 236 indices left\n",
      "  Class 2: 236 indices left\n",
      "Before batch 42, indices available per class:\n",
      "  Class 1: 231 indices left\n",
      "  Class 0: 231 indices left\n",
      "  Class 2: 231 indices left\n",
      "Before batch 43, indices available per class:\n",
      "  Class 1: 226 indices left\n",
      "  Class 0: 226 indices left\n",
      "  Class 2: 226 indices left\n",
      "Before batch 44, indices available per class:\n",
      "  Class 1: 221 indices left\n",
      "  Class 0: 221 indices left\n",
      "  Class 2: 221 indices left\n",
      "Before batch 45, indices available per class:\n",
      "  Class 1: 216 indices left\n",
      "  Class 0: 216 indices left\n",
      "  Class 2: 216 indices left\n",
      "Before batch 46, indices available per class:\n",
      "  Class 1: 211 indices left\n",
      "  Class 0: 211 indices left\n",
      "  Class 2: 211 indices left\n",
      "Before batch 47, indices available per class:\n",
      "  Class 1: 206 indices left\n",
      "  Class 0: 206 indices left\n",
      "  Class 2: 206 indices left\n",
      "Before batch 48, indices available per class:\n",
      "  Class 1: 201 indices left\n",
      "  Class 0: 201 indices left\n",
      "  Class 2: 201 indices left\n",
      "Before batch 49, indices available per class:\n",
      "  Class 1: 196 indices left\n",
      "  Class 0: 196 indices left\n",
      "  Class 2: 196 indices left\n",
      "Before batch 50, indices available per class:\n",
      "  Class 1: 191 indices left\n",
      "  Class 0: 191 indices left\n",
      "  Class 2: 191 indices left\n",
      "Before batch 51, indices available per class:\n",
      "  Class 1: 186 indices left\n",
      "  Class 0: 186 indices left\n",
      "  Class 2: 186 indices left\n",
      "Before batch 52, indices available per class:\n",
      "  Class 1: 181 indices left\n",
      "  Class 0: 181 indices left\n",
      "  Class 2: 181 indices left\n",
      "Before batch 53, indices available per class:\n",
      "  Class 1: 176 indices left\n",
      "  Class 0: 176 indices left\n",
      "  Class 2: 176 indices left\n",
      "Before batch 54, indices available per class:\n",
      "  Class 1: 171 indices left\n",
      "  Class 0: 171 indices left\n",
      "  Class 2: 171 indices left\n",
      "Before batch 55, indices available per class:\n",
      "  Class 1: 166 indices left\n",
      "  Class 0: 166 indices left\n",
      "  Class 2: 166 indices left\n",
      "Before batch 56, indices available per class:\n",
      "  Class 1: 161 indices left\n",
      "  Class 0: 161 indices left\n",
      "  Class 2: 161 indices left\n",
      "Before batch 57, indices available per class:\n",
      "  Class 1: 156 indices left\n",
      "  Class 0: 156 indices left\n",
      "  Class 2: 156 indices left\n",
      "Before batch 58, indices available per class:\n",
      "  Class 1: 151 indices left\n",
      "  Class 0: 151 indices left\n",
      "  Class 2: 151 indices left\n",
      "Before batch 59, indices available per class:\n",
      "  Class 1: 146 indices left\n",
      "  Class 0: 146 indices left\n",
      "  Class 2: 146 indices left\n",
      "Before batch 60, indices available per class:\n",
      "  Class 1: 141 indices left\n",
      "  Class 0: 141 indices left\n",
      "  Class 2: 141 indices left\n",
      "Before batch 61, indices available per class:\n",
      "  Class 1: 136 indices left\n",
      "  Class 0: 136 indices left\n",
      "  Class 2: 136 indices left\n",
      "Before batch 62, indices available per class:\n",
      "  Class 1: 131 indices left\n",
      "  Class 0: 131 indices left\n",
      "  Class 2: 131 indices left\n",
      "Before batch 63, indices available per class:\n",
      "  Class 1: 126 indices left\n",
      "  Class 0: 126 indices left\n",
      "  Class 2: 126 indices left\n",
      "Before batch 64, indices available per class:\n",
      "  Class 1: 121 indices left\n",
      "  Class 0: 121 indices left\n",
      "  Class 2: 121 indices left\n",
      "Before batch 65, indices available per class:\n",
      "  Class 1: 116 indices left\n",
      "  Class 0: 116 indices left\n",
      "  Class 2: 116 indices left\n",
      "Before batch 66, indices available per class:\n",
      "  Class 1: 111 indices left\n",
      "  Class 0: 111 indices left\n",
      "  Class 2: 111 indices left\n",
      "Before batch 67, indices available per class:\n",
      "  Class 1: 106 indices left\n",
      "  Class 0: 106 indices left\n",
      "  Class 2: 106 indices left\n",
      "Before batch 68, indices available per class:\n",
      "  Class 1: 101 indices left\n",
      "  Class 0: 101 indices left\n",
      "  Class 2: 101 indices left\n",
      "Before batch 69, indices available per class:\n",
      "  Class 1: 96 indices left\n",
      "  Class 0: 96 indices left\n",
      "  Class 2: 96 indices left\n",
      "Before batch 70, indices available per class:\n",
      "  Class 1: 91 indices left\n",
      "  Class 0: 91 indices left\n",
      "  Class 2: 91 indices left\n",
      "Before batch 71, indices available per class:\n",
      "  Class 1: 86 indices left\n",
      "  Class 0: 86 indices left\n",
      "  Class 2: 86 indices left\n",
      "Before batch 72, indices available per class:\n",
      "  Class 1: 81 indices left\n",
      "  Class 0: 81 indices left\n",
      "  Class 2: 81 indices left\n",
      "Before batch 73, indices available per class:\n",
      "  Class 1: 76 indices left\n",
      "  Class 0: 76 indices left\n",
      "  Class 2: 76 indices left\n",
      "Before batch 74, indices available per class:\n",
      "  Class 1: 71 indices left\n",
      "  Class 0: 71 indices left\n",
      "  Class 2: 71 indices left\n",
      "Before batch 75, indices available per class:\n",
      "  Class 1: 66 indices left\n",
      "  Class 0: 66 indices left\n",
      "  Class 2: 66 indices left\n",
      "Before batch 76, indices available per class:\n",
      "  Class 1: 61 indices left\n",
      "  Class 0: 61 indices left\n",
      "  Class 2: 61 indices left\n",
      "Before batch 77, indices available per class:\n",
      "  Class 1: 56 indices left\n",
      "  Class 0: 56 indices left\n",
      "  Class 2: 56 indices left\n",
      "Before batch 78, indices available per class:\n",
      "  Class 1: 51 indices left\n",
      "  Class 0: 51 indices left\n",
      "  Class 2: 51 indices left\n",
      "Before batch 79, indices available per class:\n",
      "  Class 1: 46 indices left\n",
      "  Class 0: 46 indices left\n",
      "  Class 2: 46 indices left\n",
      "Before batch 80, indices available per class:\n",
      "  Class 1: 41 indices left\n",
      "  Class 0: 41 indices left\n",
      "  Class 2: 41 indices left\n",
      "Before batch 81, indices available per class:\n",
      "  Class 1: 36 indices left\n",
      "  Class 0: 36 indices left\n",
      "  Class 2: 36 indices left\n",
      "Before batch 82, indices available per class:\n",
      "  Class 1: 31 indices left\n",
      "  Class 0: 31 indices left\n",
      "  Class 2: 31 indices left\n",
      "Before batch 83, indices available per class:\n",
      "  Class 1: 26 indices left\n",
      "  Class 0: 26 indices left\n",
      "  Class 2: 26 indices left\n",
      "Before batch 84, indices available per class:\n",
      "  Class 1: 21 indices left\n",
      "  Class 0: 21 indices left\n",
      "  Class 2: 21 indices left\n",
      "Before batch 85, indices available per class:\n",
      "  Class 1: 16 indices left\n",
      "  Class 0: 16 indices left\n",
      "  Class 2: 16 indices left\n",
      "Before batch 86, indices available per class:\n",
      "  Class 1: 11 indices left\n",
      "  Class 0: 11 indices left\n",
      "  Class 2: 11 indices left\n",
      "Before batch 87, indices available per class:\n",
      "  Class 1: 6 indices left\n",
      "  Class 0: 6 indices left\n",
      "  Class 2: 6 indices left\n",
      "Batch 1 distribution: Counter({0: 5, 1: 5, 2: 5})\n",
      "Batch 2 distribution: Counter({1: 5, 2: 5, 0: 5})\n",
      "Batch 3 distribution: Counter({0: 5, 1: 5, 2: 5})\n",
      "\n",
      "Batch label distributions for Experience 1 Validation:\n",
      "Before batch 1, indices available per class:\n",
      "  Class 1: 93 indices left\n",
      "  Class 0: 93 indices left\n",
      "  Class 2: 93 indices left\n",
      "Before batch 2, indices available per class:\n",
      "  Class 1: 88 indices left\n",
      "  Class 0: 88 indices left\n",
      "  Class 2: 88 indices left\n",
      "Before batch 3, indices available per class:\n",
      "  Class 1: 83 indices left\n",
      "  Class 0: 83 indices left\n",
      "  Class 2: 83 indices left\n",
      "Before batch 4, indices available per class:\n",
      "  Class 1: 78 indices left\n",
      "  Class 0: 78 indices left\n",
      "  Class 2: 78 indices left\n",
      "Before batch 5, indices available per class:\n",
      "  Class 1: 73 indices left\n",
      "  Class 0: 73 indices left\n",
      "  Class 2: 73 indices left\n",
      "Before batch 6, indices available per class:\n",
      "  Class 1: 68 indices left\n",
      "  Class 0: 68 indices left\n",
      "  Class 2: 68 indices left\n",
      "Before batch 7, indices available per class:\n",
      "  Class 1: 63 indices left\n",
      "  Class 0: 63 indices left\n",
      "  Class 2: 63 indices left\n",
      "Before batch 8, indices available per class:\n",
      "  Class 1: 58 indices left\n",
      "  Class 0: 58 indices left\n",
      "  Class 2: 58 indices left\n",
      "Before batch 9, indices available per class:\n",
      "  Class 1: 53 indices left\n",
      "  Class 0: 53 indices left\n",
      "  Class 2: 53 indices left\n",
      "Before batch 10, indices available per class:\n",
      "  Class 1: 48 indices left\n",
      "  Class 0: 48 indices left\n",
      "  Class 2: 48 indices left\n",
      "Before batch 11, indices available per class:\n",
      "  Class 1: 43 indices left\n",
      "  Class 0: 43 indices left\n",
      "  Class 2: 43 indices left\n",
      "Before batch 12, indices available per class:\n",
      "  Class 1: 38 indices left\n",
      "  Class 0: 38 indices left\n",
      "  Class 2: 38 indices left\n",
      "Before batch 13, indices available per class:\n",
      "  Class 1: 33 indices left\n",
      "  Class 0: 33 indices left\n",
      "  Class 2: 33 indices left\n",
      "Before batch 14, indices available per class:\n",
      "  Class 1: 28 indices left\n",
      "  Class 0: 28 indices left\n",
      "  Class 2: 28 indices left\n",
      "Before batch 15, indices available per class:\n",
      "  Class 1: 23 indices left\n",
      "  Class 0: 23 indices left\n",
      "  Class 2: 23 indices left\n",
      "Before batch 16, indices available per class:\n",
      "  Class 1: 18 indices left\n",
      "  Class 0: 18 indices left\n",
      "  Class 2: 18 indices left\n",
      "Before batch 17, indices available per class:\n",
      "  Class 1: 13 indices left\n",
      "  Class 0: 13 indices left\n",
      "  Class 2: 13 indices left\n",
      "Before batch 18, indices available per class:\n",
      "  Class 1: 8 indices left\n",
      "  Class 0: 8 indices left\n",
      "  Class 2: 8 indices left\n",
      "Batch 1 distribution: Counter({0: 5, 2: 5, 1: 5})\n",
      "Batch 2 distribution: Counter({0: 5, 2: 5, 1: 5})\n",
      "Batch 3 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "\n",
      "Batch label distributions for Experience 1 Test:\n",
      "Before batch 1, indices available per class:\n",
      "  Class 2: 95 indices left\n",
      "  Class 0: 95 indices left\n",
      "  Class 1: 95 indices left\n",
      "Before batch 2, indices available per class:\n",
      "  Class 2: 90 indices left\n",
      "  Class 0: 90 indices left\n",
      "  Class 1: 90 indices left\n",
      "Before batch 3, indices available per class:\n",
      "  Class 2: 85 indices left\n",
      "  Class 0: 85 indices left\n",
      "  Class 1: 85 indices left\n",
      "Before batch 4, indices available per class:\n",
      "  Class 2: 80 indices left\n",
      "  Class 0: 80 indices left\n",
      "  Class 1: 80 indices left\n",
      "Before batch 5, indices available per class:\n",
      "  Class 2: 75 indices left\n",
      "  Class 0: 75 indices left\n",
      "  Class 1: 75 indices left\n",
      "Before batch 6, indices available per class:\n",
      "  Class 2: 70 indices left\n",
      "  Class 0: 70 indices left\n",
      "  Class 1: 70 indices left\n",
      "Before batch 7, indices available per class:\n",
      "  Class 2: 65 indices left\n",
      "  Class 0: 65 indices left\n",
      "  Class 1: 65 indices left\n",
      "Before batch 8, indices available per class:\n",
      "  Class 2: 60 indices left\n",
      "  Class 0: 60 indices left\n",
      "  Class 1: 60 indices left\n",
      "Before batch 9, indices available per class:\n",
      "  Class 2: 55 indices left\n",
      "  Class 0: 55 indices left\n",
      "  Class 1: 55 indices left\n",
      "Before batch 10, indices available per class:\n",
      "  Class 2: 50 indices left\n",
      "  Class 0: 50 indices left\n",
      "  Class 1: 50 indices left\n",
      "Before batch 11, indices available per class:\n",
      "  Class 2: 45 indices left\n",
      "  Class 0: 45 indices left\n",
      "  Class 1: 45 indices left\n",
      "Before batch 12, indices available per class:\n",
      "  Class 2: 40 indices left\n",
      "  Class 0: 40 indices left\n",
      "  Class 1: 40 indices left\n",
      "Before batch 13, indices available per class:\n",
      "  Class 2: 35 indices left\n",
      "  Class 0: 35 indices left\n",
      "  Class 1: 35 indices left\n",
      "Before batch 14, indices available per class:\n",
      "  Class 2: 30 indices left\n",
      "  Class 0: 30 indices left\n",
      "  Class 1: 30 indices left\n",
      "Before batch 15, indices available per class:\n",
      "  Class 2: 25 indices left\n",
      "  Class 0: 25 indices left\n",
      "  Class 1: 25 indices left\n",
      "Before batch 16, indices available per class:\n",
      "  Class 2: 20 indices left\n",
      "  Class 0: 20 indices left\n",
      "  Class 1: 20 indices left\n",
      "Before batch 17, indices available per class:\n",
      "  Class 2: 15 indices left\n",
      "  Class 0: 15 indices left\n",
      "  Class 1: 15 indices left\n",
      "Before batch 18, indices available per class:\n",
      "  Class 2: 10 indices left\n",
      "  Class 0: 10 indices left\n",
      "  Class 1: 10 indices left\n",
      "Before batch 19, indices available per class:\n",
      "  Class 2: 5 indices left\n",
      "  Class 0: 5 indices left\n",
      "  Class 1: 5 indices left\n",
      "Batch 1 distribution: Counter({0: 5, 1: 5, 2: 5})\n",
      "Batch 2 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "Batch 3 distribution: Counter({1: 5, 2: 5, 0: 5})\n",
      "\n",
      "Batch label distributions for Experience 1_2 Train:\n",
      "Batch 1 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "Batch 2 distribution: Counter({0: 5, 2: 5, 1: 5})\n",
      "Batch 3 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "\n",
      "Batch label distributions for Experience 1_2 Validation:\n",
      "Batch 1 distribution: Counter({1: 5, 2: 5, 0: 5})\n",
      "Batch 2 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "Batch 3 distribution: Counter({1: 5, 0: 5, 2: 5})\n",
      "\n",
      "Batch label distributions for Experience 1_2 Test:\n",
      "Batch 1 distribution: Counter({2: 5, 0: 5, 1: 5})\n",
      "Batch 2 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "Batch 3 distribution: Counter({0: 5, 1: 5, 2: 5})\n",
      "\n",
      "Batch label distributions for Experience 1_2_3 Train:\n",
      "Batch 1 distribution: Counter({2: 5, 0: 5, 1: 5})\n",
      "Batch 2 distribution: Counter({0: 5, 2: 5, 1: 5})\n",
      "Batch 3 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "\n",
      "Batch label distributions for Experience 1_2_3 Validation:\n",
      "Batch 1 distribution: Counter({2: 5, 0: 5, 1: 5})\n",
      "Batch 2 distribution: Counter({0: 5, 1: 5, 2: 5})\n",
      "Batch 3 distribution: Counter({2: 5, 1: 5, 0: 5})\n",
      "\n",
      "Batch label distributions for Experience 1_2_3 Test:\n",
      "Batch 1 distribution: Counter({1: 5, 0: 5, 2: 5})\n",
      "Batch 2 distribution: Counter({1: 5, 0: 5, 2: 5})\n",
      "Batch 3 distribution: Counter({1: 5, 2: 5, 0: 5})\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking class distribution in each dataset",
   "id": "be91bcc1696c2e48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:18:16.437593Z",
     "start_time": "2025-03-11T11:18:16.421299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import ConcatDataset\n",
    "import pandas as pd\n",
    "\n",
    "def count_classes(dataset):\n",
    "    counts = Counter()\n",
    "    \n",
    "    # If the dataset is a plain DataFrame:\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        values = dataset.iloc[:, 1].tolist()\n",
    "        counts.update(values)\n",
    "    # If it's a ConcatDataset:\n",
    "    elif isinstance(dataset, ConcatDataset):\n",
    "        for d in dataset.datasets:\n",
    "            if isinstance(d, pd.DataFrame):\n",
    "                values = d.iloc[:, 1].tolist()\n",
    "            elif hasattr(d, 'data'):\n",
    "                values = d.data.iloc[:, 1].tolist()\n",
    "            else:\n",
    "                raise ValueError(\"Sub-dataset type not recognized.\")\n",
    "            counts.update(values)\n",
    "    # If it's an object with a 'data' attribute:\n",
    "    elif hasattr(dataset, 'data'):\n",
    "        values = dataset.data.iloc[:, 1].tolist()\n",
    "        counts.update(values)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset type not recognized.\")\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Now, print the class distributions for your experience datasets.\n",
    "# For Experience 1:\n",
    "print(\"Class distribution in Experience 1 train dataset:\", count_classes(exp1_train))\n",
    "print(\"Class distribution in Experience 1 valid dataset:\", count_classes(exp1_valid))\n",
    "print(\"Class distribution in Experience 1 test dataset:\", count_classes(exp1_test))\n",
    "\n",
    "# For Experience 1_2:\n",
    "print(\"Class distribution in Experience 1_2 train dataset:\", count_classes(exp1_2_train))\n",
    "print(\"Class distribution in Experience 1_2 valid dataset:\", count_classes(exp1_2_valid))\n",
    "print(\"Class distribution in Experience 1_2 test dataset:\", count_classes(exp1_2_test))\n",
    "\n",
    "# For Experience 1_2_3:\n",
    "print(\"Class distribution in Experience 1_2_3 train dataset:\", count_classes(exp1_2_3_train))\n",
    "print(\"Class distribution in Experience 1_2_3 valid dataset:\", count_classes(exp1_2_3_valid))\n",
    "print(\"Class distribution in Experience 1_2_3 test dataset:\", count_classes(exp1_2_3_test))"
   ],
   "id": "c4f440a565f30b3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Experience 1 train dataset: Counter({1: 436, 0: 436, 2: 436})\n",
      "Class distribution in Experience 1 valid dataset: Counter({1: 93, 0: 93, 2: 93})\n",
      "Class distribution in Experience 1 test dataset: Counter({2: 95, 0: 95, 1: 95})\n",
      "Class distribution in Experience 1_2 train dataset: Counter({1: 872, 0: 872, 2: 872})\n",
      "Class distribution in Experience 1_2 valid dataset: Counter({1: 186, 0: 186, 2: 186})\n",
      "Class distribution in Experience 1_2 test dataset: Counter({2: 190, 0: 190, 1: 190})\n",
      "Class distribution in Experience 1_2_3 train dataset: Counter({1: 1308, 0: 1308, 2: 1308})\n",
      "Class distribution in Experience 1_2_3 valid dataset: Counter({1: 279, 0: 279, 2: 279})\n",
      "Class distribution in Experience 1_2_3 test dataset: Counter({2: 285, 0: 285, 1: 285})\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:21:29.220010Z",
     "start_time": "2025-03-11T11:21:29.182319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def extract_folder(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the folder name from the file path that matches the pattern 'print' followed by digits.\n",
    "    For example, from '/some/path/print24/image.jpg' it returns 'print24'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(print\\d+)', file_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_unique_folders(dataset):\n",
    "    \"\"\"\n",
    "    Returns a set of unique folder names extracted from the image paths in the dataset.\n",
    "    It supports plain DataFrames, ConcatDataset, and custom datasets with a 'data' attribute.\n",
    "    Assumes the image path is stored in a column named 'image_path'.\n",
    "    \"\"\"\n",
    "    folders = set()\n",
    "    \n",
    "    def process_df(df):\n",
    "        col_name = 'img_path'  # Update to the correct column name if necessary\n",
    "        if col_name not in df.columns:\n",
    "            raise KeyError(f\"Expected column '{col_name}' not found in DataFrame columns: {list(df.columns)}\")\n",
    "        for path in df[col_name]:\n",
    "            folder = extract_folder(path)\n",
    "            if folder:\n",
    "                folders.add(folder)\n",
    "    \n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        process_df(dataset)\n",
    "    elif isinstance(dataset, ConcatDataset):\n",
    "        for ds in dataset.datasets:\n",
    "            if hasattr(ds, 'data'):\n",
    "                process_df(ds.data.reset_index(drop=True))\n",
    "            else:\n",
    "                process_df(ds.reset_index(drop=True))\n",
    "    elif hasattr(dataset, 'data'):\n",
    "        process_df(dataset.data.reset_index(drop=True))\n",
    "    else:\n",
    "        raise ValueError(\"Dataset type not recognized.\")\n",
    "    \n",
    "    return folders\n",
    "\n",
    "def get_dataset_size(dataset):\n",
    "    # If the dataset is a plain DataFrame:\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        return len(dataset)\n",
    "    # If it's a ConcatDataset:\n",
    "    elif isinstance(dataset, ConcatDataset):\n",
    "        return sum(len(d) for d in dataset.datasets)\n",
    "    # If it has a __len__ attribute (like BalancedDataset)\n",
    "    elif hasattr(dataset, '__len__'):\n",
    "        return len(dataset)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset type not recognized.\")\n",
    "\n",
    "# Now, print the sizes, class distributions, and unique folders for your experience datasets.\n",
    "\n",
    "# For Experience 1:\n",
    "print(\"Size of Experience 1 train dataset:\", get_dataset_size(exp1_train))\n",
    "print(\"Class distribution in Experience 1 train dataset:\", count_classes(exp1_train))\n",
    "print(\"Unique folders in Experience 1 train dataset:\", get_unique_folders(exp1_train))\n",
    "print(\"Size of Experience 1 valid dataset:\", get_dataset_size(exp1_valid))\n",
    "print(\"Class distribution in Experience 1 valid dataset:\", count_classes(exp1_valid))\n",
    "print(\"Unique folders in Experience 1 valid dataset:\", get_unique_folders(exp1_valid))\n",
    "print(\"Size of Experience 1 test dataset:\", get_dataset_size(exp1_test))\n",
    "print(\"Class distribution in Experience 1 test dataset:\", count_classes(exp1_test))\n",
    "print(\"Unique folders in Experience 1 test dataset:\", get_unique_folders(exp1_test))\n",
    "\n",
    "# For Experience 1_2:\n",
    "print(\"Size of Experience 1_2 train dataset:\", get_dataset_size(exp1_2_train))\n",
    "print(\"Class distribution in Experience 1_2 train dataset:\", count_classes(exp1_2_train))\n",
    "print(\"Unique folders in Experience 1_2 train dataset:\", get_unique_folders(exp1_2_train))\n",
    "print(\"Size of Experience 1_2 valid dataset:\", get_dataset_size(exp1_2_valid))\n",
    "print(\"Class distribution in Experience 1_2 valid dataset:\", count_classes(exp1_2_valid))\n",
    "print(\"Unique folders in Experience 1_2 valid dataset:\", get_unique_folders(exp1_2_valid))\n",
    "print(\"Size of Experience 1_2 test dataset:\", get_dataset_size(exp1_2_test))\n",
    "print(\"Class distribution in Experience 1_2 test dataset:\", count_classes(exp1_2_test))\n",
    "print(\"Unique folders in Experience 1_2 test dataset:\", get_unique_folders(exp1_2_test))\n",
    "\n",
    "# For Experience 1_2_3:\n",
    "print(\"Size of Experience 1_2_3 train dataset:\", get_dataset_size(exp1_2_3_train))\n",
    "print(\"Class distribution in Experience 1_2_3 train dataset:\", count_classes(exp1_2_3_train))\n",
    "print(\"Unique folders in Experience 1_2_3 train dataset:\", get_unique_folders(exp1_2_3_train))\n",
    "print(\"Size of Experience 1_2_3 valid dataset:\", get_dataset_size(exp1_2_3_valid))\n",
    "print(\"Class distribution in Experience 1_2_3 valid dataset:\", count_classes(exp1_2_3_valid))\n",
    "print(\"Unique folders in Experience 1_2_3 valid dataset:\", get_unique_folders(exp1_2_3_valid))\n",
    "print(\"Size of Experience 1_2_3 test dataset:\", get_dataset_size(exp1_2_3_test))\n",
    "print(\"Class distribution in Experience 1_2_3 test dataset:\", count_classes(exp1_2_3_test))\n",
    "print(\"Unique folders in Experience 1_2_3 test dataset:\", get_unique_folders(exp1_2_3_test))"
   ],
   "id": "302a3f30b1401f30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Experience 1 train dataset: 1308\n",
      "Class distribution in Experience 1 train dataset: Counter({1: 436, 0: 436, 2: 436})\n",
      "Unique folders in Experience 1 train dataset: {'print24', 'print46', 'print171', 'print131', 'print111', 'print0', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1 valid dataset: 279\n",
      "Class distribution in Experience 1 valid dataset: Counter({1: 93, 0: 93, 2: 93})\n",
      "Unique folders in Experience 1 valid dataset: {'print24', 'print171', 'print46', 'print131', 'print111', 'print0', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1 test dataset: 285\n",
      "Class distribution in Experience 1 test dataset: Counter({2: 95, 0: 95, 1: 95})\n",
      "Unique folders in Experience 1 test dataset: {'print24', 'print171', 'print46', 'print131', 'print111', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1_2 train dataset: 2616\n",
      "Class distribution in Experience 1_2 train dataset: Counter({1: 872, 0: 872, 2: 872})\n",
      "Unique folders in Experience 1_2 train dataset: {'print24', 'print46', 'print171', 'print131', 'print111', 'print0', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1_2 valid dataset: 558\n",
      "Class distribution in Experience 1_2 valid dataset: Counter({1: 186, 0: 186, 2: 186})\n",
      "Unique folders in Experience 1_2 valid dataset: {'print24', 'print171', 'print46', 'print131', 'print111', 'print0', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1_2 test dataset: 570\n",
      "Class distribution in Experience 1_2 test dataset: Counter({2: 190, 0: 190, 1: 190})\n",
      "Unique folders in Experience 1_2 test dataset: {'print24', 'print171', 'print46', 'print131', 'print111', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1_2_3 train dataset: 3924\n",
      "Class distribution in Experience 1_2_3 train dataset: Counter({1: 1308, 0: 1308, 2: 1308})\n",
      "Unique folders in Experience 1_2_3 train dataset: {'print24', 'print46', 'print171', 'print131', 'print111', 'print0', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1_2_3 valid dataset: 837\n",
      "Class distribution in Experience 1_2_3 valid dataset: Counter({1: 279, 0: 279, 2: 279})\n",
      "Unique folders in Experience 1_2_3 valid dataset: {'print24', 'print171', 'print46', 'print131', 'print111', 'print0', 'print132', 'print82', 'print109'}\n",
      "Size of Experience 1_2_3 test dataset: 855\n",
      "Class distribution in Experience 1_2_3 test dataset: Counter({2: 285, 0: 285, 1: 285})\n",
      "Unique folders in Experience 1_2_3 test dataset: {'print24', 'print171', 'print46', 'print131', 'print111', 'print0', 'print132', 'print82', 'print109'}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking image and label alignment",
   "id": "a6036ee53d6bb4c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def display_random_images(dataset, dataset_name, num_images=5, root_folder=r\"C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\"):\n",
    "    \"\"\"\n",
    "    Display num_images random images from the given dataset along with their hotend class labels.\n",
    "    \n",
    "    If the underlying DataFrame has 2 columns:\n",
    "      - Column 0: relative image path (e.g., \"print46/image-8719.jpg\")\n",
    "      - Column 1: hotend class label\n",
    "    If the DataFrame has 3 (or more) columns, it ignores the first column and uses:\n",
    "      - Column 1: relative image path\n",
    "      - Column 2: hotend class label\n",
    "    \n",
    "    The full image path is constructed as:\n",
    "         root_folder/<relative image path>\n",
    "    \n",
    "    If the dataset is a ConcatDataset, the function iterates over its sub-datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(dataset, 'data'):\n",
    "        cols = dataset.data.shape[1]\n",
    "        if cols == 2:\n",
    "            # Structure: [image_path, hotend_label]\n",
    "            get_img_path = lambda row: row[0]\n",
    "            get_label   = lambda row: row[1]\n",
    "        elif cols >= 3:\n",
    "            # Structure: [extra, image_path, hotend_label, ...] â€“ ignore the first column\n",
    "            get_img_path = lambda row: row[1]\n",
    "            get_label   = lambda row: row[2]\n",
    "        else:\n",
    "            print(f\"Dataset {dataset_name} does not have enough columns (got {cols}).\")\n",
    "            return\n",
    "\n",
    "        print(f\"Displaying {num_images} random images from: {dataset_name}\")\n",
    "        sample_indices = random.sample(range(len(dataset.data)), num_images)\n",
    "        for idx in sample_indices:\n",
    "            row = dataset.data.iloc[idx]\n",
    "            # Clean the strings: remove extra spaces and ensure correct separator\n",
    "            img_rel_path = str(get_img_path(row)).strip().replace(\"/\", os.path.sep)\n",
    "            label = str(get_label(row)).strip()\n",
    "            full_path = os.path.join(root_folder, img_rel_path)\n",
    "            \n",
    "            # Debug print: print the full path with repr to reveal any hidden characters\n",
    "            print(f\"Attempting to open: {repr(full_path)}\")\n",
    "            \n",
    "            if not os.path.exists(full_path):\n",
    "                print(f\"File does not exist: {repr(full_path)}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(full_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error opening {full_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Hotend Class: {label}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "    elif isinstance(dataset, ConcatDataset):\n",
    "        print(f\"Dataset '{dataset_name}' is a ConcatDataset; iterating through sub-datasets...\")\n",
    "        for i, subdataset in enumerate(dataset.datasets):\n",
    "            display_random_images(subdataset, f\"{dataset_name} - Subdataset {i}\", num_images, root_folder)\n",
    "    else:\n",
    "        print(\"Unsupported dataset type:\", type(dataset))\n",
    "\n",
    "# Example usage:\n",
    "display_random_images(exp1_train, \"Experience 1 Train Dataset\")\n",
    "display_random_images(exp1_valid, \"Experience 1 Valid Dataset\")\n",
    "display_random_images(exp1_test, \"Experience 1 Test Dataset\")\n",
    "\n",
    "display_random_images(exp1_2_train, \"Experience 1_2 Train Dataset\")\n",
    "display_random_images(exp1_2_valid, \"Experience 1_2 Valid Dataset\")\n",
    "display_random_images(exp1_2_test, \"Experience 1_2 Test Dataset\")\n",
    "\n",
    "display_random_images(exp1_2_3_train, \"Experience 1_2_3 Train Dataset\")\n",
    "display_random_images(exp1_2_3_valid, \"Experience 1_2_3 Valid Dataset\")\n",
    "display_random_images(exp1_2_3_test, \"Experience 1_2_3 Test Dataset\")"
   ],
   "id": "41978ac175608d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmark experiment ",
   "id": "f4dbd0010be59393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from models.cnn_models import SimpleCNN\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the experiment configurations in a dictionary.\n",
    "experiments = {\n",
    "    \"experience_1\": (exp1_train, exp1_valid, exp1_test),\n",
    "    \"experience_1_2\": (exp1_2_train, exp1_2_valid, exp1_2_test),\n",
    "    \"experience_1_2_3\": (exp1_2_3_train, exp1_2_3_valid, exp1_2_3_test)\n",
    "}\n",
    "\n",
    "# Create a mapping between experiment names and your new DataLoader variables.\n",
    "# (Make sure these variables are already created: exp1_train_loader, exp1_valid_loader, etc.)\n",
    "# For example:\n",
    "#   \"experience_1\" --> exp1_train_loader, exp1_valid_loader, exp1_test_loader\n",
    "#   \"experience_1_2\" --> exp1_2_train_loader, exp1_2_valid_loader, exp1_2_test_loader\n",
    "#   \"experience_1_2_3\" --> exp1_2_3_train_loader, exp1_2_3_valid_loader, exp1_2_3_test_loader\n",
    "\n",
    "# Get the current working directory and define the benchmark folder.\n",
    "current_dir = os.getcwd()\n",
    "benchmark_folder = os.path.join(current_dir, \"benchmark_experiment_increased_data\")\n",
    "os.makedirs(benchmark_folder, exist_ok=True)\n",
    "print(f\"Benchmark folder created at: {benchmark_folder}\")\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 30\n",
    "num_classes = 3  # update if needed\n",
    "\n",
    "# Loop over each experiment configuration.\n",
    "for exp_name, _ in experiments.items():\n",
    "    print(f\"\\nStarting experiment: {exp_name}\\n\")\n",
    "    \n",
    "    # Create a subfolder for this experiment.\n",
    "    exp_folder = os.path.join(benchmark_folder, exp_name)\n",
    "    os.makedirs(exp_folder, exist_ok=True)\n",
    "    \n",
    "    # Set the best model path.\n",
    "    best_model_path = os.path.join(exp_folder, f\"model_{exp_name}.pth\")\n",
    "    \n",
    "    # Retrieve the appropriate pre-created DataLoaders.\n",
    "    if exp_name == \"experience_1\":\n",
    "        train_loader = exp1_train_loader\n",
    "        val_loader = exp1_valid_loader\n",
    "        test_loader = exp1_test_loader\n",
    "    elif exp_name == \"experience_1_2\":\n",
    "        train_loader = exp1_2_train_loader\n",
    "        val_loader = exp1_2_valid_loader\n",
    "        test_loader = exp1_2_test_loader\n",
    "    elif exp_name == \"experience_1_2_3\":\n",
    "        train_loader = exp1_2_3_train_loader\n",
    "        val_loader = exp1_2_3_valid_loader\n",
    "        test_loader = exp1_2_3_test_loader\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown experiment name: {exp_name}\")\n",
    "    \n",
    "    # Set device to GPU if available.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model, loss function, optimizer, and scheduler.\n",
    "    model = SimpleCNN(num_classes=num_classes).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    # Initialize confusion matrix trackers.\n",
    "    train_cm = ConfusionMatrix(task='multiclass', num_classes=num_classes).to(device)\n",
    "    val_cm = ConfusionMatrix(task='multiclass', num_classes=num_classes).to(device)\n",
    "    \n",
    "    # For plotting losses and accuracies.\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Create CSV file to store epoch losses and accuracies.\n",
    "    csv_file_path = os.path.join(exp_folder, \"training_validation_losses.csv\")\n",
    "    header = [\"Epoch\", \"Training Loss\", \"Training Accuracy\", \"Validation Loss\", \"Validation Accuracy\"]\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    start_epoch = 0  # always start fresh for each experiment\n",
    "    \n",
    "    # ----------------- Training and Validation Loop -----------------\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        class_counts = [0] * num_classes\n",
    "        \n",
    "        # Training phase with progress bar.\n",
    "        for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            #print(f\"Outputs (Raw): {outputs}\")  # Log raw outputs\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            # Update confusion matrix and class counts.\n",
    "            train_cm.update(predicted, labels)\n",
    "            for label in labels:\n",
    "                class_counts[label.item()] += 1\n",
    "                \n",
    "            # Print predicted vs actual labels for each batch.\n",
    "            #for i in range(len(labels)):\n",
    "                #print(f\"Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "        \n",
    "        train_epoch_loss = running_loss / total_samples\n",
    "        train_epoch_accuracy = correct_predictions / total_samples\n",
    "        print(f\"Training Loss: {train_epoch_loss:.4f}, Training Accuracy: {train_epoch_accuracy:.4f}\")\n",
    "        print(f\"Training Class Distribution: {class_counts}\")\n",
    "        \n",
    "        # Update learning rate scheduler.\n",
    "        scheduler.step()\n",
    "        train_losses.append(train_epoch_loss)\n",
    "        train_accuracies.append(train_epoch_accuracy)\n",
    "        \n",
    "        # Compute and save training confusion matrix.\n",
    "        cm_train = train_cm.compute()\n",
    "        print(f\"Training Confusion Matrix:\\n{cm_train}\")\n",
    "        sns.heatmap(cm_train.cpu().numpy(), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Training Confusion Matrix - Epoch {epoch + 1}')\n",
    "        output_path_train = os.path.join(exp_folder, f\"training_confusion_matrix_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(output_path_train)\n",
    "        plt.clf()  # Clear the plot\n",
    "        print(f\"Training Confusion Matrix saved to: {output_path_train}\")\n",
    "        train_cm.reset()\n",
    "        \n",
    "        # ----------------- Validation Phase -----------------\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct_predictions = 0\n",
    "        val_total_samples = 0\n",
    "        val_class_counts = [0] * num_classes\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                #print(f\"Outputs (Raw): {outputs}\")  # Log raw outputs\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct_predictions += (predicted == labels).sum().item()\n",
    "                val_total_samples += labels.size(0)\n",
    "                \n",
    "                val_cm.update(predicted, labels)\n",
    "                for label in labels:\n",
    "                    val_class_counts[label.item()] += 1\n",
    "                \n",
    "                #for i in range(len(labels)):\n",
    "                    #print(f\"Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "        \n",
    "        val_epoch_loss = val_loss / val_total_samples\n",
    "        val_epoch_accuracy = val_correct_predictions / val_total_samples\n",
    "        print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}\")\n",
    "        print(f\"Validation Class Distribution: {val_class_counts}\")\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accuracies.append(val_epoch_accuracy)\n",
    "        \n",
    "        cm_val = val_cm.compute()\n",
    "        print(f\"Validation Confusion Matrix:\\n{cm_val}\")\n",
    "        sns.heatmap(cm_val.cpu().numpy(), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Validation Confusion Matrix - Epoch {epoch + 1}')\n",
    "        output_path_val = os.path.join(exp_folder, f\"validation_confusion_matrix_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(output_path_val)\n",
    "        plt.clf()\n",
    "        print(f\"Validation Confusion Matrix saved to: {output_path_val}\")\n",
    "        val_cm.reset()\n",
    "        \n",
    "        # Save the best model if validation accuracy improves.\n",
    "        if val_epoch_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_epoch_accuracy\n",
    "            torch.save({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"scheduler_state\": scheduler.state_dict(),\n",
    "                \"best_val_accuracy\": best_val_accuracy\n",
    "            }, best_model_path)\n",
    "            print(f\"Saved best model for {exp_name} at epoch {epoch + 1} with accuracy {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        # Append losses and accuracies to CSV.\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, train_epoch_loss, train_epoch_accuracy, val_epoch_loss, val_epoch_accuracy])\n",
    "    \n",
    "    print(f\"Experiment {exp_name} training complete. Losses saved to: {csv_file_path}\")\n",
    "    \n",
    "    # ----------------- Testing Phase -----------------\n",
    "    def test_model(model, test_loader):\n",
    "        model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        test_class_counts = [0] * num_classes\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                for label in labels:\n",
    "                    test_class_counts[label.item()] += 1\n",
    "                #for i in range(len(labels)):\n",
    "                    #print(f\"Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "        avg_accuracy = correct_predictions / total_samples\n",
    "        print(f\"Test Accuracy: {avg_accuracy:.4f}\")\n",
    "        print(f\"Test Class Distribution: {test_class_counts}\")\n",
    "        \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm_test = confusion_matrix(all_labels, all_predictions, labels=list(range(num_classes)))\n",
    "        print(f\"Test Confusion Matrix:\\n{cm_test}\")\n",
    "        \n",
    "        sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Test Confusion Matrix')\n",
    "        output_path_test = os.path.join(exp_folder, \"test_confusion_matrix.png\")\n",
    "        plt.savefig(output_path_test)\n",
    "        plt.clf()\n",
    "        print(f\"Test Confusion Matrix saved to: {output_path_test}\")\n",
    "    \n",
    "    test_model(model, test_loader)\n",
    "    \n",
    "    # Plot the training and validation losses.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Losses for {exp_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(exp_folder, \"training_validation_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.clf()\n",
    "    print(f\"Training and Validation Loss plot saved to: {loss_plot_path}\")\n",
    "    \n",
    "    # Plot the training and validation accuracies.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Training and Validation Accuracies for {exp_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    accuracy_plot_path = os.path.join(exp_folder, \"training_validation_accuracy.png\")\n",
    "    plt.savefig(accuracy_plot_path)\n",
    "    plt.clf()\n",
    "    print(f\"Training and Validation Accuracy plot saved to: {accuracy_plot_path}\")\n",
    "\n",
    "print(\"\\nAll benchmark experiments completed.\")"
   ],
   "id": "fdde0fbf43f25963",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking image and label alignment",
   "id": "322bf20b220dcd25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
