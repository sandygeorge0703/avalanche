{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import libraries ",
   "id": "36671eacce8cb8a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:28:32.919847Z",
     "start_time": "2024-11-12T10:28:26.392119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm  # Import tqdm for progress visualization\n",
    "from models.cnn_models import SimpleCNN\n",
    "from collections import Counter\n",
    "import random"
   ],
   "id": "afcfee75be6eeefc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define filepaths as constant",
   "id": "71e62fe7b8ae8d44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:28:32.926293Z",
     "start_time": "2024-11-12T10:28:32.920853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define file paths as constants\n",
    "CSV_FILE_PATH = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'\n",
    "ROOT_DIR_PATH = r'C:\\Users\\Sandhra George\\avalanche'\n",
    "\n",
    "csv_file = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'  # Path to the CSV file\n",
    "root_dir = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\\print0'  # Path to the image directory"
   ],
   "id": "f7422c446985ea38",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data into DataFrame",
   "id": "569bddba04d6df1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:28:35.553457Z",
     "start_time": "2024-11-12T10:28:32.927293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data into a DataFrame for easier processing\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Limit dataset to the first 3084 images (excluding header)\n",
    "data_limited = data.iloc[0:3085].reset_index(drop=True)\n",
    "\n",
    "# Filter the dataset to only include images containing \"print0\"\n",
    "data_filtered = data_limited[data_limited.iloc[:, 0].str.contains('print0', na=False)]\n",
    "\n",
    "# Update the first column to contain only the image filenames\n",
    "data_filtered.iloc[:, 0] = data_filtered.iloc[:, 0].str.replace(r'.*?/(image-\\d+\\.jpg)', r'\\1', regex=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"First rows of filtered DataFrame:\")\n",
    "print(data_filtered.head())\n",
    "\n",
    "# Display the last few rows of the updated DataFrame\n",
    "print(\"\\nLast rows of filtered DataFrame:\")\n",
    "print(data_filtered.tail())"
   ],
   "id": "ba8aa2adc060c334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of filtered DataFrame:\n",
      "       img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "0   image-6.jpg  2020-10-08T13:12:50-34        100        100       0.0   \n",
      "1   image-7.jpg  2020-10-08T13:12:50-80        100        100       0.0   \n",
      "2   image-8.jpg  2020-10-08T13:12:51-27        100        100       0.0   \n",
      "3   image-9.jpg  2020-10-08T13:12:51-74        100        100       0.0   \n",
      "4  image-10.jpg  2020-10-08T13:12:52-20        100        100       0.0   \n",
      "\n",
      "   target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "0          205.0  204.13  65.74           531           554        5   \n",
      "1          205.0  204.13  65.74           531           554        6   \n",
      "2          205.0  204.24  65.84           531           554        7   \n",
      "3          205.0  204.24  65.84           531           554        8   \n",
      "4          205.0  204.24  65.84           531           554        9   \n",
      "\n",
      "   print_id  flow_rate_class  feed_rate_class  z_offset_class  hotend_class  \\\n",
      "0         0                1                1               1             1   \n",
      "1         0                1                1               1             1   \n",
      "2         0                1                1               1             1   \n",
      "3         0                1                1               1             1   \n",
      "4         0                1                1               1             1   \n",
      "\n",
      "    img_mean    img_std  \n",
      "0  18.687230  13.809311  \n",
      "1  27.321104  22.875292  \n",
      "2  23.138174  17.933411  \n",
      "3  21.014212  17.120604  \n",
      "4  27.481729  15.091996  \n",
      "\n",
      "Last rows of filtered DataFrame:\n",
      "            img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "2705  image-3081.jpg  2020-10-08T13:36:43-20         91        167      0.21   \n",
      "2706  image-3082.jpg  2020-10-08T13:36:43-67         91        167      0.21   \n",
      "2707  image-3083.jpg  2020-10-08T13:36:44-13         91        167      0.21   \n",
      "2708  image-3084.jpg  2020-10-08T13:36:44-61         91        167      0.21   \n",
      "2709  image-3085.jpg  2020-10-08T13:36:45-35         91        167      0.21   \n",
      "\n",
      "      target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "2705          189.0  184.28  64.99           531           554     3080   \n",
      "2706          189.0  185.23  64.94           531           554     3081   \n",
      "2707          189.0  185.23  64.94           531           554     3082   \n",
      "2708          189.0  185.23  64.94           531           554     3083   \n",
      "2709          189.0  185.23  64.94           531           554     3084   \n",
      "\n",
      "      print_id  flow_rate_class  feed_rate_class  z_offset_class  \\\n",
      "2705         0                1                2               2   \n",
      "2706         0                1                2               2   \n",
      "2707         0                1                2               2   \n",
      "2708         0                1                2               2   \n",
      "2709         0                1                2               2   \n",
      "\n",
      "      hotend_class   img_mean    img_std  \n",
      "2705             0  43.920465  29.409431  \n",
      "2706             0  50.623226  38.112987  \n",
      "2707             0  50.155072  27.381694  \n",
      "2708             0  50.157106  28.875840  \n",
      "2709             0  52.109121  27.771782  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysing the hotend temperature column",
   "id": "af909128331dfa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:28:35.575848Z",
     "start_time": "2024-11-12T10:28:35.557957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract unique temperatures in the 'target_hotend' column and sort them\n",
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())  # Sort temperatures in ascending order\n",
    "\n",
    "# Calculate the full range of temperatures (min and max)\n",
    "temperature_min = data_filtered['target_hotend'].min()\n",
    "temperature_max = data_filtered['target_hotend'].max()\n",
    "\n",
    "# Print the unique temperatures (sorted), count, and full range\n",
    "print(\"\\nUnique target hotend temperatures in the dataset (sorted):\")\n",
    "print(unique_temperatures)\n",
    "print(f\"\\nNumber of unique target hotend temperatures: {len(unique_temperatures)}\")\n",
    "print(f\"Temperature range: {temperature_min}° to {temperature_max}°\")"
   ],
   "id": "5a2e601b8c1a2b3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique target hotend temperatures in the dataset (sorted):\n",
      "[181.0, 183.0, 187.0, 189.0, 194.0, 196.0, 205.0, 206.0, 209.0, 210.0, 214.0, 220.0, 226.0, 227.0, 228.0, 229.0, 230.0]\n",
      "\n",
      "Number of unique target hotend temperatures: 17\n",
      "Temperature range: 181.0° to 230.0°\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a \"random\" temperature sub list",
   "id": "cc36722b6f015211"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:30:59.303681Z",
     "start_time": "2024-11-12T10:30:59.295985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if we have enough unique temperatures to select from\n",
    "if len(unique_temperatures) >= 13:\n",
    "    # Select the lowest and highest temperatures\n",
    "    temperature_sublist = [temperature_min, temperature_max]\n",
    "\n",
    "    # Remove the lowest and highest temperatures from the unique temperatures list\n",
    "    remaining_temperatures = [temp for temp in unique_temperatures if temp != temperature_min and temp != temperature_max]\n",
    "\n",
    "    # Randomly select 11 other temperatures from the remaining ones\n",
    "    random_temperatures = random.sample(remaining_temperatures, 11)\n",
    "\n",
    "    # Add the random temperatures to the temperature_sublist\n",
    "    temperature_sublist.extend(random_temperatures)\n",
    "    \n",
    "    # Sort from lowest to highest hotend temperature\n",
    "    temperature_sublist = sorted(temperature_sublist)\n",
    "\n",
    "    # Print the temperature sublist\n",
    "    print(\"\\nTemperature sublist:\")\n",
    "    print(temperature_sublist)\n",
    "else:\n",
    "    print(\"Not enough unique temperatures to select from. At least 13 unique temperatures are required.\")"
   ],
   "id": "3d1ee38b740b9aa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature sublist:\n",
      "[181.0, 183.0, 187.0, 189.0, 194.0, 196.0, 206.0, 209.0, 210.0, 220.0, 226.0, 228.0, 230.0]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split the dataset into separate DataFrames for each class",
   "id": "6dda5270ad2ca419"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:31:02.531581Z",
     "start_time": "2024-11-12T10:31:02.485144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialise a dictionary to store DataFrames for each class and temperature combination\n",
    "class_temperature_datasets = {}\n",
    "\n",
    "# Iterate over all temperatures in the temperature_sublist\n",
    "for temp in temperature_sublist:\n",
    "    print(f\"Processing temperature: {temp}°\")\n",
    "    \n",
    "    # Filter the dataset for the current temperature\n",
    "    temp_filtered = data_filtered[data_filtered['target_hotend'] == temp]\n",
    "    \n",
    "    # Now, iterate over all classes (0, 1, 2)\n",
    "    for class_id in [0, 1, 2]:  # Ensure we process all classes: 0, 1, 2\n",
    "        # Filter the data for the current class\n",
    "        class_temp_data = temp_filtered[temp_filtered['hotend_class'] == class_id]\n",
    "        \n",
    "        if class_temp_data.empty:\n",
    "            # If there are no images for this class at the current temperature, print a message\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: 0\")\n",
    "        else:\n",
    "            # Shuffle the data (if needed) and store it in the dictionary\n",
    "            class_temperature_datasets[(class_id, temp)] = class_temp_data.sample(frac=1, random_state=42)\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: {len(class_temp_data)}\")\n",
    "    \n",
    "# Print the size of each class-temperature dataset (even if the size is 0)\n",
    "for temp in temperature_sublist:\n",
    "    print(f\"\\nSummary for Temperature: {temp}°\")\n",
    "    for class_id in [0, 1, 2]:\n",
    "        # Retrieve the data for the current temperature and class from the dictionary\n",
    "        if (class_id, temp) in class_temperature_datasets:\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: {len(class_temperature_datasets[(class_id, temp)])}\")\n",
    "        else:\n",
    "            # If no data for this class-temperature combination, print 0\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: 0\")\n",
    "\n",
    "# OPTIONAL: Process the minimum class sizes and print the class with the minimum size\n",
    "min_class_size = float('inf')  # Start with infinity as a comparison baseline\n",
    "min_class_id = None  # Variable to hold the class ID with the minimum size\n",
    "min_temp = None  # Variable to hold the temperature with the minimum size\n",
    "\n",
    "# Iterate over class-temperature datasets to find the minimum class size and its corresponding class-temperature\n",
    "for (class_id, temp), df in class_temperature_datasets.items():\n",
    "    class_size = len(df)\n",
    "    if class_size < min_class_size:\n",
    "        min_class_size = class_size\n",
    "        min_class_id = class_id\n",
    "        min_temp = temp\n",
    "\n",
    "# Print the minimum class size and the corresponding class ID and temperature\n",
    "print(f\"\\nMinimum class size: {min_class_size} (Class: {min_class_id}, Temperature: {min_temp}°)\")"
   ],
   "id": "597436ab2068d25a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing temperature: 181.0°\n",
      "Class 0 at 181.0° dataset size: 116\n",
      "Class 1 at 181.0° dataset size: 20\n",
      "Class 2 at 181.0° dataset size: 0\n",
      "Processing temperature: 183.0°\n",
      "Class 0 at 183.0° dataset size: 119\n",
      "Class 1 at 183.0° dataset size: 16\n",
      "Class 2 at 183.0° dataset size: 0\n",
      "Processing temperature: 187.0°\n",
      "Class 0 at 187.0° dataset size: 135\n",
      "Class 1 at 187.0° dataset size: 0\n",
      "Class 2 at 187.0° dataset size: 0\n",
      "Processing temperature: 189.0°\n",
      "Class 0 at 189.0° dataset size: 64\n",
      "Class 1 at 189.0° dataset size: 26\n",
      "Class 2 at 189.0° dataset size: 8\n",
      "Processing temperature: 194.0°\n",
      "Class 0 at 194.0° dataset size: 109\n",
      "Class 1 at 194.0° dataset size: 26\n",
      "Class 2 at 194.0° dataset size: 0\n",
      "Processing temperature: 196.0°\n",
      "Class 0 at 196.0° dataset size: 138\n",
      "Class 1 at 196.0° dataset size: 125\n",
      "Class 2 at 196.0° dataset size: 7\n",
      "Processing temperature: 206.0°\n",
      "Class 0 at 206.0° dataset size: 26\n",
      "Class 1 at 206.0° dataset size: 102\n",
      "Class 2 at 206.0° dataset size: 8\n",
      "Processing temperature: 209.0°\n",
      "Class 0 at 209.0° dataset size: 0\n",
      "Class 1 at 209.0° dataset size: 135\n",
      "Class 2 at 209.0° dataset size: 0\n",
      "Processing temperature: 210.0°\n",
      "Class 0 at 210.0° dataset size: 3\n",
      "Class 1 at 210.0° dataset size: 267\n",
      "Class 2 at 210.0° dataset size: 0\n",
      "Processing temperature: 220.0°\n",
      "Class 0 at 220.0° dataset size: 0\n",
      "Class 1 at 220.0° dataset size: 122\n",
      "Class 2 at 220.0° dataset size: 14\n",
      "Processing temperature: 226.0°\n",
      "Class 0 at 226.0° dataset size: 0\n",
      "Class 1 at 226.0° dataset size: 57\n",
      "Class 2 at 226.0° dataset size: 78\n",
      "Processing temperature: 228.0°\n",
      "Class 0 at 228.0° dataset size: 0\n",
      "Class 1 at 228.0° dataset size: 90\n",
      "Class 2 at 228.0° dataset size: 45\n",
      "Processing temperature: 230.0°\n",
      "Class 0 at 230.0° dataset size: 0\n",
      "Class 1 at 230.0° dataset size: 0\n",
      "Class 2 at 230.0° dataset size: 135\n",
      "\n",
      "Summary for Temperature: 181.0°\n",
      "Class 0 at 181.0° dataset size: 116\n",
      "Class 1 at 181.0° dataset size: 20\n",
      "Class 2 at 181.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 183.0°\n",
      "Class 0 at 183.0° dataset size: 119\n",
      "Class 1 at 183.0° dataset size: 16\n",
      "Class 2 at 183.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 187.0°\n",
      "Class 0 at 187.0° dataset size: 135\n",
      "Class 1 at 187.0° dataset size: 0\n",
      "Class 2 at 187.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 189.0°\n",
      "Class 0 at 189.0° dataset size: 64\n",
      "Class 1 at 189.0° dataset size: 26\n",
      "Class 2 at 189.0° dataset size: 8\n",
      "\n",
      "Summary for Temperature: 194.0°\n",
      "Class 0 at 194.0° dataset size: 109\n",
      "Class 1 at 194.0° dataset size: 26\n",
      "Class 2 at 194.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 196.0°\n",
      "Class 0 at 196.0° dataset size: 138\n",
      "Class 1 at 196.0° dataset size: 125\n",
      "Class 2 at 196.0° dataset size: 7\n",
      "\n",
      "Summary for Temperature: 206.0°\n",
      "Class 0 at 206.0° dataset size: 26\n",
      "Class 1 at 206.0° dataset size: 102\n",
      "Class 2 at 206.0° dataset size: 8\n",
      "\n",
      "Summary for Temperature: 209.0°\n",
      "Class 0 at 209.0° dataset size: 0\n",
      "Class 1 at 209.0° dataset size: 135\n",
      "Class 2 at 209.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 210.0°\n",
      "Class 0 at 210.0° dataset size: 3\n",
      "Class 1 at 210.0° dataset size: 267\n",
      "Class 2 at 210.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 220.0°\n",
      "Class 0 at 220.0° dataset size: 0\n",
      "Class 1 at 220.0° dataset size: 122\n",
      "Class 2 at 220.0° dataset size: 14\n",
      "\n",
      "Summary for Temperature: 226.0°\n",
      "Class 0 at 226.0° dataset size: 0\n",
      "Class 1 at 226.0° dataset size: 57\n",
      "Class 2 at 226.0° dataset size: 78\n",
      "\n",
      "Summary for Temperature: 228.0°\n",
      "Class 0 at 228.0° dataset size: 0\n",
      "Class 1 at 228.0° dataset size: 90\n",
      "Class 2 at 228.0° dataset size: 45\n",
      "\n",
      "Summary for Temperature: 230.0°\n",
      "Class 0 at 230.0° dataset size: 0\n",
      "Class 1 at 230.0° dataset size: 0\n",
      "Class 2 at 230.0° dataset size: 135\n",
      "\n",
      "Minimum class size: 3 (Class: 0, Temperature: 210.0°)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a balanced dataset",
   "id": "d96b4ed4148ae5d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:31:07.961480Z",
     "start_time": "2024-11-12T10:31:07.919514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the minimum number of images required in each class for a temperature to be included\n",
    "min_class_size = 3  # Adjust this to your desired minimum class size\n",
    "\n",
    "# Initialize a list to store valid datasets for each temperature\n",
    "valid_class_temperature_datasets = []\n",
    "\n",
    "# Process each temperature in the temperature sublist\n",
    "for temp in temperature_sublist:\n",
    "    print(f\"\\nProcessing temperature: {temp}°\")\n",
    "    \n",
    "    # Filter the dataset for the current temperature\n",
    "    temp_filtered = data_filtered[data_filtered['target_hotend'] == temp]\n",
    "    \n",
    "    # Dictionary to store class-specific data for the current temperature\n",
    "    temp_class_data = {}\n",
    "    meets_criteria = True  # Assume the temperature meets criteria until proven otherwise\n",
    "\n",
    "    # Iterate through each class (0, 1, 2)\n",
    "    for class_id in [0, 1, 2]:\n",
    "        # Filter by both class and temperature\n",
    "        class_temp_data = temp_filtered[temp_filtered['hotend_class'] == class_id]\n",
    "        \n",
    "        # Check and print actual dataset size for verification\n",
    "        actual_class_size = len(class_temp_data)\n",
    "        print(f\"Class {class_id} at {temp}° actual dataset size: {actual_class_size}\")\n",
    "\n",
    "        # Only add if the dataset size for this class meets the minimum requirement\n",
    "        if actual_class_size >= min_class_size:\n",
    "            # Sample exactly min_class_size images\n",
    "            temp_class_data[class_id] = class_temp_data.sample(n=min_class_size, random_state=42)\n",
    "        else:\n",
    "            print(f\"Class {class_id} at {temp}° does not have enough images ({actual_class_size}). Skipping this temperature.\")\n",
    "            meets_criteria = False\n",
    "            break  # Stop processing this temperature if any class fails to meet min_class_size\n",
    "\n",
    "    # If all classes at this temperature meet the criteria, add to valid datasets\n",
    "    if meets_criteria:\n",
    "        combined_data_for_temp = pd.concat(temp_class_data.values(), ignore_index=True)\n",
    "        valid_class_temperature_datasets.append(combined_data_for_temp)\n",
    "        print(f\"Temperature {temp}° included with {min_class_size} images per class.\")\n",
    "\n",
    "# Combine all valid datasets for all temperatures into one DataFrame\n",
    "balanced_data = pd.concat(valid_class_temperature_datasets, ignore_index=True) if valid_class_temperature_datasets else pd.DataFrame()\n",
    "\n",
    "# Shuffle the balanced dataset if it’s not empty\n",
    "if not balanced_data.empty:\n",
    "    balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"\\nTotal number of images in the balanced dataset: {len(balanced_data)}\")\n",
    "else:\n",
    "    print(\"No valid data left after filtering temperatures with insufficient class sizes.\")\n",
    "\n",
    "# Print the final class and temperature counts in the balanced dataset\n",
    "if not balanced_data.empty:\n",
    "    print(\"\\nClass and Temperature counts in the balanced dataset:\")\n",
    "    for temp in balanced_data['target_hotend'].unique():\n",
    "        print(f\"\\nTemperature: {temp}°\")\n",
    "        for class_id in [0, 1, 2]:\n",
    "            count = len(balanced_data[(balanced_data['hotend_class'] == class_id) & (balanced_data['target_hotend'] == temp)])\n",
    "            print(f\"Class {class_id}: {count} images\")\n",
    "else:\n",
    "    print(\"Balanced dataset is empty.\")"
   ],
   "id": "455b4c712af60cc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing temperature: 181.0°\n",
      "Class 0 at 181.0° actual dataset size: 116\n",
      "Class 1 at 181.0° actual dataset size: 20\n",
      "Class 2 at 181.0° actual dataset size: 0\n",
      "Class 2 at 181.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 183.0°\n",
      "Class 0 at 183.0° actual dataset size: 119\n",
      "Class 1 at 183.0° actual dataset size: 16\n",
      "Class 2 at 183.0° actual dataset size: 0\n",
      "Class 2 at 183.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 187.0°\n",
      "Class 0 at 187.0° actual dataset size: 135\n",
      "Class 1 at 187.0° actual dataset size: 0\n",
      "Class 1 at 187.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 189.0°\n",
      "Class 0 at 189.0° actual dataset size: 64\n",
      "Class 1 at 189.0° actual dataset size: 26\n",
      "Class 2 at 189.0° actual dataset size: 8\n",
      "Temperature 189.0° included with 3 images per class.\n",
      "\n",
      "Processing temperature: 194.0°\n",
      "Class 0 at 194.0° actual dataset size: 109\n",
      "Class 1 at 194.0° actual dataset size: 26\n",
      "Class 2 at 194.0° actual dataset size: 0\n",
      "Class 2 at 194.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 196.0°\n",
      "Class 0 at 196.0° actual dataset size: 138\n",
      "Class 1 at 196.0° actual dataset size: 125\n",
      "Class 2 at 196.0° actual dataset size: 7\n",
      "Temperature 196.0° included with 3 images per class.\n",
      "\n",
      "Processing temperature: 206.0°\n",
      "Class 0 at 206.0° actual dataset size: 26\n",
      "Class 1 at 206.0° actual dataset size: 102\n",
      "Class 2 at 206.0° actual dataset size: 8\n",
      "Temperature 206.0° included with 3 images per class.\n",
      "\n",
      "Processing temperature: 209.0°\n",
      "Class 0 at 209.0° actual dataset size: 0\n",
      "Class 0 at 209.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 210.0°\n",
      "Class 0 at 210.0° actual dataset size: 3\n",
      "Class 1 at 210.0° actual dataset size: 267\n",
      "Class 2 at 210.0° actual dataset size: 0\n",
      "Class 2 at 210.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 220.0°\n",
      "Class 0 at 220.0° actual dataset size: 0\n",
      "Class 0 at 220.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 226.0°\n",
      "Class 0 at 226.0° actual dataset size: 0\n",
      "Class 0 at 226.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 228.0°\n",
      "Class 0 at 228.0° actual dataset size: 0\n",
      "Class 0 at 228.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 230.0°\n",
      "Class 0 at 230.0° actual dataset size: 0\n",
      "Class 0 at 230.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Total number of images in the balanced dataset: 27\n",
      "\n",
      "Class and Temperature counts in the balanced dataset:\n",
      "\n",
      "Temperature: 189.0°\n",
      "Class 0: 3 images\n",
      "Class 1: 3 images\n",
      "Class 2: 3 images\n",
      "\n",
      "Temperature: 196.0°\n",
      "Class 0: 3 images\n",
      "Class 1: 3 images\n",
      "Class 2: 3 images\n",
      "\n",
      "Temperature: 206.0°\n",
      "Class 0: 3 images\n",
      "Class 1: 3 images\n",
      "Class 2: 3 images\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "~",
   "id": "ca9f9d77d4d016b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create training, validation, and testing datasets",
   "id": "a9d13161d1fe7599"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:31:12.426935Z",
     "start_time": "2024-11-12T10:31:12.370830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the total number of images\n",
    "total_images = len(balanced_data)\n",
    "print(f'Total images: {total_images}')\n",
    "\n",
    "# Get the minimum number of samples available in any class for any temperature\n",
    "min_class_counts_per_temp = (\n",
    "    balanced_data.groupby(['hotend_class', 'target_hotend']).size().groupby(level=1).min()\n",
    ")\n",
    "min_class_counts = min_class_counts_per_temp.min()\n",
    "\n",
    "# Calculate samples for train, validation, and test ensuring non-zero samples for each\n",
    "train_samples_per_class = max(int(0.8 * min_class_counts), 1)\n",
    "val_samples_per_class = max(int(0.1 * min_class_counts), 1)\n",
    "test_samples_per_class = max(min_class_counts - train_samples_per_class - val_samples_per_class, 1)\n",
    "\n",
    "# Ensure the sum does not exceed `min_class_counts`\n",
    "if train_samples_per_class + val_samples_per_class + test_samples_per_class > min_class_counts:\n",
    "    train_samples_per_class = min_class_counts - (val_samples_per_class + test_samples_per_class)\n",
    "\n",
    "# Debug: Print the minimum samples per temperature and intended dataset sizes per class\n",
    "print(f'Minimum samples per class per temperature: {min_class_counts}')\n",
    "print(f'Samples per class - Train: {train_samples_per_class}, Validation: {val_samples_per_class}, Test: {test_samples_per_class}')\n",
    "\n",
    "# Initialize empty DataFrames for each dataset\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through each temperature and sample equally from each class\n",
    "for temp in balanced_data['target_hotend'].unique():\n",
    "    for class_id in [0, 1, 2]:\n",
    "        # Filter the data by temperature and class\n",
    "        class_temp_data = balanced_data[(\n",
    "            balanced_data['target_hotend'] == temp) & \n",
    "            (balanced_data['hotend_class'] == class_id)\n",
    "        ]\n",
    "\n",
    "        # Ensure the data is shuffled\n",
    "        class_temp_data = class_temp_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Check if we have enough samples; take `min_class_counts` if available\n",
    "        if len(class_temp_data) >= min_class_counts:\n",
    "            # Split the class_temp_data into train, val, and test subsets\n",
    "            train_subset = class_temp_data.iloc[:train_samples_per_class]\n",
    "            val_subset = class_temp_data.iloc[train_samples_per_class:train_samples_per_class + val_samples_per_class]\n",
    "            test_subset = class_temp_data.iloc[train_samples_per_class + val_samples_per_class:train_samples_per_class + val_samples_per_class + test_samples_per_class]\n",
    "\n",
    "            # Append to respective datasets\n",
    "            train_data = pd.concat([train_data, train_subset], ignore_index=True)\n",
    "            val_data = pd.concat([val_data, val_subset], ignore_index=True)\n",
    "            test_data = pd.concat([test_data, test_subset], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Skipping temperature {temp} for class {class_id} due to insufficient data (only {len(class_temp_data)} images).\")\n",
    "\n",
    "# Debug: Print the sizes of the datasets after the split\n",
    "print(f'Training set size: {len(train_data)}')\n",
    "print(f'Validation set size: {len(val_data)}')\n",
    "print(f'Testing set size: {len(test_data)}')\n",
    "\n",
    "# Function to print class counts for a given dataset, grouped by temperature\n",
    "def print_class_temperature_counts(dataset, dataset_name):\n",
    "    print(f\"\\nClass and Temperature counts in {dataset_name}:\")\n",
    "    # Group by temperature and class\n",
    "    grouped = dataset.groupby(['target_hotend', 'hotend_class']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Iterate through temperatures and print the counts for each class\n",
    "    for temp in grouped.index:\n",
    "        print(f\"Temperature {temp}: \", end=\"\")\n",
    "        for class_id in grouped.columns:\n",
    "            class_count = grouped.loc[temp, class_id]\n",
    "            print(f\"Class {class_id}: {class_count}\", end=\", \")\n",
    "        print()  # New line for the next temperature\n",
    "\n",
    "# Print class counts for each dataset, grouped by temperature\n",
    "print_class_temperature_counts(train_data, \"training data\")\n",
    "print_class_temperature_counts(val_data, \"validation data\")\n",
    "print_class_temperature_counts(test_data, \"testing data\")\n",
    "\n",
    "# Print the first five rows of each dataset\n",
    "print(\"\\nFirst five rows of training data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nFirst five rows of validation data:\")\n",
    "print(val_data.head())\n",
    "\n",
    "print(\"\\nFirst five rows of testing data:\")\n",
    "print(test_data.head())"
   ],
   "id": "2915614c098633b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 27\n",
      "Minimum samples per class per temperature: 3\n",
      "Samples per class - Train: 1, Validation: 1, Test: 1\n",
      "Training set size: 9\n",
      "Validation set size: 9\n",
      "Testing set size: 9\n",
      "\n",
      "Class and Temperature counts in training data:\n",
      "Temperature 189.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "Temperature 196.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "Temperature 206.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "\n",
      "Class and Temperature counts in validation data:\n",
      "Temperature 189.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "Temperature 196.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "Temperature 206.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "\n",
      "Class and Temperature counts in testing data:\n",
      "Temperature 189.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "Temperature 196.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "Temperature 206.0: Class 0: 1, Class 1: 1, Class 2: 1, \n",
      "\n",
      "First five rows of training data:\n",
      "         img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "0  image-3074.jpg  2020-10-08T13:36:39-95         91        167      0.21   \n",
      "1  image-3012.jpg  2020-10-08T13:36:11-02         91        167      0.21   \n",
      "2  image-2988.jpg  2020-10-08T13:35:59-87         91        167      0.21   \n",
      "3  image-2741.jpg  2020-10-08T13:34:04-92         29        125      0.14   \n",
      "4  image-1076.jpg  2020-10-08T13:21:09-16        200         20      0.00   \n",
      "\n",
      "   target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "0          189.0  183.39  65.05           531           554     3073   \n",
      "1          189.0  204.90  64.39           531           554     3011   \n",
      "2          189.0  223.57  65.25           531           554     2987   \n",
      "3          196.0  184.01  65.75           531           554     2740   \n",
      "4          196.0  195.82  65.79           531           554     1075   \n",
      "\n",
      "   print_id  flow_rate_class  feed_rate_class  z_offset_class  hotend_class  \\\n",
      "0         0                1                2               2             0   \n",
      "1         0                1                2               2             1   \n",
      "2         0                1                2               2             2   \n",
      "3         0                0                1               2             0   \n",
      "4         0                2                0               1             1   \n",
      "\n",
      "    img_mean    img_std  \n",
      "0  53.545153  26.314393  \n",
      "1  71.590078  46.039682  \n",
      "2  49.681406  28.132891  \n",
      "3  52.976312  27.947203  \n",
      "4  55.345208  31.473371  \n",
      "\n",
      "First five rows of validation data:\n",
      "         img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "0  image-3080.jpg  2020-10-08T13:36:42-74         91        167      0.21   \n",
      "1  image-2996.jpg  2020-10-08T13:36:03-58         91        167      0.21   \n",
      "2  image-2993.jpg  2020-10-08T13:36:02-19         91        167      0.21   \n",
      "3   image-985.jpg  2020-10-08T13:20:26-88        200         20      0.00   \n",
      "4  image-1052.jpg  2020-10-08T13:20:58-00        200         20      0.00   \n",
      "\n",
      "   target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "0          189.0  184.28  64.99           531           554     3079   \n",
      "1          189.0  217.92  64.78           531           554     2995   \n",
      "2          189.0  221.61  64.95           531           554     2992   \n",
      "3          196.0  184.78  65.13           531           554      984   \n",
      "4          196.0  196.90  64.84           531           554     1051   \n",
      "\n",
      "   print_id  flow_rate_class  feed_rate_class  z_offset_class  hotend_class  \\\n",
      "0         0                1                2               2             0   \n",
      "1         0                1                2               2             1   \n",
      "2         0                1                2               2             2   \n",
      "3         0                2                0               1             0   \n",
      "4         0                2                0               1             1   \n",
      "\n",
      "    img_mean    img_std  \n",
      "0  47.114303  29.797352  \n",
      "1  70.690316  39.586707  \n",
      "2  47.821491  26.513528  \n",
      "3  38.823822  32.540429  \n",
      "4  37.773633  32.580234  \n",
      "\n",
      "First five rows of testing data:\n",
      "         img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "0  image-3022.jpg  2020-10-08T13:36:15-66         91        167      0.21   \n",
      "1  image-3004.jpg  2020-10-08T13:36:07-30         91        167      0.21   \n",
      "2  image-2989.jpg  2020-10-08T13:36:00-33         91        167      0.21   \n",
      "3  image-2757.jpg  2020-10-08T13:34:12-35         29        125      0.14   \n",
      "4  image-1070.jpg  2020-10-08T13:21:06-37        200         20      0.00   \n",
      "\n",
      "   target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "0          189.0  192.81  65.03           531           554     3021   \n",
      "1          189.0  213.87  64.57           531           554     3003   \n",
      "2          189.0  223.57  65.25           531           554     2988   \n",
      "3          196.0  188.80  65.42           531           554     2756   \n",
      "4          196.0  196.48  65.38           531           554     1069   \n",
      "\n",
      "   print_id  flow_rate_class  feed_rate_class  z_offset_class  hotend_class  \\\n",
      "0         0                1                2               2             0   \n",
      "1         0                1                2               2             1   \n",
      "2         0                1                2               2             2   \n",
      "3         0                0                1               2             0   \n",
      "4         0                2                0               1             1   \n",
      "\n",
      "    img_mean    img_std  \n",
      "0  63.496322  29.241519  \n",
      "1  64.661989  31.122375  \n",
      "2  52.298171  32.235433  \n",
      "3  44.160618  25.863315  \n",
      "4  49.150498  32.857476  \n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialise model, loss function, and optimiser",
   "id": "fa0984f0f7e78413"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:31:47.378685Z",
     "start_time": "2024-11-12T10:31:47.369748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = 3  # Number of hot end rate classes\n",
    "model = SimpleCNN(num_classes=num_classes)  # Assuming SimpleCNN is defined in cnn_models\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Cross Entropy Loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ],
   "id": "83d67216aa3206ad",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training, Validation and Testing batches",
   "id": "85bfbdc424b7a17a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:31:48.484959Z",
     "start_time": "2024-11-12T10:31:48.474076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file=None, root_dir=None, transform=None, data_frame=None):\n",
    "        if data_frame is not None:\n",
    "            self.data = data_frame\n",
    "        elif csv_file is not None:\n",
    "            self.data = pd.read_csv(csv_file, header=0, dtype=str)\n",
    "        else:\n",
    "            raise ValueError(\"Either csv_file or data_frame must be provided.\")\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or self.default_transform()\n",
    "        self.valid_indices = self.get_valid_indices()\n",
    "\n",
    "    def default_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def get_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "            img_name = self.data.iloc[idx, 0].strip()\n",
    "            img_name = img_name.split('/')[-1]\n",
    "    \n",
    "            if img_name.startswith(\"image-\"):\n",
    "                try:\n",
    "                    image_number = int(img_name.split('-')[1].split('.')[0])\n",
    "                    if image_number <= 3084:\n",
    "                        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "                        if os.path.exists(full_img_path):\n",
    "                            valid_indices.append(idx)\n",
    "                        else:\n",
    "                            print(f\"Image does not exist: {full_img_path}\")\n",
    "                except ValueError:\n",
    "                    print(f\"Invalid filename format for {img_name}. Skipping...\")\n",
    "        \n",
    "        print(f\"Total valid indices found: {len(valid_indices)}\")  # Debugging output\n",
    "        return valid_indices\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            # Debugging: Print length of valid indices\n",
    "            # print(f\"Valid indices count: {len(self.valid_indices)}\")\n",
    "    \n",
    "            items = [self._load_sample(i) for i in idx]\n",
    "            items = [item for item in items if item is not None]  # Filter out None entries\n",
    "    \n",
    "            if not items:\n",
    "                raise RuntimeError(\"No valid items found in the batch.\")\n",
    "    \n",
    "            # Unzip items and stack images\n",
    "            images, labels = zip(*items)\n",
    "            return torch.stack(images), torch.tensor(labels)\n",
    "    \n",
    "        else:\n",
    "            return self._load_sample(idx)\n",
    "\n",
    "\n",
    "    def _load_sample(self, idx):\n",
    "        # Get the actual index from valid indices\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        img_name = self.data.iloc[actual_idx, 0].strip()\n",
    "        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "    \n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')  # Ensure image is RGB\n",
    "            label_str = self.data.iloc[actual_idx, 15]  # Assuming label is in the second column\n",
    "            \n",
    "            # Attempt to convert label to integer; handle exceptions\n",
    "            try:\n",
    "                label = int(label_str)  # Try converting to int\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Non-integer label found for image {img_name}: {label_str}\")\n",
    "                print()\n",
    "                return None  # Skip this sample if label conversion fails\n",
    "    \n",
    "            image = self.transform(image)  # Apply transformation\n",
    "    \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {full_img_path}: {e}\")\n",
    "            return None  # Handle error gracefully"
   ],
   "id": "352d0462d9375674",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:31:50.008995Z",
     "start_time": "2024-11-12T10:31:49.615152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming CustomDataset is already defined earlier (you should have this from the previous code)\n",
    "\n",
    "class TemperatureBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.data_by_temperature = self.group_data_by_temperature()\n",
    "\n",
    "    def group_data_by_temperature(self):\n",
    "        \"\"\"Group indices by temperature.\"\"\"\n",
    "        data_by_temp = {}\n",
    "        for idx in range(len(self.dataset)):\n",
    "            temp = self.dataset.data.iloc[idx]['target_hotend']  # Get temperature for each image\n",
    "            if temp not in data_by_temp:\n",
    "                data_by_temp[temp] = []\n",
    "            data_by_temp[temp].append(idx)\n",
    "        return data_by_temp\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate batches of indices, where each batch corresponds to one temperature.\"\"\"\n",
    "        for temp, indices in self.data_by_temperature.items():\n",
    "            # Shuffle indices within each temperature group (optional)\n",
    "            random.shuffle(indices)\n",
    "            # Yield batches of the specified batch size for each temperature group\n",
    "            for i in range(0, len(indices), self.batch_size):\n",
    "                yield indices[i:i + self.batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches.\"\"\"\n",
    "        return sum([len(indices) // self.batch_size for indices in self.data_by_temperature.values()])\n",
    "\n",
    "\n",
    "# Initialize datasets for training, validation, and testing\n",
    "train_dataset = CustomDataset(data_frame=train_data, root_dir=root_dir, transform=transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "val_dataset = CustomDataset(data_frame=val_data, root_dir=root_dir, transform=transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "test_dataset = CustomDataset(data_frame=test_data, root_dir=root_dir, transform=transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "\n",
    "# Create custom batch samplers for training, validation, and testing data\n",
    "train_sampler = TemperatureBatchSampler(train_dataset, batch_size=3)\n",
    "val_sampler = TemperatureBatchSampler(val_dataset, batch_size=3)\n",
    "test_sampler = TemperatureBatchSampler(test_dataset, batch_size=3)\n",
    "\n",
    "# Create DataLoaders using the custom samplers (no batch_size here, as samplers handle it)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, sampler=val_sampler)\n",
    "test_loader = DataLoader(test_dataset, sampler=test_sampler)\n",
    "\n",
    "def print_class_distribution(data_loader, sampler, dataset):\n",
    "    \"\"\"Print the temperature and class distribution for all batches.\"\"\"\n",
    "    temperature_keys = list(sampler.data_by_temperature.keys())\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "        # Get the temperature for the current batch using the indices from the sampler\n",
    "        batch_indices = sampler.data_by_temperature[temperature_keys[batch_idx % len(temperature_keys)]]\n",
    "        \n",
    "        # Ensure we only take the temperature from the first index in this batch (they all share the same temperature)\n",
    "        temp = dataset.data.iloc[batch_indices[0]]['target_hotend']\n",
    "        \n",
    "        # Ensure labels are a 1D array and count occurrences of each class in the batch\n",
    "        labels = labels.view(-1).cpu().numpy()  # Flatten the labels if needed\n",
    "        \n",
    "        # Count the number of occurrences of each class in the batch\n",
    "        class_counts = Counter(labels)\n",
    "        \n",
    "        # Print the temperature and class distribution\n",
    "        print(f\"Batch {batch_idx + 1} contains images from temperature {temp}°\")\n",
    "        print(f\"Class distribution in this batch: {dict(class_counts)}\")\n",
    "\n",
    "# Printing the class distribution for all batches in the train, validation, and test data\n",
    "print(\"Training Data:\")\n",
    "print_class_distribution(train_loader, train_sampler, train_dataset)\n",
    "\n",
    "print(\"\\nValidation Data:\")\n",
    "print_class_distribution(val_loader, val_sampler, val_dataset)\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print_class_distribution(test_loader, test_sampler, test_dataset)\n"
   ],
   "id": "52f60939df636227",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 9/9 [00:00<00:00, 7694.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 9/9 [00:00<00:00, 4513.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 9/9 [00:00<00:00, 8463.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 9\n",
      "Training Data:\n",
      "Batch 1 contains images from temperature 189.0°\n",
      "Class distribution in this batch: {2: 1, 1: 1, 0: 1}\n",
      "Batch 2 contains images from temperature 196.0°\n",
      "Class distribution in this batch: {2: 1, 0: 1, 1: 1}\n",
      "Batch 3 contains images from temperature 206.0°\n",
      "Class distribution in this batch: {2: 1, 1: 1, 0: 1}\n",
      "\n",
      "Validation Data:\n",
      "Batch 1 contains images from temperature 189.0°\n",
      "Class distribution in this batch: {2: 1, 0: 1, 1: 1}\n",
      "Batch 2 contains images from temperature 196.0°\n",
      "Class distribution in this batch: {2: 1, 0: 1, 1: 1}\n",
      "Batch 3 contains images from temperature 206.0°\n",
      "Class distribution in this batch: {0: 1, 1: 1, 2: 1}\n",
      "\n",
      "Test Data:\n",
      "Batch 1 contains images from temperature 189.0°\n",
      "Class distribution in this batch: {1: 1, 2: 1, 0: 1}\n",
      "Batch 2 contains images from temperature 196.0°\n",
      "Class distribution in this batch: {0: 1, 1: 1, 2: 1}\n",
      "Batch 3 contains images from temperature 206.0°\n",
      "Class distribution in this batch: {2: 1, 1: 1, 0: 1}\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:32:42.349736Z",
     "start_time": "2024-11-12T10:32:37.224385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set batch size for training, validation, and testing\n",
    "batch_size = 3\n",
    "\n",
    "# Define DataLoader for training, validation, and testing datasets\n",
    "train_dataset = CustomDataset(data_frame=train_data, root_dir=root_dir, transform=None)\n",
    "val_dataset = CustomDataset(data_frame=val_data, root_dir=root_dir, transform=None)\n",
    "test_dataset = CustomDataset(data_frame=test_data, root_dir=root_dir, transform=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if CUDA (GPU) is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create and move the model to the device\n",
    "model = SimpleCNN(num_classes=3).to(device)\n",
    "\n",
    "# Initialize the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-Entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / len(labels)\n",
    "    return accuracy\n",
    "\n",
    "# Debugging: Print the structure of TemperatureBatchSampler\n",
    "print(\"Debug: TemperatureBatchSampler Data by Temperature:\")\n",
    "for temp_key, indices in train_sampler.data_by_temperature.items():\n",
    "    print(f\"Temperature {temp_key}° has {len(indices)} samples.\")\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=1000):\n",
    "    best_val_accuracy = 0  # Track the best validation accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Get temperature keys in order to cycle through them\n",
    "        temperature_keys = list(train_sampler.data_by_temperature.keys())\n",
    "\n",
    "        # Iterate over training batches\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Retrieve the temperature key for the current batch\n",
    "            temp_key_idx = batch_idx % len(temperature_keys)\n",
    "            temp_key = temperature_keys[temp_key_idx]\n",
    "\n",
    "            # Verify that the temperature key exists in the dictionary\n",
    "            if temp_key in train_sampler.data_by_temperature:\n",
    "                temp = train_dataset.data.iloc[train_sampler.data_by_temperature[temp_key][0]]['target_hotend']\n",
    "            else:\n",
    "                temp = \"Unknown\"\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate the loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the model weights\n",
    "\n",
    "            # Update running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += calculate_accuracy(outputs, labels)\n",
    "\n",
    "            # Extract predictions and true labels for printing\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class indices\n",
    "\n",
    "            # Print batch details\n",
    "            print(f\"\\nBatch {batch_idx + 1} from Temperature: {temp}°\")\n",
    "            print(f\"  Predictions: {predicted.cpu().numpy()}\")\n",
    "            print(f\"  Labels:      {labels.cpu().numpy()}\")\n",
    "            # print(f\"  Sample Outputs: \\n{outputs[:3].cpu().detach().numpy()}\")  # Show first 3 outputs for readability\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_train_accuracy = running_accuracy / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.2f}%\")\n",
    "\n",
    "        # Evaluate on validation set after each epoch\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Validation loop\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate the loss\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += calculate_accuracy(outputs, labels)\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_accuracy = running_accuracy / len(dataloader)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "# Test loop\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_accuracy = 0.0\n",
    "    with torch.no_grad():  # No gradients needed for testing\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            running_accuracy += calculate_accuracy(outputs, labels)\n",
    "\n",
    "    avg_accuracy = running_accuracy / len(test_loader)\n",
    "    print(f\"Test Accuracy: {avg_accuracy:.2f}%\")\n",
    "\n",
    "# Run the training loop\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=1000)\n",
    "test_model(trained_model, test_loader)"
   ],
   "id": "2dd16a83d569cf84",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 9/9 [00:00<00:00, 8861.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 9/9 [00:00<00:00, 9032.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 9/9 [00:00<00:00, 2972.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 9\n",
      "Debug: TemperatureBatchSampler Data by Temperature:\n",
      "Temperature 189.0° has 3 samples.\n",
      "Temperature 196.0° has 3 samples.\n",
      "Temperature 206.0° has 3 samples.\n",
      "\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:00<00:00,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 from Temperature: 189.0°\n",
      "  Predictions: [0 0 0]\n",
      "  Labels:      [2 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:00<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 2 from Temperature: 196.0°\n",
      "  Predictions: [1 0 1]\n",
      "  Labels:      [1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 3 from Temperature: 206.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [2 2 0]\n",
      "Epoch 1/1000 - Train Loss: 1.1220, Train Accuracy: 22.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 - Validation Loss: 1.1013, Validation Accuracy: 33.33%\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:00<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 from Temperature: 189.0°\n",
      "  Predictions: [0 0 1]\n",
      "  Labels:      [1 2 1]\n",
      "\n",
      "Batch 2 from Temperature: 196.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [2 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 3 from Temperature: 206.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [0 1 0]\n",
      "Epoch 2/1000 - Train Loss: 1.1092, Train Accuracy: 22.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000 - Validation Loss: 1.1008, Validation Accuracy: 33.33%\n",
      "\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:00<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 from Temperature: 189.0°\n",
      "  Predictions: [0 1 1]\n",
      "  Labels:      [1 0 2]\n",
      "\n",
      "Batch 2 from Temperature: 196.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [1 2 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 3 from Temperature: 206.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [0 1 2]\n",
      "Epoch 3/1000 - Train Loss: 1.1070, Train Accuracy: 22.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000 - Validation Loss: 1.1006, Validation Accuracy: 33.33%\n",
      "\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:00<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 from Temperature: 189.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [1 1 2]\n",
      "\n",
      "Batch 2 from Temperature: 196.0°"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:00<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Predictions: [0 1 1]\n",
      "  Labels:      [0 2 2]\n",
      "\n",
      "Batch 3 from Temperature: 206.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000 - Train Loss: 1.1001, Train Accuracy: 44.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000 - Validation Loss: 1.1005, Validation Accuracy: 33.33%\n",
      "\n",
      "Epoch 5/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:00<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 from Temperature: 189.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [0 2 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 2 from Temperature: 196.0°\n",
      "  Predictions: [1 1 1]\n",
      "  Labels:      [1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 133\u001B[0m\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_accuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    132\u001B[0m \u001B[38;5;66;03m# Run the training loop\u001B[39;00m\n\u001B[1;32m--> 133\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m test_model(trained_model, test_loader)\n",
      "Cell \u001B[1;32mIn[40], line 65\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m     64\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Zero the gradients\u001B[39;00m\n\u001B[1;32m---> 65\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m     66\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)  \u001B[38;5;66;03m# Calculate the loss\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\models\\cnn_models.py:35\u001B[0m, in \u001B[0;36mSimpleCNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 35\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Flatten\u001B[39;00m\n\u001B[0;32m     37\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(x)\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "514fc33e5d3d7366",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
