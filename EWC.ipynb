{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries",
   "id": "cf83250ee7f63eeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:41:42.625633Z",
     "start_time": "2025-03-01T12:41:21.731077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Sampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm  \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,\\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,\\\n",
    "    disk_usage_metrics\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training import EWC\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "from models.cnn_models import SimpleCNN\n",
    "\n",
    "\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)  # If using a GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ],
   "id": "b8061d6902ab803b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define filepaths as constant",
   "id": "41b9047ff69b1348"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:42:29.529665Z",
     "start_time": "2025-03-01T12:42:29.522048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define file paths as constants\n",
    "CSV_FILE_PATH = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'\n",
    "ROOT_DIR_PATH = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\\print24'\n",
    "\n",
    "csv_file = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'  # Path to the CSV file\n",
    "root_dir = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\\print24'  # Path to the image directory"
   ],
   "id": "1abad502d1ece263",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data into DataFrame and filter print24",
   "id": "cb52d1ca4b474151"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:42:40.549477Z",
     "start_time": "2025-03-01T12:42:30.618940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data into a DataFrame for easier processing\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Limit dataset to the images between row indices 454 and 7058 (inclusive)\n",
    "#data_limited = data.iloc[454:7059].reset_index(drop=True)\n",
    "\n",
    "# Filter the dataset to only include images containing \"print24\"\n",
    "data_filtered = data[data.iloc[:, 0].str.contains('print24', na=False)]\n",
    "\n",
    "# Update the first column to contain only the image filenames\n",
    "data_filtered.iloc[:, 0] = data_filtered.iloc[:, 0].str.replace(r'.*?/(image-\\d+\\.jpg)', r'\\1', regex=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"First rows of filtered DataFrame:\")\n",
    "print(data_filtered.head())\n",
    "\n",
    "# Display the last few rows of the updated DataFrame\n",
    "print(\"\\nLast rows of filtered DataFrame:\")\n",
    "print(data_filtered.tail())"
   ],
   "id": "a798b6f5b4adbf65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of filtered DataFrame:\n",
      "          img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "99496  image-4.jpg  2020-10-07T11:45:35-86        100        100       0.0   \n",
      "99497  image-5.jpg  2020-10-07T11:45:36-32        100        100       0.0   \n",
      "99498  image-6.jpg  2020-10-07T11:45:36-79        100        100       0.0   \n",
      "99499  image-7.jpg  2020-10-07T11:45:37-26        100        100       0.0   \n",
      "99500  image-8.jpg  2020-10-07T11:45:37-72        100        100       0.0   \n",
      "\n",
      "       target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "99496          205.0  204.86  64.83           654           560        3   \n",
      "99497          205.0  204.62  65.08           654           560        4   \n",
      "99498          205.0  204.62  65.08           654           560        5   \n",
      "99499          205.0  204.62  65.08           654           560        6   \n",
      "99500          205.0  204.62  65.08           654           560        7   \n",
      "\n",
      "       print_id  flow_rate_class  feed_rate_class  z_offset_class  \\\n",
      "99496        24                1                1               1   \n",
      "99497        24                1                1               1   \n",
      "99498        24                1                1               1   \n",
      "99499        24                1                1               1   \n",
      "99500        24                1                1               1   \n",
      "\n",
      "       hotend_class   img_mean    img_std  \n",
      "99496             1  21.047311  31.160557  \n",
      "99497             1  23.239277  32.133393  \n",
      "99498             1  23.686270  31.702140  \n",
      "99499             1  21.645111  31.329910  \n",
      "99500             1  20.883776  32.322521  \n",
      "\n",
      "Last rows of filtered DataFrame:\n",
      "               img_path               timestamp  flow_rate  feed_rate  \\\n",
      "120639  image-26633.jpg  2020-10-07T15:11:15-03        167         39   \n",
      "120640  image-26634.jpg  2020-10-07T15:11:15-48        167         39   \n",
      "120641  image-26635.jpg  2020-10-07T15:11:15-94        167         39   \n",
      "120642  image-26636.jpg  2020-10-07T15:11:16-40        167         39   \n",
      "120643  image-26637.jpg  2020-10-07T15:11:16-85        167         39   \n",
      "\n",
      "        z_offset  target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  \\\n",
      "120639     -0.02          226.0  226.00  65.05           654           560   \n",
      "120640     -0.02          226.0  226.00  65.05           654           560   \n",
      "120641     -0.02          226.0  226.00  65.05           654           560   \n",
      "120642     -0.02          226.0  226.00  65.05           654           560   \n",
      "120643     -0.02          226.0  226.25  64.94           654           560   \n",
      "\n",
      "        img_num  print_id  flow_rate_class  feed_rate_class  z_offset_class  \\\n",
      "120639    26632        24                2                0               1   \n",
      "120640    26633        24                2                0               1   \n",
      "120641    26634        24                2                0               1   \n",
      "120642    26635        24                2                0               1   \n",
      "120643    26636        24                2                0               1   \n",
      "\n",
      "        hotend_class    img_mean    img_std  \n",
      "120639             2   99.738226  71.471386  \n",
      "120640             2  102.395052  72.259859  \n",
      "120641             2  105.231595  74.336885  \n",
      "120642             2   97.592887  67.624012  \n",
      "120643             2   97.459404  67.672459  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysing the target hotend temperature column",
   "id": "5d0423ac0a8bc90c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:42:45.766407Z",
     "start_time": "2025-03-01T12:42:45.745677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract unique temperatures in the 'target_hotend' column and sort them\n",
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())  # Sort temperatures in ascending order\n",
    "\n",
    "# Calculate the full range of temperatures (min and max)\n",
    "temperature_min = data_filtered['target_hotend'].min()\n",
    "temperature_max = data_filtered['target_hotend'].max()\n",
    "\n",
    "# Print the unique temperatures (sorted), count, and full range\n",
    "print(\"\\nUnique target hotend temperatures in the dataset (sorted):\")\n",
    "print(unique_temperatures)\n",
    "print(f\"\\nNumber of unique target hotend temperatures: {len(unique_temperatures)}\")\n",
    "print(f\"Temperature range: {temperature_min} to {temperature_max}\")"
   ],
   "id": "f29c405cf341b9ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique target hotend temperatures in the dataset (sorted):\n",
      "[180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0]\n",
      "\n",
      "Number of unique target hotend temperatures: 51\n",
      "Temperature range: 180.0 to 230.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a random temperature sub list and new dataframes with equal class distribution",
   "id": "a22e7cd8879fcbeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:42:51.318342Z",
     "start_time": "2025-03-01T12:42:51.175511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract unique temperatures and sort them\n",
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())  # Sort temperatures in ascending order\n",
    "\n",
    "# Check if we have enough unique temperatures to select from\n",
    "if len(unique_temperatures) >= 50:\n",
    "    # Select the lowest and highest temperatures\n",
    "    temperature_min = unique_temperatures[0]\n",
    "    temperature_max = unique_temperatures[-1]\n",
    "\n",
    "    # Remove the lowest and highest temperatures from the unique temperatures list\n",
    "    remaining_temperatures = [temp for temp in unique_temperatures if temp != temperature_min and temp != temperature_max]\n",
    "\n",
    "    # Randomly select 40 other temperatures from the remaining ones\n",
    "    random_temperatures = random.sample(remaining_temperatures, 40)\n",
    "\n",
    "    # Add the random temperatures to the temperature_sublist\n",
    "    temperature_sublist = [temperature_min, temperature_max] + random_temperatures\n",
    "    \n",
    "    # Sort from lowest to highest hotend temperature\n",
    "    temperature_sublist = sorted(temperature_sublist)\n",
    "\n",
    "    # Print the temperature sublist\n",
    "    print(\"\\nTemperature sublist:\")\n",
    "    print(temperature_sublist)\n",
    "    \n",
    "    # Split into three experience groups\n",
    "    split_size = len(temperature_sublist) // 3\n",
    "    experience_1 = temperature_sublist[:split_size]  # First third\n",
    "    experience_2 = temperature_sublist[split_size:2*split_size]  # Second third\n",
    "    experience_3 = temperature_sublist[2*split_size:]  # Last third\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nExperience Group 1:\", experience_1)\n",
    "    print(\"\\nExperience Group 2:\", experience_2)\n",
    "    print(\"\\nExperience Group 3:\", experience_3)\n",
    "else:\n",
    "    print(\"Not enough unique temperatures to select from. At least 50 unique temperatures are required.\")\n",
    "    experience_1 = experience_2 = experience_3 = []\n",
    "\n",
    "# Initialize a dictionary to store DataFrames for each class per experience\n",
    "experience_datasets = {1: {}, 2: {}, 3: {}}\n",
    "\n",
    "# Iterate through the three experience groups\n",
    "for exp_id, experience_temps in enumerate([experience_1, experience_2, experience_3], start=1):\n",
    "    if not experience_temps:\n",
    "        print(f\"Skipping Experience {exp_id} due to insufficient temperatures.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing Experience {exp_id} with temperatures: {experience_temps}...\")\n",
    "\n",
    "    # Filter the dataset based on the current experience's temperature range\n",
    "    exp_data = data_filtered[data_filtered['target_hotend'].isin(experience_temps)]\n",
    "    \n",
    "    # Check if exp_data is empty after filtering\n",
    "    if exp_data.empty:\n",
    "        print(f\"No data found for Experience {exp_id} with temperatures {experience_temps}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create a dictionary to store class-wise data for this experience\n",
    "    class_datasets = {}\n",
    "\n",
    "    # Iterate through each class (0, 1, 2) and filter data\n",
    "    for class_id in [0, 1, 2]:\n",
    "        class_data = exp_data[exp_data['hotend_class'] == class_id]\n",
    "        \n",
    "        if class_data.empty:\n",
    "            print(f\"Warning: Class {class_id} in Experience {exp_id} has no data!\")\n",
    "        else:\n",
    "            class_datasets[class_id] = class_data\n",
    "            print(f\"Class {class_id} dataset size in Experience {exp_id}: {len(class_data)}\")\n",
    "\n",
    "    # Ensure that all classes have data before proceeding to balance\n",
    "    if len(class_datasets) != 3:\n",
    "        print(f\"Skipping Experience {exp_id} because one or more classes are missing data!\")\n",
    "        continue  # Skip processing this experience if any class has no data\n",
    "\n",
    "    # Find the smallest class size in this experience\n",
    "    min_class_size = min(len(class_datasets[class_id]) for class_id in class_datasets)\n",
    "    print(f\"Smallest class size in Experience {exp_id}: {min_class_size}\")\n",
    "\n",
    "    # Balance the dataset for this experience\n",
    "    balanced_data = []\n",
    "\n",
    "    for class_id in class_datasets:\n",
    "        class_data = class_datasets[class_id]\n",
    "        # Randomly sample 'min_class_size' images from the class data to balance class distribution\n",
    "        sampled_class_data = class_data.sample(n=min_class_size, random_state=42)  # Sample equally\n",
    "        balanced_data.append(sampled_class_data)\n",
    "\n",
    "    # Combine all class data for this experience into one balanced dataset\n",
    "    balanced_dataset = pd.concat(balanced_data).reset_index(drop=True)\n",
    "\n",
    "    # Shuffle the final balanced dataset\n",
    "    balanced_dataset = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Store the balanced dataset in the experience_datasets dictionary\n",
    "    experience_datasets[exp_id] = balanced_dataset\n",
    "\n",
    "    # Print summary for this experience\n",
    "    print(f\"\\nBalanced dataset size for Experience {exp_id}: {len(balanced_dataset)}\")\n",
    "    print(\"Number of images in each class after balancing:\")\n",
    "\n",
    "    for class_id in [0, 1, 2]:\n",
    "        class_count = len(balanced_dataset[balanced_dataset['hotend_class'] == class_id])\n",
    "        print(f\"Class {class_id}: {class_count} images\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "for exp_id in [1, 2, 3]:\n",
    "    if exp_id in experience_datasets:\n",
    "        print(f\"\\nFirst five rows of Experience {exp_id} dataset:\")\n",
    "        print(experience_datasets[exp_id].head())"
   ],
   "id": "47acde363e257d88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature sublist:\n",
      "[180.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 195.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 227.0, 228.0, 229.0, 230.0]\n",
      "\n",
      "Experience Group 1: [180.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 195.0]\n",
      "\n",
      "Experience Group 2: [197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 210.0, 211.0, 212.0, 213.0, 214.0]\n",
      "\n",
      "Experience Group 3: [215.0, 216.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 227.0, 228.0, 229.0, 230.0]\n",
      "\n",
      "Processing Experience 1 with temperatures: [180.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 195.0]...\n",
      "Class 0 dataset size in Experience 1: 5448\n",
      "Class 1 dataset size in Experience 1: 759\n",
      "Class 2 dataset size in Experience 1: 33\n",
      "Smallest class size in Experience 1: 33\n",
      "\n",
      "Balanced dataset size for Experience 1: 99\n",
      "Number of images in each class after balancing:\n",
      "Class 0: 33 images\n",
      "Class 1: 33 images\n",
      "Class 2: 33 images\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing Experience 2 with temperatures: [197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 210.0, 211.0, 212.0, 213.0, 214.0]...\n",
      "Class 0 dataset size in Experience 2: 691\n",
      "Class 1 dataset size in Experience 2: 5267\n",
      "Class 2 dataset size in Experience 2: 46\n",
      "Smallest class size in Experience 2: 46\n",
      "\n",
      "Balanced dataset size for Experience 2: 138\n",
      "Number of images in each class after balancing:\n",
      "Class 0: 46 images\n",
      "Class 1: 46 images\n",
      "Class 2: 46 images\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing Experience 3 with temperatures: [215.0, 216.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 227.0, 228.0, 229.0, 230.0]...\n",
      "Class 0 dataset size in Experience 3: 249\n",
      "Class 1 dataset size in Experience 3: 2848\n",
      "Class 2 dataset size in Experience 3: 2176\n",
      "Smallest class size in Experience 3: 249\n",
      "\n",
      "Balanced dataset size for Experience 3: 747\n",
      "Number of images in each class after balancing:\n",
      "Class 0: 249 images\n",
      "Class 1: 249 images\n",
      "Class 2: 249 images\n",
      "--------------------------------------------------\n",
      "\n",
      "First five rows of Experience 1 dataset:\n",
      "          img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "0  image-12253.jpg  2020-10-07T13:20:29-72        149        113      0.23   \n",
      "1   image-7151.jpg  2020-10-07T12:40:58-10         71        142      0.18   \n",
      "2   image-7142.jpg  2020-10-07T12:40:53-91         71        142      0.18   \n",
      "3  image-26268.jpg  2020-10-07T15:08:27-08        102         69      0.28   \n",
      "4   image-7149.jpg  2020-10-07T12:40:57-17         71        142      0.18   \n",
      "\n",
      "   target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "0          195.0  198.72  64.98           654           560    12252   \n",
      "1          185.0  216.51  66.07           654           560     7150   \n",
      "2          185.0  224.33  65.38           654           560     7141   \n",
      "3          186.0  185.91  64.80           654           560    26267   \n",
      "4          185.0  221.07  65.69           654           560     7148   \n",
      "\n",
      "   print_id  flow_rate_class  feed_rate_class  z_offset_class  hotend_class  \\\n",
      "0        24                2                1               2             1   \n",
      "1        24                0                2               2             1   \n",
      "2        24                0                2               2             2   \n",
      "3        24                1                0               2             0   \n",
      "4        24                0                2               2             2   \n",
      "\n",
      "    img_mean    img_std  \n",
      "0  62.608164  53.004668  \n",
      "1  69.050062  38.616382  \n",
      "2  65.622393  41.818446  \n",
      "3  65.496416  64.800241  \n",
      "4  65.601012  42.230603  \n",
      "\n",
      "First five rows of Experience 2 dataset:\n",
      "          img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "0    image-559.jpg  2020-10-07T11:49:53-80        139         58      0.20   \n",
      "1   image-7945.jpg  2020-10-07T12:47:07-03         65        192      0.18   \n",
      "2  image-16574.jpg  2020-10-07T13:53:54-99         92        199      0.12   \n",
      "3   image-6565.jpg  2020-10-07T12:36:25-81        176         47      0.01   \n",
      "4  image-22079.jpg  2020-10-07T14:36:14-39        106        193      0.30   \n",
      "\n",
      "   target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "0          211.0  199.15  65.65           654           560      558   \n",
      "1          205.0  221.07  64.76           654           560     7944   \n",
      "2          200.0  189.92  65.22           654           560    16573   \n",
      "3          199.0  193.44  65.06           654           560     6564   \n",
      "4          200.0  199.49  65.15           654           560    22078   \n",
      "\n",
      "   print_id  flow_rate_class  feed_rate_class  z_offset_class  hotend_class  \\\n",
      "0        24                2                0               2             1   \n",
      "1        24                0                2               2             2   \n",
      "2        24                1                2               2             0   \n",
      "3        24                2                0               1             0   \n",
      "4        24                1                2               2             1   \n",
      "\n",
      "    img_mean    img_std  \n",
      "0  36.002373  31.084220  \n",
      "1  40.100514  37.705981  \n",
      "2  55.746292  49.327787  \n",
      "3  60.743258  45.167865  \n",
      "4  92.355872  67.875308  \n",
      "\n",
      "First five rows of Experience 3 dataset:\n",
      "          img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "0  image-23305.jpg  2020-10-07T14:45:40-28         52         55      0.27   \n",
      "1  image-20599.jpg  2020-10-07T14:24:51-06         32        101      0.28   \n",
      "2   image-6351.jpg  2020-10-07T12:34:46-38        143         52      0.06   \n",
      "3  image-22431.jpg  2020-10-07T14:38:57-04         72         34      0.18   \n",
      "4    image-825.jpg  2020-10-07T11:51:57-40         49         92      0.08   \n",
      "\n",
      "   target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "0          230.0  194.06  65.42           654           560    23304   \n",
      "1          222.0  201.81  65.50           654           560    20598   \n",
      "2          219.0  194.80  65.36           654           560     6350   \n",
      "3          218.0  193.89  64.88           654           560    22430   \n",
      "4          223.0  212.27  65.01           654           560      824   \n",
      "\n",
      "   print_id  flow_rate_class  feed_rate_class  z_offset_class  hotend_class  \\\n",
      "0        24                0                0               2             0   \n",
      "1        24                0                1               2             1   \n",
      "2        24                2                0               1             0   \n",
      "3        24                0                0               2             0   \n",
      "4        24                0                1               2             1   \n",
      "\n",
      "     img_mean    img_std  \n",
      "0  112.033838  83.575780  \n",
      "1   87.447282  74.519651  \n",
      "2   69.921719  49.964794  \n",
      "3  101.589466  71.475478  \n",
      "4   47.823053  41.924315  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking the class distribution of all the experience datasets",
   "id": "32f7aecc5666b507"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:42:51.950124Z",
     "start_time": "2025-03-01T12:42:51.934222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Check if the experience dataset exists (in case an experience was skipped)\n",
    "    if exp_id in experience_datasets:\n",
    "        # Select only the 'img_path' and 'hotend_class' columns\n",
    "        balanced_dataset_filtered = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "\n",
    "        # Check the class distribution in the filtered dataset\n",
    "        class_distribution = balanced_dataset_filtered['hotend_class'].value_counts()\n",
    "        \n",
    "        # Print the class distribution for the current experience\n",
    "        print(f\"\\nClass distribution for Experience {exp_id}:\")\n",
    "        print(class_distribution)"
   ],
   "id": "84535699bd125895",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution for Experience 1:\n",
      "hotend_class\n",
      "1    33\n",
      "2    33\n",
      "0    33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution for Experience 2:\n",
      "hotend_class\n",
      "1    46\n",
      "2    46\n",
      "0    46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution for Experience 3:\n",
      "hotend_class\n",
      "0    249\n",
      "1    249\n",
      "2    249\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Printing the indices, the classes, and the number of images in each class",
   "id": "77f6b191f590e6f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:42:54.146486Z",
     "start_time": "2025-03-01T12:42:54.121978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Check if the experience dataset exists (in case an experience was skipped)\n",
    "    if exp_id in experience_datasets:\n",
    "        # Select only the 'img_path' and 'hotend_class' columns for the current experience dataset\n",
    "        balanced_dataset_filtered = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "\n",
    "        # Get the class distribution for the current experience dataset\n",
    "        class_distribution = balanced_dataset_filtered['hotend_class'].value_counts()\n",
    "        \n",
    "        # Step 1: Print the indices, the classes, and the number of images in each class\n",
    "        print(f\"\\n--- Experience {exp_id} ---\")\n",
    "        for class_label in class_distribution.index:\n",
    "            # Get all indices for the current class\n",
    "            class_indices = balanced_dataset_filtered[balanced_dataset_filtered['hotend_class'] == class_label].index.tolist()\n",
    "\n",
    "            # Count the number of images for the current class\n",
    "            num_images_in_class = len(class_indices)\n",
    "\n",
    "            # Print the details for this class\n",
    "            print(f\"\\nClass: {class_label} (Total images: {num_images_in_class})\")\n",
    "            print(\"Indices: \", class_indices)\n",
    "            print(f\"Number of images in class {class_label}: {num_images_in_class}\")\n",
    "\n",
    "        # Step 2: Get the number of unique classes\n",
    "        num_classes = len(class_distribution)\n",
    "\n",
    "        # Step 3: Set a small batch size\n",
    "        small_batch_size = 15  # You can change this to a value like 32, 64, etc.\n",
    "\n",
    "        # Step 4: Calculate the number of samples per class per batch\n",
    "        samples_per_class = small_batch_size // num_classes  # Ensure it's divisible\n",
    "\n",
    "        # Make sure we don't ask for more samples than available in the smallest class\n",
    "        samples_per_class = min(samples_per_class, class_distribution.min())\n",
    "\n",
    "        # Step 5: Calculate the total batch size\n",
    "        batch_size = samples_per_class * num_classes\n",
    "\n",
    "        print(f\"\\nRecommended Small Batch Size for Experience {exp_id}: {batch_size}\")\n",
    "        print(f\"Samples per class in Experience {exp_id}: {samples_per_class}\")\n",
    "        print(\"-\" * 50)  # To separate each experience's results"
   ],
   "id": "49fc331070b94ce1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experience 1 ---\n",
      "\n",
      "Class: 1 (Total images: 33)\n",
      "Indices:  [0, 1, 6, 7, 12, 14, 20, 24, 25, 29, 35, 37, 40, 42, 47, 52, 57, 59, 61, 62, 63, 65, 67, 70, 71, 72, 74, 78, 79, 81, 84, 94, 98]\n",
      "Number of images in class 1: 33\n",
      "\n",
      "Class: 2 (Total images: 33)\n",
      "Indices:  [2, 4, 5, 11, 18, 19, 21, 22, 27, 30, 33, 36, 39, 45, 54, 56, 58, 60, 64, 66, 68, 69, 73, 75, 77, 80, 88, 89, 90, 91, 92, 95, 97]\n",
      "Number of images in class 2: 33\n",
      "\n",
      "Class: 0 (Total images: 33)\n",
      "Indices:  [3, 8, 9, 10, 13, 15, 16, 17, 23, 26, 28, 31, 32, 34, 38, 41, 43, 44, 46, 48, 49, 50, 51, 53, 55, 76, 82, 83, 85, 86, 87, 93, 96]\n",
      "Number of images in class 0: 33\n",
      "\n",
      "Recommended Small Batch Size for Experience 1: 15\n",
      "Samples per class in Experience 1: 5\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Experience 2 ---\n",
      "\n",
      "Class: 1 (Total images: 46)\n",
      "Indices:  [0, 4, 5, 6, 9, 10, 12, 13, 18, 19, 30, 33, 34, 42, 50, 53, 55, 61, 62, 64, 65, 66, 68, 69, 70, 76, 77, 86, 92, 93, 95, 97, 99, 101, 103, 104, 105, 106, 109, 110, 113, 114, 121, 128, 129, 133]\n",
      "Number of images in class 1: 46\n",
      "\n",
      "Class: 2 (Total images: 46)\n",
      "Indices:  [1, 7, 8, 20, 22, 24, 25, 29, 32, 35, 36, 38, 39, 43, 44, 47, 51, 52, 56, 59, 73, 78, 79, 82, 87, 89, 90, 91, 94, 96, 98, 100, 107, 108, 112, 115, 118, 119, 125, 126, 127, 130, 131, 134, 136, 137]\n",
      "Number of images in class 2: 46\n",
      "\n",
      "Class: 0 (Total images: 46)\n",
      "Indices:  [2, 3, 11, 14, 15, 16, 17, 21, 23, 26, 27, 28, 31, 37, 40, 41, 45, 46, 48, 49, 54, 57, 58, 60, 63, 67, 71, 72, 74, 75, 80, 81, 83, 84, 85, 88, 102, 111, 116, 117, 120, 122, 123, 124, 132, 135]\n",
      "Number of images in class 0: 46\n",
      "\n",
      "Recommended Small Batch Size for Experience 2: 15\n",
      "Samples per class in Experience 2: 5\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Experience 3 ---\n",
      "\n",
      "Class: 0 (Total images: 249)\n",
      "Indices:  [0, 2, 3, 6, 7, 15, 18, 20, 26, 29, 34, 35, 40, 43, 45, 48, 53, 54, 55, 56, 57, 58, 61, 62, 65, 72, 75, 76, 79, 82, 84, 89, 93, 94, 96, 97, 99, 101, 102, 107, 116, 118, 119, 121, 123, 128, 129, 134, 136, 143, 146, 148, 149, 150, 151, 155, 157, 161, 162, 163, 165, 166, 167, 168, 171, 175, 176, 178, 179, 184, 185, 186, 190, 191, 192, 199, 201, 202, 204, 207, 208, 211, 216, 224, 227, 233, 237, 246, 247, 252, 253, 254, 261, 262, 263, 264, 267, 270, 275, 285, 287, 288, 294, 296, 301, 304, 305, 308, 310, 312, 313, 315, 316, 319, 320, 327, 328, 334, 343, 345, 348, 349, 352, 357, 362, 363, 366, 375, 377, 380, 381, 385, 388, 390, 391, 393, 399, 400, 404, 406, 409, 411, 413, 420, 424, 425, 433, 435, 438, 439, 440, 443, 450, 453, 454, 458, 460, 461, 462, 463, 469, 471, 473, 482, 485, 488, 490, 496, 499, 500, 503, 508, 509, 512, 513, 514, 517, 520, 522, 532, 536, 537, 544, 545, 547, 549, 553, 554, 555, 557, 560, 564, 576, 577, 580, 590, 603, 604, 605, 607, 609, 613, 615, 620, 624, 625, 629, 633, 634, 635, 636, 637, 639, 647, 648, 651, 654, 655, 656, 657, 660, 661, 662, 670, 671, 675, 678, 681, 682, 683, 688, 692, 693, 698, 704, 708, 713, 717, 720, 722, 729, 731, 733, 736, 738, 740, 742, 743, 746]\n",
      "Number of images in class 0: 249\n",
      "\n",
      "Class: 1 (Total images: 249)\n",
      "Indices:  [1, 4, 5, 9, 10, 11, 13, 14, 17, 22, 25, 30, 31, 32, 33, 37, 38, 39, 41, 44, 46, 47, 50, 51, 52, 64, 66, 67, 68, 69, 73, 78, 81, 83, 87, 88, 90, 98, 105, 106, 110, 112, 114, 115, 122, 124, 127, 133, 135, 138, 139, 141, 142, 147, 153, 154, 159, 164, 172, 173, 177, 180, 181, 187, 188, 189, 197, 198, 200, 205, 210, 212, 213, 214, 215, 218, 219, 220, 221, 223, 225, 226, 230, 232, 238, 242, 243, 249, 257, 260, 266, 268, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 290, 299, 300, 303, 307, 311, 318, 322, 325, 329, 330, 331, 332, 335, 336, 339, 341, 342, 344, 356, 358, 359, 360, 371, 372, 373, 378, 379, 383, 386, 387, 392, 394, 397, 402, 403, 405, 407, 412, 415, 416, 423, 426, 427, 432, 436, 445, 446, 447, 449, 451, 452, 456, 457, 459, 465, 466, 467, 476, 484, 486, 489, 492, 493, 504, 505, 510, 515, 523, 524, 525, 529, 530, 531, 535, 541, 542, 543, 550, 551, 552, 556, 558, 561, 569, 572, 574, 575, 579, 582, 583, 585, 586, 587, 588, 589, 593, 594, 617, 621, 622, 631, 640, 641, 642, 643, 644, 649, 658, 659, 663, 664, 665, 666, 667, 669, 673, 674, 677, 685, 686, 687, 689, 691, 694, 696, 697, 701, 710, 714, 716, 718, 719, 721, 723, 724, 725, 726, 727, 732, 734, 735, 737, 744, 745]\n",
      "Number of images in class 1: 249\n",
      "\n",
      "Class: 2 (Total images: 249)\n",
      "Indices:  [8, 12, 16, 19, 21, 23, 24, 27, 28, 36, 42, 49, 59, 60, 63, 70, 71, 74, 77, 80, 85, 86, 91, 92, 95, 100, 103, 104, 108, 109, 111, 113, 117, 120, 125, 126, 130, 131, 132, 137, 140, 144, 145, 152, 156, 158, 160, 169, 170, 174, 182, 183, 193, 194, 195, 196, 203, 206, 209, 217, 222, 228, 229, 231, 234, 235, 236, 239, 240, 241, 244, 245, 248, 250, 251, 255, 256, 258, 259, 265, 269, 271, 272, 289, 291, 292, 293, 295, 297, 298, 302, 306, 309, 314, 317, 321, 323, 324, 326, 333, 337, 338, 340, 346, 347, 350, 351, 353, 354, 355, 361, 364, 365, 367, 368, 369, 370, 374, 376, 382, 384, 389, 395, 396, 398, 401, 408, 410, 414, 417, 418, 419, 421, 422, 428, 429, 430, 431, 434, 437, 441, 442, 444, 448, 455, 464, 468, 470, 472, 474, 475, 477, 478, 479, 480, 481, 483, 487, 491, 494, 495, 497, 498, 501, 502, 506, 507, 511, 516, 518, 519, 521, 526, 527, 528, 533, 534, 538, 539, 540, 546, 548, 559, 562, 563, 565, 566, 567, 568, 570, 571, 573, 578, 581, 584, 591, 592, 595, 596, 597, 598, 599, 600, 601, 602, 606, 608, 610, 611, 612, 614, 616, 618, 619, 623, 626, 627, 628, 630, 632, 638, 645, 646, 650, 652, 653, 668, 672, 676, 679, 680, 684, 690, 695, 699, 700, 702, 703, 705, 706, 707, 709, 711, 712, 715, 728, 730, 739, 741]\n",
      "Number of images in class 2: 249\n",
      "\n",
      "Recommended Small Batch Size for Experience 3: 15\n",
      "Samples per class in Experience 3: 5\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## At this point a balanced dataset for each experience has been created",
   "id": "b6f62e66e01398b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create training, validation, and testing datasets",
   "id": "1be9c1eebf9fd709"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:42:56.417673Z",
     "start_time": "2025-03-01T12:42:56.351876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Check if the experience dataset exists (in case an experience was skipped)\n",
    "    if exp_id in experience_datasets:\n",
    "        # Select only the 'img_path' and 'hotend_class' columns for the current experience dataset\n",
    "        balanced_dataset_filtered = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "\n",
    "        # Number of images per class (this will be the same after balancing)\n",
    "        num_images_per_class = len(balanced_dataset_filtered) // 3  # Assuming there are 3 classes (0, 1, 2)\n",
    "\n",
    "        # Calculate the number of samples per class for train, validation, and test sets\n",
    "        train_size = int(0.7 * num_images_per_class)\n",
    "        valid_size = int(0.15 * num_images_per_class)\n",
    "        test_size = num_images_per_class - train_size - valid_size\n",
    "\n",
    "        # Lists to hold indices for each class's dataset (train, validation, test)\n",
    "        train_indices, valid_indices, test_indices = [], [], []\n",
    "\n",
    "        # Split the data by class (assuming classes are 0, 1, 2)\n",
    "        for class_label in [0, 1, 2]:\n",
    "            class_data = balanced_dataset_filtered[balanced_dataset_filtered['hotend_class'] == class_label].index.tolist()\n",
    "\n",
    "            # Shuffle the indices of the current class\n",
    "            random.shuffle(class_data)\n",
    "\n",
    "            # Split the indices for each class into train, validation, and test\n",
    "            train_indices.extend(class_data[:train_size])\n",
    "            valid_indices.extend(class_data[train_size:train_size + valid_size])\n",
    "            test_indices.extend(class_data[train_size + valid_size:])\n",
    "\n",
    "        # Sort the indices to ensure consistent processing\n",
    "        train_indices, valid_indices, test_indices = sorted(train_indices), sorted(valid_indices), sorted(test_indices)\n",
    "\n",
    "        # Create DataFrames for train, validation, and test sets based on the indices\n",
    "        globals()[f'train_{exp_id}'] = balanced_dataset_filtered.loc[train_indices].reset_index(drop=True)\n",
    "        globals()[f'valid_{exp_id}'] = balanced_dataset_filtered.loc[valid_indices].reset_index(drop=True)\n",
    "        globals()[f'test_{exp_id}'] = balanced_dataset_filtered.loc[test_indices].reset_index(drop=True)\n",
    "\n",
    "        # Count class distribution for each of the datasets\n",
    "        def count_class_distribution(indices):\n",
    "            class_counts = [0, 0, 0]  # Assuming 3 classes (0, 1, 2)\n",
    "            for index in indices:\n",
    "                class_label = balanced_dataset_filtered.loc[index, 'hotend_class']\n",
    "                class_counts[class_label] += 1\n",
    "            return class_counts\n",
    "\n",
    "        # Count class distribution for each of the datasets\n",
    "        train_class_distribution = count_class_distribution(train_indices)\n",
    "        valid_class_distribution = count_class_distribution(valid_indices)\n",
    "        test_class_distribution = count_class_distribution(test_indices)\n",
    "\n",
    "        # Print the class distribution and dataset sizes\n",
    "        print(f\"\\n--- Experience {exp_id} ---\")\n",
    "        print(f\"Train set size: {len(train_indices)} | Class distribution: {train_class_distribution}\")\n",
    "        print(f\"Validation set size: {len(valid_indices)} | Class distribution: {valid_class_distribution}\")\n",
    "        print(f\"Test set size: {len(test_indices)} | Class distribution: {test_class_distribution}\")\n",
    "\n",
    "        print(f\"Experience {exp_id} datasets created successfully!\\n\")\n",
    "\n",
    "# Now, the datasets are directly available as:\n",
    "# train_1, valid_1, test_1, train_2, valid_2, test_2, train_3, valid_3, test_3"
   ],
   "id": "cb6e91a94bc66fc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Experience 1 ---\n",
      "Train set size: 69 | Class distribution: [23, 23, 23]\n",
      "Validation set size: 12 | Class distribution: [4, 4, 4]\n",
      "Test set size: 18 | Class distribution: [6, 6, 6]\n",
      "Experience 1 datasets created successfully!\n",
      "\n",
      "\n",
      "--- Experience 2 ---\n",
      "Train set size: 96 | Class distribution: [32, 32, 32]\n",
      "Validation set size: 18 | Class distribution: [6, 6, 6]\n",
      "Test set size: 24 | Class distribution: [8, 8, 8]\n",
      "Experience 2 datasets created successfully!\n",
      "\n",
      "\n",
      "--- Experience 3 ---\n",
      "Train set size: 522 | Class distribution: [174, 174, 174]\n",
      "Validation set size: 111 | Class distribution: [37, 37, 37]\n",
      "Test set size: 114 | Class distribution: [38, 38, 38]\n",
      "Experience 3 datasets created successfully!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check for Missing or Invalid Labels in Training, Validation, and Test Data",
   "id": "9c6d9800330d9cf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:39.219867Z",
     "start_time": "2025-03-01T12:43:39.193296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for any missing labels or invalid labels\n",
    "print(train_1['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(train_1['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(train_2['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(train_2['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(train_3['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(train_3['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(valid_1['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(valid_1['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(valid_2['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(valid_2['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(valid_3['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(valid_3['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(test_1['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(test_1['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(test_2['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(test_2['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(test_3['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(test_3['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values"
   ],
   "id": "9013740cba3d35b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1 0 2]\n",
      "0\n",
      "[1 0 2]\n",
      "0\n",
      "[0 1 2]\n",
      "0\n",
      "[1 0 2]\n",
      "0\n",
      "[1 0 2]\n",
      "0\n",
      "[0 1 2]\n",
      "0\n",
      "[2 0 1]\n",
      "0\n",
      "[2 1 0]\n",
      "0\n",
      "[1 2 0]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Balanced Dataset class",
   "id": "8cd987e605164710"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:40.081425Z",
     "start_time": "2025-03-01T12:43:40.061642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the dataset class\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None):\n",
    "        self.data = data_frame\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Validate that the images exist in the directory\n",
    "        self.valid_indices = self.get_valid_indices()\n",
    "\n",
    "    def get_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "            img_name = self.data.iloc[idx, 0].strip()\n",
    "            img_name = img_name.split('/')[-1]  # Extract file name\n",
    "            \n",
    "            if img_name.startswith(\"image-\"):\n",
    "                try:\n",
    "                    # Ensure we only include images in the valid range\n",
    "                    image_number = int(img_name.split('-')[1].split('.')[0])\n",
    "                    if 4 <= image_number <= 26637:\n",
    "                        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "                        if os.path.exists(full_img_path):\n",
    "                            valid_indices.append(idx)\n",
    "                        else:\n",
    "                            print(f\"Image does not exist: {full_img_path}\")\n",
    "                except ValueError:\n",
    "                    print(f\"Invalid filename format for {img_name}. Skipping...\")\n",
    "        \n",
    "        print(f\"Total valid indices found: {len(valid_indices)}\")  # Debugging output\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Wrap around the index if it exceeds the length of valid indices\n",
    "        idx = idx % len(self.valid_indices)\n",
    "        \n",
    "        # Get the actual index from valid indices\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        img_name = self.data.iloc[actual_idx, 0].strip()\n",
    "        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "        label = self.targets[actual_idx]  # Get the label from the targets tensor\n",
    "    \n",
    "        try:\n",
    "            # Attempt to open the image and convert to RGB\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "    \n",
    "            # Apply transformations if defined\n",
    "            if self._transform_groups.get('train'):\n",
    "                image = self._transform_groups['train'](image)\n",
    "    \n",
    "            return image, label, task_label  # Return image, label, and task label\n",
    "        except (OSError, IOError, ValueError) as e:\n",
    "            # Print error message for debugging\n",
    "            print(f\"Error loading image {full_img_path}: {e}\")\n",
    "    \n",
    "            # Handle gracefully by skipping the corrupted/missing file\n",
    "            return self.__getitem__((idx + 1) % len(self.valid_indices))  # Try next valid index\n"
   ],
   "id": "f9b871fcd654111f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Balanced Batch Sampler class",
   "id": "f288ab510f111651"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:41.133093Z",
     "start_time": "2025-03-01T12:43:41.115440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, data_frame, batch_size=15, samples_per_class=5):\n",
    "        \"\"\"\n",
    "        data_frame: Pandas DataFrame with image paths and their respective class labels.\n",
    "        batch_size: Total batch size.\n",
    "        samples_per_class: Number of samples to draw from each class per batch.\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.batch_size = batch_size\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.num_classes = len(data_frame['hotend_class'].unique())\n",
    "        \n",
    "        if self.batch_size % self.num_classes != 0:\n",
    "            raise ValueError(\"Batch size must be divisible by the number of classes.\")\n",
    "\n",
    "        self.class_indices = {\n",
    "            class_id: self.data_frame[self.data_frame['hotend_class'] == class_id].index.tolist()\n",
    "            for class_id in self.data_frame['hotend_class'].unique()\n",
    "        }\n",
    "        \n",
    "        # Shuffle class indices initially\n",
    "        for class_id in self.class_indices:\n",
    "            random.shuffle(self.class_indices[class_id])\n",
    "\n",
    "        self.num_samples_per_epoch = sum(len(indices) for indices in self.class_indices.values())\n",
    "        self.indices_used = {class_id: [] for class_id in self.class_indices}\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = []\n",
    "\n",
    "        # Replenish indices for each class\n",
    "        for class_id in self.class_indices:\n",
    "            if not self.class_indices[class_id]:\n",
    "                raise ValueError(f\"Class {class_id} has no samples. Cannot form balanced batches.\")\n",
    "\n",
    "            # Shuffle and use all indices from this class\n",
    "            self.indices_used[class_id] = self.class_indices[class_id].copy()\n",
    "            random.shuffle(self.indices_used[class_id])\n",
    "\n",
    "        # Generate balanced batches\n",
    "        while len(batches) * self.batch_size < self.num_samples_per_epoch:\n",
    "            batch = []\n",
    "            for class_id in self.indices_used:\n",
    "                if len(self.indices_used[class_id]) < self.samples_per_class:\n",
    "                    # If a class runs out of samples, reshuffle and replenish\n",
    "                    self.indices_used[class_id] = self.class_indices[class_id].copy()\n",
    "                    random.shuffle(self.indices_used[class_id])\n",
    "\n",
    "                # Take `samples_per_class` indices from the current class\n",
    "                batch.extend(self.indices_used[class_id][:self.samples_per_class])\n",
    "                self.indices_used[class_id] = self.indices_used[class_id][self.samples_per_class:]\n",
    "\n",
    "            # Shuffle the batch and append\n",
    "            random.shuffle(batch)\n",
    "            batches.append(batch)\n",
    "\n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of batches per epoch\n",
    "        return self.num_samples_per_epoch // self.batch_size"
   ],
   "id": "d5d08879cfc9d799",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:45.534796Z",
     "start_time": "2025-03-01T12:43:44.321727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a dictionary to store datasets and DataLoaders\n",
    "datasets = {}\n",
    "dataloaders = {}\n",
    "\n",
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Ensure the dataset exists\n",
    "    if f\"train_{exp_id}\" in globals():\n",
    "        train_data = globals()[f\"train_{exp_id}\"]\n",
    "        val_data = globals()[f\"valid_{exp_id}\"]\n",
    "        test_data = globals()[f\"test_{exp_id}\"]\n",
    "\n",
    "        # Create dataset instances\n",
    "        datasets[f\"train_{exp_id}\"] = BalancedDataset(data_frame=train_data, root_dir=root_dir)\n",
    "        datasets[f\"valid_{exp_id}\"] = BalancedDataset(data_frame=val_data, root_dir=root_dir)\n",
    "        datasets[f\"test_{exp_id}\"] = BalancedDataset(data_frame=test_data, root_dir=root_dir)\n",
    "\n",
    "        # Create batch samplers for balanced training\n",
    "        train_sampler = BalancedBatchSampler(data_frame=train_data, batch_size=15, samples_per_class=5)\n",
    "        val_sampler = BalancedBatchSampler(data_frame=val_data, batch_size=15, samples_per_class=5)\n",
    "        test_sampler = BalancedBatchSampler(data_frame=test_data, batch_size=15, samples_per_class=5)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        dataloaders[f\"train_{exp_id}\"] = DataLoader(datasets[f\"train_{exp_id}\"], batch_sampler=train_sampler, shuffle=False)\n",
    "        dataloaders[f\"valid_{exp_id}\"] = DataLoader(datasets[f\"valid_{exp_id}\"], batch_sampler=val_sampler, shuffle=False)\n",
    "        dataloaders[f\"test_{exp_id}\"] = DataLoader(datasets[f\"test_{exp_id}\"], batch_sampler=test_sampler)\n",
    "\n",
    "        # Print dataset lengths\n",
    "        print(f\"   Experience {exp_id} datasets and DataLoaders created successfully!\")\n",
    "        print(f\"   Train dataset length: {len(datasets[f'train_{exp_id}'])}\")\n",
    "        print(f\"   Validation dataset length: {len(datasets[f'valid_{exp_id}'])}\")\n",
    "        print(f\"   Test dataset length: {len(datasets[f'test_{exp_id}'])}\")\n"
   ],
   "id": "d19ddec286b9fbfc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 69/69 [00:00<00:00, 975.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 12/12 [00:00<00:00, 928.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 18/18 [00:00<00:00, 1182.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 18\n",
      "   Experience 1 datasets and DataLoaders created successfully!\n",
      "   Train dataset length: 69\n",
      "   Validation dataset length: 12\n",
      "   Test dataset length: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 96/96 [00:00<00:00, 984.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 18/18 [00:00<00:00, 903.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 24/24 [00:00<00:00, 933.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 24\n",
      "   Experience 2 datasets and DataLoaders created successfully!\n",
      "   Train dataset length: 96\n",
      "   Validation dataset length: 18\n",
      "   Test dataset length: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 522/522 [00:00<00:00, 895.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 111/111 [00:00<00:00, 1153.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 114/114 [00:00<00:00, 1215.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid indices found: 114\n",
      "   Experience 3 datasets and DataLoaders created successfully!\n",
      "   Train dataset length: 522\n",
      "   Validation dataset length: 111\n",
      "   Test dataset length: 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting up a new folder for each experiment",
   "id": "6781c72ea237b19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:46.462603Z",
     "start_time": "2025-03-01T12:43:46.428338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set base directory\n",
    "base_dir = \"experiments\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Function to get the next experiment folder\n",
    "def get_experiment_folder(exp_num):\n",
    "    return os.path.join(base_dir, f\"Experiment_{exp_num:02d}\")  # Keeps two-digit format (01, 02, ..., 10)\n",
    "\n",
    "# Set initial experiment number\n",
    "experiment_num = 1\n",
    "experiment_folder = get_experiment_folder(experiment_num)\n",
    "\n",
    "# Create the main experiment directory if it doesn't exist\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "# Set model path inside experiment folder\n",
    "model_path = os.path.join(experiment_folder, \"best_model.pth\")\n",
    "\n",
    "# Create subdirectories for training, validation, and test confusion matrices\n",
    "train_folder = os.path.join(experiment_folder, \"training_confusion_matrices\")\n",
    "val_folder = os.path.join(experiment_folder, \"validation_confusion_matrices\")\n",
    "test_folder = os.path.join(experiment_folder, \"test_confusion_matrices\")\n",
    "\n",
    "# Ensure that the subdirectories exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Print the directory where results will be saved\n",
    "print(f\"Saving results to: {experiment_folder}\")"
   ],
   "id": "1a67c960943dfe8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to: experiments\\Experiment_01\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Display a Random Image from the Dataset with Its Label",
   "id": "a021cfa5257c953e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:55.553049Z",
     "start_time": "2025-03-01T12:43:47.761717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_random_image_from_experiment(exp_id, dataset_type):\n",
    "    \"\"\"\n",
    "    Selects a random image from the specified dataset (train, valid, or test) for a given experience ID,\n",
    "    loads it, displays it, and saves it to the corresponding experiment folder.\n",
    "\n",
    "    Args:\n",
    "        exp_id (int): The experience group number (1, 2, or 3).\n",
    "        dataset_type (str): The dataset type - 'train', 'valid', or 'test'.\n",
    "    \"\"\"\n",
    "    # Ensure the dataset exists\n",
    "    dataset_key = f\"{dataset_type}_{exp_id}\"  # Example: 'train_1', 'valid_2', 'test_3'\n",
    "    if dataset_key not in datasets:\n",
    "        print(f\"Dataset {dataset_key} not found!\")\n",
    "        return\n",
    "\n",
    "    dataset = datasets[dataset_key]  # Retrieve the dataset\n",
    "    data_frame = dataset.data  # Get the underlying DataFrame\n",
    "\n",
    "    # Ensure the dataset is not empty\n",
    "    if data_frame.empty:\n",
    "        print(f\"Dataset {dataset_key} is empty!\")\n",
    "        return\n",
    "\n",
    "    # Select a random index\n",
    "    random_index = random.choice(data_frame.index)\n",
    "    img_path = os.path.join(root_dir, data_frame.iloc[random_index, 0].strip())\n",
    "    label = data_frame.loc[random_index, 'hotend_class']\n",
    "\n",
    "    # Load and display the image\n",
    "    img = plt.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Label: {label}\")\n",
    "\n",
    "    # Define the path to save the image inside the current experiment folder\n",
    "    experiment_folder = os.path.join(\"experiments\", f\"experiment_{exp_id}\")\n",
    "    os.makedirs(experiment_folder, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "    output_path = os.path.join(experiment_folder, f\"random_{dataset_type}.png\")\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(output_path)\n",
    "    plt.clf()  # Clear the plot to avoid overlaps\n",
    "\n",
    "    print(f\"Image saved to: {output_path}\")\n",
    "\n",
    "# Example Usage:\n",
    "save_random_image_from_experiment(exp_id=1, dataset_type='train')  # Random training image from Experience 1\n",
    "save_random_image_from_experiment(exp_id=2, dataset_type='valid')  # Random validation image from Experience 2\n",
    "save_random_image_from_experiment(exp_id=3, dataset_type='test')   # Random test image from Experience 3"
   ],
   "id": "e1236742af8c2373",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to: experiments\\experiment_1\\random_train.png\n",
      "Image saved to: experiments\\experiment_2\\random_valid.png\n",
      "Image saved to: experiments\\experiment_3\\random_test.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:55.587675Z",
     "start_time": "2025-03-01T12:43:55.558763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iterate over all experience groups\n",
    "for exp_id in [1, 2, 3]:  \n",
    "    dataset_key = f\"train_{exp_id}\"  # e.g., 'train_1', 'train_2', 'train_3'\n",
    "    \n",
    "    # Ensure the dataset exists\n",
    "    if dataset_key in datasets:\n",
    "        data_frame = datasets[dataset_key].data  # Access the DataFrame from BalancedDataset\n",
    "\n",
    "        # Ensure the dataset is not empty\n",
    "        if not data_frame.empty:\n",
    "            # First image\n",
    "            first_index = data_frame.index[0]\n",
    "            first_image = data_frame.loc[first_index, 'img_path']\n",
    "            first_label = data_frame.loc[first_index, 'hotend_class']\n",
    "            print(f\"Experience {exp_id} - First Image Path: {first_image}, First Label: {first_label}\")\n",
    "\n",
    "            # Last image\n",
    "            last_index = data_frame.index[-1]\n",
    "            last_image = data_frame.loc[last_index, 'img_path']\n",
    "            last_label = data_frame.loc[last_index, 'hotend_class']\n",
    "            print(f\"Experience {exp_id} - Last Image Path: {last_image}, Last Label: {last_label}\\n\")\n",
    "        else:\n",
    "            print(f\"Experience {exp_id} - Training dataset is empty!\\n\")\n",
    "    else:\n",
    "        print(f\"Experience {exp_id} - Training dataset not found!\\n\")"
   ],
   "id": "791d1d863779a6a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience 1 - First Image Path: image-12253.jpg, First Label: 1\n",
      "Experience 1 - Last Image Path: image-5677.jpg, Last Label: 2\n",
      "\n",
      "Experience 2 - First Image Path: image-559.jpg, First Label: 1\n",
      "Experience 2 - Last Image Path: image-522.jpg, Last Label: 2\n",
      "\n",
      "Experience 3 - First Image Path: image-23305.jpg, First Label: 0\n",
      "Experience 3 - Last Image Path: image-15003.jpg, Last Label: 0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating an EWC Class which inherits from AvalancheDataset and contains all the expected functions",
   "id": "ef63a9c0fbec3354"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:43:59.356803Z",
     "start_time": "2025-03-01T12:43:59.299661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from avalanche.benchmarks.utils import AvalancheDataset, DataAttribute\n",
    "from avalanche.benchmarks.utils.transforms import TupleTransform\n",
    "\n",
    "class EWCCompatibleBalancedDataset(AvalancheDataset):\n",
    "    def __init__(self, data_frame, root_dir=None, transform=None, task_label=0, indices=None):\n",
    "        \"\"\"\n",
    "        Custom dataset compatible with EWC that inherits from AvalancheDataset.\n",
    "        It loads images from disk, applies transforms, and provides sample-wise\n",
    "        attributes for targets and task labels.\n",
    "        \n",
    "        Args:\n",
    "            data_frame (pd.DataFrame or list): If a DataFrame, it must contain columns\n",
    "                'image_path' and 'hotend_class'. If a list, it is assumed to be a pre-built\n",
    "                list of datasets (used in subset calls).\n",
    "            root_dir (str, optional): Directory where images are stored. Must be provided if data_frame is a DataFrame.\n",
    "            transform (callable, optional): Transformations to apply.\n",
    "            task_label (int, optional): Task label for continual learning.\n",
    "            indices (Sequence[int], optional): Optional indices for subsetting.\n",
    "        \"\"\"\n",
    "        # If data_frame is a list, assume this is a call from subset() and forward the call.\n",
    "        if isinstance(data_frame, list):\n",
    "            super().__init__(data_frame, indices=indices)\n",
    "            return\n",
    "\n",
    "        # Otherwise, data_frame is a DataFrame. Ensure root_dir is provided.\n",
    "        if root_dir is None:\n",
    "            raise ValueError(\"root_dir must be provided when data_frame is a DataFrame\")\n",
    "        \n",
    "        # Reset DataFrame index for consistency.\n",
    "        self.data = data_frame.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.task_label = task_label\n",
    "\n",
    "        # Define a default transform if none provided.\n",
    "        default_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # Wrap the transform in TupleTransform so that it applies only to the image element.\n",
    "        self._transform_groups = {\n",
    "            \"train\": TupleTransform([transform or default_transform]),\n",
    "            \"eval\": TupleTransform([transform or default_transform])\n",
    "        }\n",
    "        \n",
    "        # Ensure required columns exist.\n",
    "        if 'hotend_class' not in self.data.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'hotend_class' for labels.\")\n",
    "        if 'image_path' not in self.data.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'image_path' for image paths.\")\n",
    "        \n",
    "        # Validate image paths and obtain valid indices.\n",
    "        valid_indices = self.get_valid_indices()\n",
    "        if len(valid_indices) == 0:\n",
    "            raise ValueError(\"No valid image paths found.\")\n",
    "        \n",
    "        # Compute targets and task labels for valid samples.\n",
    "        targets_data = torch.tensor(self.data.loc[valid_indices, 'hotend_class'].values)\n",
    "        targets_task_labels_data = torch.full_like(targets_data, self.task_label)\n",
    "        \n",
    "        # Prepare sample entries (one per valid image).\n",
    "        samples = []\n",
    "        for idx in valid_indices:\n",
    "            img_name = self.data.loc[idx, 'image_path'].strip()\n",
    "            full_img_path = os.path.join(self.root_dir, img_name)\n",
    "            label = int(self.data.loc[idx, 'hotend_class'])\n",
    "            samples.append({\n",
    "                \"img_path\": full_img_path,\n",
    "                \"label\": label,\n",
    "                \"task_label\": self.task_label\n",
    "            })\n",
    "        \n",
    "        # Define an internal basic dataset that loads images.\n",
    "        class BasicDataset(Dataset):\n",
    "            def __init__(self, samples):\n",
    "                self.samples = samples\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.samples)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                sample = self.samples[idx]\n",
    "                img_path = sample[\"img_path\"]\n",
    "                try:\n",
    "                    # Load the image (ensure it is a PIL image).\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "                    # If an error occurs, try the next sample.\n",
    "                    return self.__getitem__((idx + 1) % len(self.samples))\n",
    "                return image, sample[\"label\"], sample[\"task_label\"]\n",
    "        \n",
    "        basic_dataset = BasicDataset(samples)\n",
    "        \n",
    "        # Create data attributes.\n",
    "        data_attributes = [\n",
    "            DataAttribute(targets_data, name=\"targets\", use_in_getitem=True),\n",
    "            DataAttribute(targets_task_labels_data, name=\"targets_task_labels\", use_in_getitem=True)\n",
    "        ]\n",
    "        \n",
    "        # IMPORTANT: Pass the basic_dataset inside a list so that AvalancheDataset\n",
    "        # correctly sets up its internal flat data, and forward the indices parameter.\n",
    "        super().__init__(\n",
    "            [basic_dataset],\n",
    "            data_attributes=data_attributes,\n",
    "            transform_groups=self._transform_groups,\n",
    "            indices=indices\n",
    "        )\n",
    "    \n",
    "    def get_valid_indices(self):\n",
    "        \"\"\"Return indices for which the image file exists.\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "            img_name = self.data.loc[idx, 'image_path'].strip()\n",
    "            full_img_path = os.path.join(self.root_dir, img_name)\n",
    "            if os.path.exists(full_img_path):\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                print(f\"Image does not exist: {full_img_path}\")\n",
    "        print(f\"Total valid images: {len(valid_indices)}\")\n",
    "        return valid_indices"
   ],
   "id": "8a080a958d713300",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating training, validation and testing datasets to implement EWC",
   "id": "1b3bca2f633ddf40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:01.263084Z",
     "start_time": "2025-03-01T12:44:00.649559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the transformation (e.g., normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Experience 1\n",
    "train_dataset_exp1 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=train_1.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "val_dataset_exp1 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=valid_1.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "test_dataset_exp1 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=test_1.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "# Experience 2\n",
    "train_dataset_exp2 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=train_2.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "val_dataset_exp2 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=valid_2.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "test_dataset_exp2 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=test_2.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "# Experience 3\n",
    "train_dataset_exp3 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=train_3.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "val_dataset_exp3 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=valid_3.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")\n",
    "\n",
    "test_dataset_exp3 = EWCCompatibleBalancedDataset(\n",
    "    data_frame=test_3.rename(columns={'img_path': 'image_path', 'class': 'hotend_class'}),\n",
    "    root_dir=root_dir,\n",
    "    transform=transform,\n",
    "    task_label=0\n",
    ")"
   ],
   "id": "45de73a63067e725",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 69/69 [00:00<00:00, 3866.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 12/12 [00:00<00:00, 2931.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 18/18 [00:00<00:00, 4445.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 96/96 [00:00<00:00, 6586.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 18/18 [00:00<00:00, 3032.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 24/24 [00:00<00:00, 2438.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 522/522 [00:00<00:00, 5030.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 111/111 [00:00<00:00, 5424.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating images: 100%|██████████| 114/114 [00:00<00:00, 4987.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images: 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating Dataloaders for more efficient data processing",
   "id": "f5c1108be69f60e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:03.089534Z",
     "start_time": "2025-03-01T12:44:03.012773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# Experience 1\n",
    "train_sampler_exp1 = BalancedBatchSampler(data_frame=train_1.rename(columns={'img_path': 'image_path'}), \n",
    "                                          batch_size=15, samples_per_class=5)\n",
    "val_sampler_exp1 = BalancedBatchSampler(data_frame=valid_1.rename(columns={'img_path': 'image_path'}), \n",
    "                                        batch_size=15, samples_per_class=5)\n",
    "test_sampler_exp1 = BalancedBatchSampler(data_frame=test_1.rename(columns={'img_path': 'image_path'}), \n",
    "                                         batch_size=15, samples_per_class=5)\n",
    "\n",
    "train_loader_exp1 = DataLoader(train_dataset_exp1, batch_sampler=train_sampler_exp1, shuffle=False)\n",
    "val_loader_exp1 = DataLoader(val_dataset_exp1, batch_sampler=val_sampler_exp1, shuffle=False)\n",
    "test_loader_exp1 = DataLoader(test_dataset_exp1, batch_sampler=test_sampler_exp1, shuffle=False)\n",
    "\n",
    "# Experience 2\n",
    "train_sampler_exp2 = BalancedBatchSampler(data_frame=train_2.rename(columns={'img_path': 'image_path'}), \n",
    "                                          batch_size=15, samples_per_class=5)\n",
    "val_sampler_exp2 = BalancedBatchSampler(data_frame=valid_2.rename(columns={'img_path': 'image_path'}), \n",
    "                                        batch_size=15, samples_per_class=5)\n",
    "test_sampler_exp2 = BalancedBatchSampler(data_frame=test_2.rename(columns={'img_path': 'image_path'}), \n",
    "                                         batch_size=15, samples_per_class=5)\n",
    "\n",
    "train_loader_exp2 = DataLoader(train_dataset_exp2, batch_sampler=train_sampler_exp2, shuffle=False)\n",
    "val_loader_exp2 = DataLoader(val_dataset_exp2, batch_sampler=val_sampler_exp2, shuffle=False)\n",
    "test_loader_exp2 = DataLoader(test_dataset_exp2, batch_sampler=test_sampler_exp2, shuffle=False)\n",
    "\n",
    "# Experience 3\n",
    "train_sampler_exp3 = BalancedBatchSampler(data_frame=train_3.rename(columns={'img_path': 'image_path'}), \n",
    "                                          batch_size=15, samples_per_class=5)\n",
    "val_sampler_exp3 = BalancedBatchSampler(data_frame=valid_3.rename(columns={'img_path': 'image_path'}), \n",
    "                                        batch_size=15, samples_per_class=5)\n",
    "test_sampler_exp3 = BalancedBatchSampler(data_frame=test_3.rename(columns={'img_path': 'image_path'}), \n",
    "                                         batch_size=15, samples_per_class=5)\n",
    "\n",
    "train_loader_exp3 = DataLoader(train_dataset_exp3, batch_sampler=train_sampler_exp3, shuffle=False)\n",
    "val_loader_exp3 = DataLoader(val_dataset_exp3, batch_sampler=val_sampler_exp3, shuffle=False)\n",
    "test_loader_exp3 = DataLoader(test_dataset_exp3, batch_sampler=test_sampler_exp3, shuffle=False)\n",
    "\n",
    "# Print to check if the DataLoaders are created successfully\n",
    "print(\"DataLoaders for all experiences created successfully!\")"
   ],
   "id": "d64daafd0fa36342",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders for all experiences created successfully!\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking if the datasets are AvalancheDatasets and whether they contain the correct Attributes",
   "id": "c60992f53784d719"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:05.523828Z",
     "start_time": "2025-03-01T12:44:04.232382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to check if a dataset is an instance of AvalancheDataset\n",
    "def check_avalanche_dataset(dataset):\n",
    "    # Check if dataset is an instance of AvalancheDataset\n",
    "    if isinstance(dataset, AvalancheDataset):\n",
    "        print(f\"Dataset is an instance of AvalancheDataset.\")\n",
    "    else:\n",
    "        print(f\"Dataset is NOT an instance of AvalancheDataset.\")\n",
    "        \n",
    "    # Inspect the internal structure to understand where the data attributes are stored\n",
    "    print(f\"Dataset internal structure: {dir(dataset)}\")\n",
    "\n",
    "    # Check if dataset has the core attributes: 'data', 'targets', 'task_label'\n",
    "    if hasattr(dataset, 'data') and hasattr(dataset, 'targets') and hasattr(dataset, 'task_label'):\n",
    "        print(\"Dataset contains 'data', 'targets', and 'task_label' attributes.\")\n",
    "    else:\n",
    "        print(\"Dataset is missing one or more of the required attributes: 'data', 'targets', 'task_label'.\")\n",
    "        \n",
    "    # Verify the length and sample data\n",
    "    try:\n",
    "        # Let's print the first sample to see how data is structured\n",
    "        sample = dataset[0]\n",
    "        print(f\"First sample structure: {sample}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching first sample: {e}\")\n",
    "    \n",
    "    # If there's data, check for its expected shape and content\n",
    "    if hasattr(dataset, 'data'):\n",
    "        print(f\"Dataset contains data with shape: {len(dataset.data)} samples.\")\n",
    "    \n",
    "    if hasattr(dataset, 'targets'):\n",
    "        print(f\"Dataset contains targets with length: {len(dataset.targets)}.\")\n",
    "\n",
    "# Experience 1\n",
    "check_avalanche_dataset(train_dataset_exp1)\n",
    "check_avalanche_dataset(val_dataset_exp1)\n",
    "check_avalanche_dataset(test_dataset_exp1)\n",
    "\n",
    "# Experience 2\n",
    "check_avalanche_dataset(train_dataset_exp2)\n",
    "check_avalanche_dataset(val_dataset_exp2)\n",
    "check_avalanche_dataset(test_dataset_exp2)\n",
    "\n",
    "# Experience 3\n",
    "check_avalanche_dataset(train_dataset_exp3)\n",
    "check_avalanche_dataset(val_dataset_exp3)\n",
    "check_avalanche_dataset(test_dataset_exp3)"
   ],
   "id": "7e16f685386a43ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.8268, -1.8268, -1.8268,  ..., -0.4054, -0.3883, -0.3541],\n",
      "         [-1.8268, -1.8268, -1.8268,  ..., -0.3883, -0.3541, -0.3198],\n",
      "         [-1.8268, -1.8268, -1.8610,  ..., -0.3541, -0.3198, -0.2684],\n",
      "         ...,\n",
      "         [ 1.8722,  1.8722,  1.8208,  ...,  1.1015,  1.1015,  1.1187],\n",
      "         [ 1.8208,  1.8037,  1.7694,  ...,  1.0844,  1.0844,  1.1187],\n",
      "         [ 1.8208,  1.7865,  1.7523,  ...,  1.0502,  1.0673,  1.1187]],\n",
      "\n",
      "        [[-1.7731, -1.7731, -1.7906,  ..., -0.6702, -0.6352, -0.5651],\n",
      "         [-1.7731, -1.7731, -1.7906,  ..., -0.6527, -0.6001, -0.5301],\n",
      "         [-1.7731, -1.7731, -1.8081,  ..., -0.6352, -0.5826, -0.5301],\n",
      "         ...,\n",
      "         [ 2.4286,  2.4286,  2.4111,  ...,  0.5203,  0.5028,  0.5028],\n",
      "         [ 2.4286,  2.4286,  2.3936,  ...,  0.5203,  0.5203,  0.5203],\n",
      "         [ 2.4286,  2.4111,  2.3761,  ...,  0.5028,  0.5203,  0.5378]],\n",
      "\n",
      "        [[-1.7522, -1.7522, -1.7347,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.7522, -1.7522, -1.7347,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.7522, -1.7522, -1.7696,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [ 2.5354,  2.5703,  2.5877,  ..., -1.5081, -1.5256, -1.4210],\n",
      "         [ 2.5180,  2.5877,  2.6226,  ..., -1.6127, -1.6824, -1.4559],\n",
      "         [ 2.5180,  2.5529,  2.5877,  ..., -1.7173, -1.7696, -1.5604]]]), 1, 0, 1, 0]\n",
      "Dataset contains data with shape: 69 samples.\n",
      "Dataset contains targets with length: 69.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.4329, -1.5185, -1.5185,  ..., -0.6452, -0.7308, -0.7650],\n",
      "         [-1.4500, -1.4843, -1.4843,  ..., -0.6452, -0.7308, -0.7650],\n",
      "         [-1.4329, -1.5014, -1.5185,  ..., -0.6452, -0.6965, -0.7479],\n",
      "         ...,\n",
      "         [-0.3712, -0.3712, -0.3712,  ..., -0.1828, -0.1999, -0.1828],\n",
      "         [-0.3883, -0.3883, -0.3883,  ..., -0.1486, -0.1828, -0.1657],\n",
      "         [-0.4226, -0.4226, -0.4226,  ..., -0.1486, -0.1828, -0.1657]],\n",
      "\n",
      "        [[-1.3529, -1.4230, -1.4405,  ..., -0.4251, -0.4951, -0.5826],\n",
      "         [-1.3354, -1.3880, -1.4055,  ..., -0.4426, -0.5301, -0.6001],\n",
      "         [-1.3179, -1.4055, -1.4405,  ..., -0.4601, -0.5651, -0.6352],\n",
      "         ...,\n",
      "         [ 0.0301,  0.0301,  0.0126,  ...,  0.2577,  0.2227,  0.2577],\n",
      "         [-0.0049, -0.0049, -0.0224,  ...,  0.2927,  0.2577,  0.2927],\n",
      "         [-0.0049, -0.0049, -0.0224,  ...,  0.3102,  0.2752,  0.3102]],\n",
      "\n",
      "        [[-1.4733, -1.6476, -1.7173,  ..., -1.4733, -1.3861, -1.2641],\n",
      "         [-1.5953, -1.5953, -1.5953,  ..., -1.3687, -1.2816, -1.1770],\n",
      "         [-1.6650, -1.6824, -1.6650,  ..., -1.3339, -1.1770, -1.1770],\n",
      "         ...,\n",
      "         [-1.4733, -1.4210, -1.4384,  ..., -1.4907, -1.4559, -1.4733],\n",
      "         [-1.4384, -1.4384, -1.4384,  ..., -1.4210, -1.4384, -1.5081],\n",
      "         [-1.6650, -1.6650, -1.6650,  ..., -1.4907, -1.5256, -1.5953]]]), 1, 0, 1, 0]\n",
      "Dataset contains data with shape: 12 samples.\n",
      "Dataset contains targets with length: 12.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.8097, -1.7925, -1.8268,  ..., -0.6452, -0.6452, -0.5938],\n",
      "         [-1.8097, -1.7925, -1.8268,  ..., -0.6623, -0.6452, -0.5938],\n",
      "         [-1.8097, -1.8268, -1.8439,  ..., -0.6281, -0.6281, -0.5767],\n",
      "         ...,\n",
      "         [-0.5938, -0.5767, -0.5938,  ...,  0.3823,  0.1939,  0.0398],\n",
      "         [-0.5767, -0.5596, -0.5938,  ...,  0.3994,  0.2453,  0.0569],\n",
      "         [-0.5767, -0.5596, -0.5938,  ...,  0.3994,  0.2796,  0.0912]],\n",
      "\n",
      "        [[-1.7031, -1.6856, -1.7206,  ..., -0.4776, -0.4601, -0.4076],\n",
      "         [-1.7031, -1.6856, -1.7206,  ..., -0.4776, -0.4776, -0.4426],\n",
      "         [-1.6856, -1.7031, -1.7206,  ..., -0.4776, -0.5126, -0.4951],\n",
      "         ...,\n",
      "         [-0.3200, -0.3025, -0.3025,  ...,  0.2402,  0.1527,  0.1176],\n",
      "         [-0.3025, -0.2850, -0.3025,  ...,  0.2402,  0.1877,  0.1527],\n",
      "         [-0.3025, -0.2850, -0.2850,  ...,  0.2577,  0.2227,  0.1877]],\n",
      "\n",
      "        [[-1.5604, -1.5430, -1.5779,  ..., -1.0724, -1.0898, -1.0201],\n",
      "         [-1.5604, -1.5430, -1.5779,  ..., -1.1073, -1.0027, -0.8284],\n",
      "         [-1.6302, -1.6476, -1.6476,  ..., -1.0898, -0.9504, -0.7238],\n",
      "         ...,\n",
      "         [-1.0376, -1.0550, -1.1073,  ..., -0.2707, -0.4450, -0.7064],\n",
      "         [-1.0201, -1.0898, -1.1944,  ..., -0.2010, -0.3055, -0.6890],\n",
      "         [-1.0201, -1.0898, -1.1770,  ..., -0.2184, -0.2707, -0.6541]]]), 2, 0, 2, 0]\n",
      "Dataset contains data with shape: 18 samples.\n",
      "Dataset contains targets with length: 18.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.7240, -1.7412, -1.7754,  ..., -0.8678, -0.9363, -0.9877],\n",
      "         [-1.7240, -1.7412, -1.7754,  ..., -0.8507, -0.9020, -0.9192],\n",
      "         [-1.7583, -1.7583, -1.7754,  ..., -0.8164, -0.8678, -0.9020],\n",
      "         ...,\n",
      "         [-2.0494, -2.0665, -2.0665,  ...,  0.7077,  0.7419,  0.7933],\n",
      "         [-2.0665, -2.0837, -2.0837,  ...,  0.7077,  0.7591,  0.7933],\n",
      "         [-2.0665, -2.0837, -2.0837,  ...,  0.7077,  0.7591,  0.7933]],\n",
      "\n",
      "        [[-1.6681, -1.6856, -1.7031,  ..., -0.6702, -0.7227, -0.7752],\n",
      "         [-1.6681, -1.6856, -1.7031,  ..., -0.6176, -0.6702, -0.7227],\n",
      "         [-1.6681, -1.6681, -1.6681,  ..., -0.5476, -0.6527, -0.6877],\n",
      "         ...,\n",
      "         [-1.8957, -1.9132, -1.9132,  ...,  1.0455,  1.1331,  1.2906],\n",
      "         [-1.8957, -1.9132, -1.9307,  ...,  1.0805,  1.2031,  1.3081],\n",
      "         [-1.8957, -1.9132, -1.9307,  ...,  1.1155,  1.2731,  1.3081]],\n",
      "\n",
      "        [[-1.6476, -1.6650, -1.7173,  ..., -1.4733, -1.5604, -1.6302],\n",
      "         [-1.6476, -1.6650, -1.7173,  ..., -1.6127, -1.6127, -1.5081],\n",
      "         [-1.6824, -1.6824, -1.7173,  ..., -1.4907, -1.3339, -1.2816],\n",
      "         ...,\n",
      "         [-1.7173, -1.7347, -1.7347,  ..., -1.4036, -1.4559, -1.7347],\n",
      "         [-1.7696, -1.7870, -1.7696,  ..., -1.5081, -1.6302, -1.7870],\n",
      "         [-1.7696, -1.7870, -1.7696,  ..., -1.6824, -1.7870, -1.8044]]]), 1, 0, 1, 0]\n",
      "Dataset contains data with shape: 96 samples.\n",
      "Dataset contains targets with length: 96.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.4158, -1.4329, -1.4500,  ..., -1.1075, -1.1247, -1.0904],\n",
      "         [-1.4158, -1.4329, -1.4500,  ..., -1.1075, -1.1247, -1.0904],\n",
      "         [-1.4843, -1.4843, -1.5014,  ..., -1.1075, -1.1247, -1.0904],\n",
      "         ...,\n",
      "         [ 0.6049,  0.6221,  0.6392,  ..., -0.7650, -0.7479, -0.7479],\n",
      "         [ 0.5707,  0.6049,  0.6392,  ..., -0.7650, -0.7650, -0.7822],\n",
      "         [ 0.5707,  0.6049,  0.6221,  ..., -0.7993, -0.7822, -0.7822]],\n",
      "\n",
      "        [[-1.5630, -1.5805, -1.6155,  ..., -1.0553, -1.0378, -0.9853],\n",
      "         [-1.5630, -1.5805, -1.6155,  ..., -1.0378, -1.0378, -1.0028],\n",
      "         [-1.5630, -1.5630, -1.5980,  ..., -1.0203, -1.0203, -1.0203],\n",
      "         ...,\n",
      "         [ 0.6429,  0.6078,  0.6078,  ..., -0.7752, -0.7752, -0.7752],\n",
      "         [ 0.6429,  0.5903,  0.5728,  ..., -0.7927, -0.7927, -0.8102],\n",
      "         [ 0.6429,  0.5903,  0.5728,  ..., -0.8277, -0.8102, -0.8102]],\n",
      "\n",
      "        [[-1.7870, -1.8044, -1.8044,  ..., -1.5779, -1.6127, -1.6650],\n",
      "         [-1.7870, -1.8044, -1.8044,  ..., -1.6824, -1.6302, -1.5604],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.7347, -1.6302, -1.4733],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -0.5670, -0.5495, -0.5495],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -0.6018, -0.6018, -0.6193],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -0.6367, -0.6193, -0.6193]]]), 1, 0, 1, 0]\n",
      "Dataset contains data with shape: 18 samples.\n",
      "Dataset contains targets with length: 18.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.1589, -1.2617, -1.2617,  ..., -1.1418, -1.2445, -1.2274],\n",
      "         [-1.1589, -1.2445, -1.2445,  ..., -1.1418, -1.2445, -1.2274],\n",
      "         [-1.1760, -1.2274, -1.2274,  ..., -1.1247, -1.2103, -1.2103],\n",
      "         ...,\n",
      "         [-0.2856, -0.2856, -0.3027,  ..., -0.6281, -0.5596, -0.5082],\n",
      "         [-0.2684, -0.2684, -0.2856,  ..., -0.6623, -0.6109, -0.5767],\n",
      "         [-0.2684, -0.2684, -0.2856,  ..., -0.6965, -0.6452, -0.6109]],\n",
      "\n",
      "        [[-1.1429, -1.2479, -1.2479,  ..., -0.9328, -1.0378, -1.0203],\n",
      "         [-1.1253, -1.2304, -1.2479,  ..., -0.9328, -1.0378, -1.0203],\n",
      "         [-1.1253, -1.2129, -1.2304,  ..., -0.9503, -1.0553, -1.0728],\n",
      "         ...,\n",
      "         [-0.0924, -0.0749, -0.0924,  ..., -1.3880, -1.3179, -1.3179],\n",
      "         [-0.0749, -0.0574, -0.0924,  ..., -1.4055, -1.3354, -1.3004],\n",
      "         [-0.0574, -0.0399, -0.0749,  ..., -1.4405, -1.3704, -1.3354]],\n",
      "\n",
      "        [[-1.6127, -1.7173, -1.7347,  ..., -1.4559, -1.4559, -1.3513],\n",
      "         [-1.7173, -1.6824, -1.5953,  ..., -1.4384, -1.4559, -1.3513],\n",
      "         [-1.7696, -1.6650, -1.5430,  ..., -1.4210, -1.4036, -1.3164],\n",
      "         ...,\n",
      "         [-1.3339, -1.4559, -1.4559,  ...,  0.0082,  0.0605,  0.0256],\n",
      "         [-1.4559, -1.5430, -1.4559,  ..., -0.0964, -0.0267,  0.0082],\n",
      "         [-1.5430, -1.6302, -1.5430,  ..., -0.1312, -0.0615, -0.0267]]]), 2, 0, 2, 0]\n",
      "Dataset contains data with shape: 24 samples.\n",
      "Dataset contains targets with length: 24.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-0.2342, -0.2513, -0.2856,  ..., -1.1932, -1.2274, -1.2274],\n",
      "         [-0.2342, -0.2513, -0.2856,  ..., -1.2103, -1.2617, -1.2274],\n",
      "         [-0.1999, -0.2171, -0.2513,  ..., -1.2103, -1.2617, -1.2445],\n",
      "         ...,\n",
      "         [ 2.1975,  2.1975,  2.1975,  ..., -0.6623, -0.6452, -0.6452],\n",
      "         [ 2.1975,  2.1975,  2.1975,  ..., -0.6452, -0.6281, -0.6452],\n",
      "         [ 2.1975,  2.1975,  2.1975,  ..., -0.6623, -0.6109, -0.6109]],\n",
      "\n",
      "        [[-0.6702, -0.6877, -0.7227,  ..., -1.1604, -1.2129, -1.2479],\n",
      "         [-0.6702, -0.6877, -0.7227,  ..., -1.1954, -1.2479, -1.2304],\n",
      "         [-0.6176, -0.6527, -0.6702,  ..., -1.1954, -1.2479, -1.2654],\n",
      "         ...,\n",
      "         [ 2.3761,  2.3761,  2.3761,  ..., -0.4951, -0.4776, -0.4776],\n",
      "         [ 2.3761,  2.3761,  2.3761,  ..., -0.4776, -0.4776, -0.4776],\n",
      "         [ 2.3761,  2.3761,  2.3761,  ..., -0.4951, -0.4776, -0.4601]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.6999, -1.6302, -1.5430],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.7173, -1.6650, -1.5430],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.7173, -1.6650, -1.5604],\n",
      "         ...,\n",
      "         [ 2.5877,  2.5877,  2.5877,  ..., -0.0790, -0.0964, -0.0964],\n",
      "         [ 2.5877,  2.5877,  2.5877,  ..., -0.0441, -0.0615, -0.0790],\n",
      "         [ 2.5877,  2.5877,  2.5877,  ...,  0.0256,  0.0431,  0.0605]]]), 0, 0, 0, 0]\n",
      "Dataset contains data with shape: 522 samples.\n",
      "Dataset contains targets with length: 522.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.6042, -1.6042, -1.6042,  ..., -0.5767, -0.6965, -0.7822],\n",
      "         [-1.6042, -1.6042, -1.6042,  ..., -0.6109, -0.7308, -0.7822],\n",
      "         [-1.6213, -1.6384, -1.6384,  ..., -0.6281, -0.6965, -0.7137],\n",
      "         ...,\n",
      "         [ 0.1254,  0.1083,  0.0912,  ...,  1.0673,  1.0844,  1.0844],\n",
      "         [ 0.1083,  0.1426,  0.1254,  ...,  1.0673,  1.0844,  1.0844],\n",
      "         [ 0.1426,  0.1768,  0.1768,  ...,  1.0673,  1.0844,  1.0844]],\n",
      "\n",
      "        [[-1.6506, -1.6506, -1.6681,  ..., -0.9153, -0.9328, -0.9328],\n",
      "         [-1.6506, -1.6506, -1.6506,  ..., -0.9503, -0.9678, -0.9328],\n",
      "         [-1.6331, -1.6331, -1.6506,  ..., -0.9503, -0.9678, -0.9503],\n",
      "         ...,\n",
      "         [ 0.4328,  0.3803,  0.3452,  ...,  0.6078,  0.5903,  0.5903],\n",
      "         [ 0.4503,  0.3803,  0.3277,  ...,  0.6254,  0.6078,  0.6078],\n",
      "         [ 0.4153,  0.3452,  0.2927,  ...,  0.6254,  0.6078,  0.6078]],\n",
      "\n",
      "        [[-1.7522, -1.7522, -1.7347,  ..., -1.7522, -1.7696, -1.8044],\n",
      "         [-1.7522, -1.7522, -1.7522,  ..., -1.7696, -1.8044, -1.8044],\n",
      "         [-1.7870, -1.7870, -1.7870,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [ 1.1759,  1.0365,  0.9145,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [ 1.0365,  0.9145,  0.7576,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [ 0.8274,  0.6879,  0.5485,  ..., -1.8044, -1.8044, -1.8044]]]), 0, 0, 0, 0]\n",
      "Dataset contains data with shape: 111 samples.\n",
      "Dataset contains targets with length: 111.\n",
      "Dataset is an instance of AvalancheDataset.\n",
      "Dataset internal structure: ['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__protocol_attrs__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_data_attributes', '_datasets', '_flat_data', '_init_collate_fn', '_is_protocol', '_is_runtime_protocol', '_shallow_clone_dataset', '_transform_groups', '_tree_depth', 'collate_fn', 'concat', 'data', 'eval', 'freeze_transforms', 'get_valid_indices', 'remove_current_transform_group', 'replace_current_transform_group', 'root_dir', 'subset', 'targets', 'targets_task_labels', 'task_label', 'train', 'transform', 'update_data_attribute', 'with_transforms']\n",
      "Dataset contains 'data', 'targets', and 'task_label' attributes.\n",
      "First sample structure: [tensor([[[-1.3987, -1.4158, -1.4158,  ..., -0.8335, -0.9534, -0.9192],\n",
      "         [-1.3987, -1.4158, -1.4158,  ..., -0.7993, -0.9192, -0.9192],\n",
      "         [-1.3644, -1.3987, -1.3987,  ..., -0.7650, -0.9020, -0.9020],\n",
      "         ...,\n",
      "         [-0.4911, -0.4911, -0.4739,  ..., -1.7583, -1.7069, -1.7240],\n",
      "         [-0.4397, -0.4568, -0.4054,  ..., -1.7583, -1.7240, -1.7069],\n",
      "         [-0.4397, -0.4568, -0.4226,  ..., -1.7583, -1.7240, -1.7069]],\n",
      "\n",
      "        [[-1.3529, -1.3529, -1.3354,  ..., -0.5826, -0.6877, -0.6527],\n",
      "         [-1.3354, -1.3529, -1.3704,  ..., -0.5476, -0.6702, -0.6527],\n",
      "         [-1.3004, -1.3354, -1.3704,  ..., -0.4951, -0.6527, -0.6352],\n",
      "         ...,\n",
      "         [-0.2325, -0.2500, -0.2675,  ..., -1.7906, -1.7556, -1.7731],\n",
      "         [-0.2150, -0.2150, -0.1975,  ..., -1.8081, -1.8081, -1.7906],\n",
      "         [-0.1975, -0.1800, -0.1625,  ..., -1.8256, -1.8081, -1.7906]],\n",
      "\n",
      "        [[-1.6824, -1.7870, -1.8044,  ..., -1.3513, -1.3339, -1.3861],\n",
      "         [-1.7696, -1.7696, -1.7347,  ..., -1.3513, -1.3164, -1.3861],\n",
      "         [-1.7870, -1.7696, -1.7173,  ..., -1.2990, -1.2816, -1.3513],\n",
      "         ...,\n",
      "         [-1.4907, -1.4559, -1.2990,  ..., -1.4210, -1.3513, -1.3687],\n",
      "         [-1.5430, -1.5953, -1.4036,  ..., -1.4384, -1.3861, -1.3687],\n",
      "         [-1.5953, -1.7696, -1.6999,  ..., -1.4384, -1.3861, -1.3687]]]), 1, 0, 1, 0]\n",
      "Dataset contains data with shape: 114 samples.\n",
      "Dataset contains targets with length: 114.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:05.548691Z",
     "start_time": "2025-03-01T12:44:05.530220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to print all attributes of the dataset\n",
    "def print_all_attributes(dataset):\n",
    "    print(f\"Attributes of the dataset:\")\n",
    "    for attr in dir(dataset):\n",
    "        # Skip private attributes (those starting with '_')\n",
    "        if not attr.startswith('_'):\n",
    "            print(f\"  {attr}\")\n",
    "\n",
    "# Check all datasets in the \"train\" and \"test\" streams\n",
    "dataset_streams = {\n",
    "    \"train\": [train_dataset_exp1, train_dataset_exp2, train_dataset_exp3],\n",
    "    \"test\": [test_dataset_exp1, test_dataset_exp2, test_dataset_exp3]\n",
    "}\n",
    "\n",
    "# Iterate over the streams and check each dataset\n",
    "for stream_name, datasets in dataset_streams.items():\n",
    "    print(f\"\\nChecking {stream_name} datasets:\")\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        print(f\"\\n  Checking dataset {stream_name}_{i + 1}:\")\n",
    "        print_all_attributes(dataset)"
   ],
   "id": "974da5519d17844c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking train datasets:\n",
      "\n",
      "  Checking dataset train_1:\n",
      "Attributes of the dataset:\n",
      "  collate_fn\n",
      "  concat\n",
      "  data\n",
      "  eval\n",
      "  freeze_transforms\n",
      "  get_valid_indices\n",
      "  remove_current_transform_group\n",
      "  replace_current_transform_group\n",
      "  root_dir\n",
      "  subset\n",
      "  targets\n",
      "  targets_task_labels\n",
      "  task_label\n",
      "  train\n",
      "  transform\n",
      "  update_data_attribute\n",
      "  with_transforms\n",
      "\n",
      "  Checking dataset train_2:\n",
      "Attributes of the dataset:\n",
      "  collate_fn\n",
      "  concat\n",
      "  data\n",
      "  eval\n",
      "  freeze_transforms\n",
      "  get_valid_indices\n",
      "  remove_current_transform_group\n",
      "  replace_current_transform_group\n",
      "  root_dir\n",
      "  subset\n",
      "  targets\n",
      "  targets_task_labels\n",
      "  task_label\n",
      "  train\n",
      "  transform\n",
      "  update_data_attribute\n",
      "  with_transforms\n",
      "\n",
      "  Checking dataset train_3:\n",
      "Attributes of the dataset:\n",
      "  collate_fn\n",
      "  concat\n",
      "  data\n",
      "  eval\n",
      "  freeze_transforms\n",
      "  get_valid_indices\n",
      "  remove_current_transform_group\n",
      "  replace_current_transform_group\n",
      "  root_dir\n",
      "  subset\n",
      "  targets\n",
      "  targets_task_labels\n",
      "  task_label\n",
      "  train\n",
      "  transform\n",
      "  update_data_attribute\n",
      "  with_transforms\n",
      "\n",
      "Checking test datasets:\n",
      "\n",
      "  Checking dataset test_1:\n",
      "Attributes of the dataset:\n",
      "  collate_fn\n",
      "  concat\n",
      "  data\n",
      "  eval\n",
      "  freeze_transforms\n",
      "  get_valid_indices\n",
      "  remove_current_transform_group\n",
      "  replace_current_transform_group\n",
      "  root_dir\n",
      "  subset\n",
      "  targets\n",
      "  targets_task_labels\n",
      "  task_label\n",
      "  train\n",
      "  transform\n",
      "  update_data_attribute\n",
      "  with_transforms\n",
      "\n",
      "  Checking dataset test_2:\n",
      "Attributes of the dataset:\n",
      "  collate_fn\n",
      "  concat\n",
      "  data\n",
      "  eval\n",
      "  freeze_transforms\n",
      "  get_valid_indices\n",
      "  remove_current_transform_group\n",
      "  replace_current_transform_group\n",
      "  root_dir\n",
      "  subset\n",
      "  targets\n",
      "  targets_task_labels\n",
      "  task_label\n",
      "  train\n",
      "  transform\n",
      "  update_data_attribute\n",
      "  with_transforms\n",
      "\n",
      "  Checking dataset test_3:\n",
      "Attributes of the dataset:\n",
      "  collate_fn\n",
      "  concat\n",
      "  data\n",
      "  eval\n",
      "  freeze_transforms\n",
      "  get_valid_indices\n",
      "  remove_current_transform_group\n",
      "  replace_current_transform_group\n",
      "  root_dir\n",
      "  subset\n",
      "  targets\n",
      "  targets_task_labels\n",
      "  task_label\n",
      "  train\n",
      "  transform\n",
      "  update_data_attribute\n",
      "  with_transforms\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:06.187287Z",
     "start_time": "2025-03-01T12:44:06.177628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if they are instances of AvalancheDataset\n",
    "print(\"Is train_dataset_exp1 an AvalancheDataset? \", isinstance(train_dataset_exp1, AvalancheDataset))\n",
    "print(\"Is train_dataset_exp2 an AvalancheDataset? \", isinstance(train_dataset_exp2, AvalancheDataset))\n",
    "print(\"Is train_dataset_exp3 an AvalancheDataset? \", isinstance(train_dataset_exp3, AvalancheDataset))"
   ],
   "id": "d7af2ee5c622bc93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is train_dataset_exp1 an AvalancheDataset?  True\n",
      "Is train_dataset_exp2 an AvalancheDataset?  True\n",
      "Is train_dataset_exp3 an AvalancheDataset?  True\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:07.039889Z",
     "start_time": "2025-03-01T12:44:06.997260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_dataset(avalanche_dataset):\n",
    "    try:\n",
    "        # Access the first sample using __getitem__\n",
    "        first_sample = avalanche_dataset[0]  # This might be a tuple (image, label, task_label)\n",
    "        \n",
    "        # Print the entire first sample to check the structure\n",
    "        print(f\"First sample: {first_sample}\")\n",
    "        \n",
    "        # Print the type of each element in the sample (image, target, task_label)\n",
    "        if isinstance(first_sample, tuple):\n",
    "            print(f\"First element type (image): {type(first_sample[0])}\")\n",
    "            print(f\"Second element type (target): {type(first_sample[1])}\")\n",
    "            if len(first_sample) >= 3:\n",
    "                print(f\"Third element type (task_label): {type(first_sample[2])}\")\n",
    "            else:\n",
    "                print(\"No task label in the sample.\")\n",
    "        else:\n",
    "            print(\"The first sample is not a tuple as expected.\")\n",
    "\n",
    "        # Check if the first element (image) is a string (file path) or a tensor\n",
    "        if isinstance(first_sample[0], str):\n",
    "            print(\"The first element is a string, which might be a file path.\")\n",
    "        elif hasattr(first_sample[0], 'shape'):\n",
    "            print(f\"The first element is an image tensor with shape: {first_sample[0].shape}\")\n",
    "        else:\n",
    "            print(\"The first element is neither a string nor a tensor.\")\n",
    "\n",
    "        # Check if the dataset has 3 elements (image, target, task_label)\n",
    "        if len(first_sample) >= 3:\n",
    "            print(f\"Target (label): {first_sample[1]}\")\n",
    "            print(f\"Task label: {first_sample[2]}\")\n",
    "        else:\n",
    "            print(\"Warning: The dataset does not contain all expected elements (image, target, task_label).\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error accessing dataset attributes: {e}\")\n",
    "    except IndexError as e:\n",
    "        print(f\"Error accessing dataset elements: {e}\")\n",
    "\n",
    "# Running the function on the first dataset\n",
    "check_dataset(train_dataset_exp1)"
   ],
   "id": "20d58bc78cd886cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: [tensor([[[-1.8268, -1.8268, -1.8268,  ..., -0.4054, -0.3883, -0.3541],\n",
      "         [-1.8268, -1.8268, -1.8268,  ..., -0.3883, -0.3541, -0.3198],\n",
      "         [-1.8268, -1.8268, -1.8610,  ..., -0.3541, -0.3198, -0.2684],\n",
      "         ...,\n",
      "         [ 1.8722,  1.8722,  1.8208,  ...,  1.1015,  1.1015,  1.1187],\n",
      "         [ 1.8208,  1.8037,  1.7694,  ...,  1.0844,  1.0844,  1.1187],\n",
      "         [ 1.8208,  1.7865,  1.7523,  ...,  1.0502,  1.0673,  1.1187]],\n",
      "\n",
      "        [[-1.7731, -1.7731, -1.7906,  ..., -0.6702, -0.6352, -0.5651],\n",
      "         [-1.7731, -1.7731, -1.7906,  ..., -0.6527, -0.6001, -0.5301],\n",
      "         [-1.7731, -1.7731, -1.8081,  ..., -0.6352, -0.5826, -0.5301],\n",
      "         ...,\n",
      "         [ 2.4286,  2.4286,  2.4111,  ...,  0.5203,  0.5028,  0.5028],\n",
      "         [ 2.4286,  2.4286,  2.3936,  ...,  0.5203,  0.5203,  0.5203],\n",
      "         [ 2.4286,  2.4111,  2.3761,  ...,  0.5028,  0.5203,  0.5378]],\n",
      "\n",
      "        [[-1.7522, -1.7522, -1.7347,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.7522, -1.7522, -1.7347,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.7522, -1.7522, -1.7696,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [ 2.5354,  2.5703,  2.5877,  ..., -1.5081, -1.5256, -1.4210],\n",
      "         [ 2.5180,  2.5877,  2.6226,  ..., -1.6127, -1.6824, -1.4559],\n",
      "         [ 2.5180,  2.5529,  2.5877,  ..., -1.7173, -1.7696, -1.5604]]]), 1, 0, 1, 0]\n",
      "The first sample is not a tuple as expected.\n",
      "The first element is an image tensor with shape: torch.Size([3, 224, 224])\n",
      "Target (label): 1\n",
      "Task label: 0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:09.101748Z",
     "start_time": "2025-03-01T12:44:09.042071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = train_dataset_exp1[0]\n",
    "image, label, task_label = sample[:3]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}, Task Label: {task_label}\")"
   ],
   "id": "2f4f94d7fc2542c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 224, 224]), Label: 1, Task Label: 0\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking class distribution in each dataset",
   "id": "9180cb3e72c81f3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:10.623379Z",
     "start_time": "2025-03-01T12:44:10.596610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def count_classes(dataset):\n",
    "    # Convert the FlatData into a list of values via list comprehension.\n",
    "    values = [x for x in dataset.targets]\n",
    "    # Convert the list of values to a tensor.\n",
    "    t = torch.tensor(values)\n",
    "    # Now, convert the tensor to a NumPy array and count the classes.\n",
    "    return Counter(t.numpy())\n",
    "\n",
    "print(\"Class distribution in Train Dataset 1:\", count_classes(train_dataset_exp1))\n",
    "print(\"Class distribution in Train Dataset 2:\", count_classes(train_dataset_exp2))\n",
    "print(\"Class distribution in Train Dataset 3:\", count_classes(train_dataset_exp3))\n",
    "print(\"Class distribution in Validation Dataset 1:\", count_classes(val_dataset_exp1))\n",
    "print(\"Class distribution in Validation Dataset 2:\", count_classes(val_dataset_exp2))\n",
    "print(\"Class distribution in Validation Dataset 3:\", count_classes(val_dataset_exp3))\n",
    "print(\"Class distribution in Test Dataset 1:\", count_classes(test_dataset_exp1))\n",
    "print(\"Class distribution in Test Dataset 2:\", count_classes(test_dataset_exp2))\n",
    "print(\"Class distribution in Test Dataset 3:\", count_classes(test_dataset_exp3))"
   ],
   "id": "96784ab22b358e59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Train Dataset 1: Counter({1: 23, 0: 23, 2: 23})\n",
      "Class distribution in Train Dataset 2: Counter({1: 32, 0: 32, 2: 32})\n",
      "Class distribution in Train Dataset 3: Counter({0: 174, 1: 174, 2: 174})\n",
      "Class distribution in Validation Dataset 1: Counter({1: 4, 0: 4, 2: 4})\n",
      "Class distribution in Validation Dataset 2: Counter({1: 6, 0: 6, 2: 6})\n",
      "Class distribution in Validation Dataset 3: Counter({0: 37, 1: 37, 2: 37})\n",
      "Class distribution in Test Dataset 1: Counter({2: 6, 0: 6, 1: 6})\n",
      "Class distribution in Test Dataset 2: Counter({2: 8, 1: 8, 0: 8})\n",
      "Class distribution in Test Dataset 3: Counter({1: 38, 2: 38, 0: 38})\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking class distribution in each experience",
   "id": "45ac6737aadc6ea6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:16.501226Z",
     "start_time": "2025-03-01T12:44:16.476919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from avalanche.benchmarks.utils import DataAttribute\n",
    "from avalanche.benchmarks import benchmark_from_datasets\n",
    "# Create the benchmark from your datasets\n",
    "dataset_streams = {\n",
    "    \"train\": [train_dataset_exp1, train_dataset_exp2, train_dataset_exp3],\n",
    "    \"test\": [test_dataset_exp1, test_dataset_exp2, test_dataset_exp3]\n",
    "}\n",
    "# You might want to ensure the benchmark is created here\n",
    "benchmark = benchmark_from_datasets(**dataset_streams)\n",
    "\n",
    "for experience in benchmark.train_stream:\n",
    "    print(f\"Start of experience: {experience.current_experience}\")\n",
    "    \n",
    "    # Try to get the targets via the dynamic property.\n",
    "    try:\n",
    "        targets_data = experience.dataset.targets.data\n",
    "    except AttributeError:\n",
    "        # Fallback: access the internal _data_attributes dictionary.\n",
    "        targets_data = experience.dataset._data_attributes[\"targets\"].data\n",
    "\n",
    "    # If targets_data doesn't have 'tolist', assume it's already iterable.\n",
    "    if hasattr(targets_data, \"tolist\"):\n",
    "        unique_classes = set(targets_data.tolist())\n",
    "    else:\n",
    "        unique_classes = set(targets_data)\n",
    "        \n",
    "    print(f\"Classes in this experience: {unique_classes}\")"
   ],
   "id": "68ee0e7f34ed6f18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of experience: 0\n",
      "Classes in this experience: {0, 1, 2}\n",
      "Start of experience: 1\n",
      "Classes in this experience: {0, 1, 2}\n",
      "Start of experience: 2\n",
      "Classes in this experience: {0, 1, 2}\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Overfitting Test",
   "id": "66418603d7b42da9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T12:44:34.139012Z",
     "start_time": "2025-03-01T12:44:18.145038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from avalanche.training import EWC\n",
    "from avalanche.benchmarks import benchmark_from_datasets\n",
    "from models.cnn_models import SimpleCNN\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = SimpleCNN(num_classes=3).to(device)\n",
    "\n",
    "# Define your loss function and optimizer with a high learning rate for overfitting test\n",
    "criterion = CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)  # High LR, no weight decay\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Disable dropout in the model to aid overfitting\n",
    "def disable_dropout(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.p = 0.0\n",
    "\n",
    "disable_dropout(model)\n",
    "\n",
    "# Create subsets of your AvalancheDataset (assuming train_dataset_exp1 exists)\n",
    "# Here, we take the first 30 samples for training and use the same set for evaluation.\n",
    "small_train_dataset = train_dataset_exp1.subset(list(range(30)))\n",
    "small_test_dataset = train_dataset_exp1.subset(list(range(30)))\n",
    "\n",
    "# Create a benchmark from these subsets\n",
    "small_benchmark = benchmark_from_datasets(train=[small_train_dataset],\n",
    "                                          test=[small_test_dataset])\n",
    "\n",
    "# Create an EWC strategy instance.\n",
    "# We set ewc_lambda to 0.0 to disable the EWC penalty and train_epochs to 50 so that\n",
    "# the strategy runs for 50 epochs internally on the small subset.\n",
    "cl_strategy = EWC(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_mb_size=5,\n",
    "    train_epochs=100,\n",
    "    eval_mb_size=5,\n",
    "    ewc_lambda=0.0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Run training on the small benchmark\n",
    "for experience in small_benchmark.train_stream:\n",
    "    print(f\"=== Overfitting Test on Small Subset, Experience {experience.current_experience} ===\")\n",
    "    cl_strategy.train(experience)\n",
    "    \n",
    "    # Evaluate on the training subset to check if the model overfits\n",
    "    train_eval_res = cl_strategy.eval(small_benchmark.test_stream)\n",
    "    print(\"Evaluation on training subset:\", train_eval_res)\n",
    "    \n",
    "    # Optionally, print predictions on a few samples to verify correctness\n",
    "    for sample in small_benchmark.test_stream[0].dataset:\n",
    "        image, label, *rest = sample\n",
    "        image = image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "        output = model(image)\n",
    "        predicted_class = output.argmax(dim=1).item()\n",
    "        print(f\"True label: {label}, Predicted: {predicted_class}\")\n",
    "        break  # Print only one sample per experience"
   ],
   "id": "74009c0d62fa93db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overfitting Test on Small Subset, Experience 0 ===\n",
      "-- >> Start of training phase << --\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 55\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m experience \u001B[38;5;129;01min\u001B[39;00m small_benchmark\u001B[38;5;241m.\u001B[39mtrain_stream:\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=== Overfitting Test on Small Subset, Experience \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexperience\u001B[38;5;241m.\u001B[39mcurrent_experience\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 55\u001B[0m     \u001B[43mcl_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperience\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;66;03m# Evaluate on the training subset to check if the model overfits\u001B[39;00m\n\u001B[0;32m     58\u001B[0m     train_eval_res \u001B[38;5;241m=\u001B[39m cl_strategy\u001B[38;5;241m.\u001B[39meval(small_benchmark\u001B[38;5;241m.\u001B[39mtest_stream)\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:211\u001B[0m, in \u001B[0;36mBaseSGDTemplate.train\u001B[1;34m(self, experiences, eval_streams, **kwargs)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    205\u001B[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    210\u001B[0m ):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_streams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluator\u001B[38;5;241m.\u001B[39mget_last_metrics()\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\avalanche\\training\\templates\\base.py:163\u001B[0m, in \u001B[0;36mBaseTemplate.train\u001B[1;34m(self, experiences, eval_streams, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperience \u001B[38;5;129;01min\u001B[39;00m experiences_list:\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_before_training_exp(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 163\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_exp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexperience\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_streams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training_exp(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:337\u001B[0m, in \u001B[0;36mBaseSGDTemplate._train_exp\u001B[1;34m(self, experience, eval_streams, **kwargs)\u001B[0m\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m--> 337\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training_epoch(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\avalanche\\training\\templates\\update_type\\sgd_update.py:31\u001B[0m, in \u001B[0;36mSGDUpdate.training_epoch\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Forward\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_before_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmb_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Loss & Backward\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\avalanche\\training\\templates\\problem_type\\supervised_problem.py:47\u001B[0m, in \u001B[0;36mSupervisedProblem.forward\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     46\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the model's output given the current mini-batch.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mavalanche_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmb_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmb_task_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\avalanche\\models\\utils.py:22\u001B[0m, in \u001B[0;36mavalanche_forward\u001B[1;34m(model, x, task_labels)\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model(x, task_labels)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# no task labels\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\models\\cnn_models.py:32\u001B[0m, in \u001B[0;36mSimpleCNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 32\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Flatten\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(x)\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\avalanche\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementing EWC using Avalanche - the end-to-end continual learning library",
   "id": "ccd760b7b5776e8b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-02T00:28:39.637750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "import os\n",
    "from models.cnn_models import SimpleCNN\n",
    "\n",
    "# Ensure the folder \"loss_plots\" exists.\n",
    "if not os.path.exists(\"loss_plots\"):\n",
    "    os.makedirs(\"loss_plots\")\n",
    "\n",
    "# Helper function to compute the average loss on a dataset.\n",
    "def compute_loss(model, dataset, criterion, device, batch_size=15):\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                inputs, targets = batch[0], batch[1]\n",
    "            else:\n",
    "                raise ValueError(\"Expected the data loader to return a tuple or list.\")\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "    model.train()  # Set back to train mode\n",
    "    return sum(losses) / len(losses) if losses else 0.0\n",
    "\n",
    "# Custom plugin to record training and validation loss after each epoch.\n",
    "class LossHistoryPlugin(SupervisedPlugin):\n",
    "    def __init__(self, validation_dataset, criterion, device, batch_size=15):\n",
    "        super().__init__()\n",
    "        self.validation_dataset = validation_dataset\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_train_losses = []\n",
    "        self.epoch_val_losses = []\n",
    "\n",
    "    def after_training_epoch(self, strategy):\n",
    "        # Compute training loss on the full training dataset of the current experience.\n",
    "        train_loss = compute_loss(strategy.model, strategy.experience.dataset, self.criterion, self.device, self.batch_size)\n",
    "        # Compute validation loss on the provided validation dataset.\n",
    "        val_loss = compute_loss(strategy.model, self.validation_dataset, self.criterion, self.device, self.batch_size)\n",
    "        self.epoch_train_losses.append(train_loss)\n",
    "        self.epoch_val_losses.append(val_loss)\n",
    "        print(f\"Epoch {len(self.epoch_train_losses)}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "# -------------------------------\n",
    "# Define candidate hyperparameters for grid search.\n",
    "# -------------------------------\n",
    "learning_rates = [0.001]\n",
    "ewc_lambdas = [50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# -------------------------------\n",
    "# Setup loggers and device\n",
    "# -------------------------------\n",
    "tb_logger = TensorboardLogger()\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "interactive_logger = InteractiveLogger()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -------------------------------\n",
    "# Setup benchmark and validation datasets\n",
    "# -------------------------------\n",
    "# (Assume train_dataset_exp1, train_dataset_exp2, train_dataset_exp3,\n",
    "#  test_dataset_exp1, test_dataset_exp2, test_dataset_exp3,\n",
    "#  val_dataset_exp1, val_dataset_exp2, val_dataset_exp3 are defined.)\n",
    "dataset_streams = {\n",
    "    \"train\": [train_dataset_exp1, train_dataset_exp2, train_dataset_exp3],\n",
    "    \"test\": [test_dataset_exp1, test_dataset_exp2, test_dataset_exp3]\n",
    "}\n",
    "benchmark = benchmark_from_datasets(**dataset_streams)\n",
    "validation_datasets = [val_dataset_exp1, val_dataset_exp2, val_dataset_exp3]\n",
    "\n",
    "# -------------------------------\n",
    "# Grid search loop over learning rate and ewc_lambda.\n",
    "# -------------------------------\n",
    "results_summary = []\n",
    "\n",
    "for lr, ewc_lambda in itertools.product(learning_rates, ewc_lambdas):\n",
    "    print(f\"\\n=== Hyperparameters: lr={lr}, ewc_lv   ambda={ewc_lambda} ===\")\n",
    "    \n",
    "    # Reinitialize model, criterion, and optimizer.\n",
    "    model = SimpleCNN(num_classes=3).to(device)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    # For example, using SGD:\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "    \n",
    "    # Setup a learning rate scheduler and its plugin.\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "    lr_plugin = LRSchedulerPlugin(scheduler)\n",
    "    \n",
    "    # Enable the evaluator to log additional metrics.\n",
    "    evaluator = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "        loggers=[interactive_logger, text_logger, tb_logger]\n",
    "    )\n",
    "    \n",
    "    # Instantiate the EWC strategy with eval_every=-1 since we’ll do our own per-epoch validation.\n",
    "    cl_strategy = EWC(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_mb_size=15,\n",
    "        train_epochs=50,\n",
    "        eval_mb_size=15,\n",
    "        ewc_lambda=ewc_lambda,\n",
    "        evaluator=evaluator,\n",
    "        eval_every=-1,  # disable the internal evaluation calls\n",
    "        device=device,\n",
    "        plugins=[lr_plugin]  # we will add our loss history plugin per experience below\n",
    "    )\n",
    "    \n",
    "    # Prepare a CSV file for logging summary metrics for this hyperparameter setting.\n",
    "    csv_file_path = f\"ewc_grid_lr{lr}_lambda{ewc_lambda}.csv\"\n",
    "    with open(csv_file_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Experience\", \"Epochs\", \"FinalTrainLoss\", \"FinalValLoss\"])\n",
    "    \n",
    "    # Train across experiences.\n",
    "    for i, experience in enumerate(benchmark.train_stream):\n",
    "        print(f\"\\n=== Start of Experience {experience.current_experience} ===\")\n",
    "        \n",
    "        # Create a validation benchmark for the current experience if needed.\n",
    "        # Here we use the corresponding validation dataset directly.\n",
    "        current_val_dataset = validation_datasets[i]\n",
    "        \n",
    "        # Instantiate the loss history plugin for this experience.\n",
    "        loss_history = LossHistoryPlugin(validation_dataset=current_val_dataset,\n",
    "                                         criterion=criterion, device=device, batch_size=15)\n",
    "        # Add our plugin to the strategy.\n",
    "        cl_strategy.plugins.append(loss_history)\n",
    "        \n",
    "        print(f\"Training Experience {experience.current_experience} for 50 epochs...\")\n",
    "        train_res = cl_strategy.train(experience)\n",
    "        # After training, remove the loss history plugin so it doesn't accumulate data from previous experiences.\n",
    "        cl_strategy.plugins.remove(loss_history)\n",
    "        \n",
    "        # At this point, loss_history.epoch_train_losses and .epoch_val_losses contain the per-epoch losses.\n",
    "        final_train_loss = loss_history.epoch_train_losses[-1] if loss_history.epoch_train_losses else None\n",
    "        final_val_loss = loss_history.epoch_val_losses[-1] if loss_history.epoch_val_losses else None\n",
    "        \n",
    "        # Save per-epoch losses to a plot.\n",
    "        epochs = list(range(1, len(loss_history.epoch_train_losses) + 1))\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, loss_history.epoch_train_losses, label='Train Loss')\n",
    "        plt.plot(epochs, loss_history.epoch_val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f\"Exp {experience.current_experience} | Optimiser: Adam | lr={lr}, ewc_lambda={ewc_lambda} | {cl_strategy.train_epochs} epochs\")\n",
    "        plt.legend()\n",
    "        plot_filename = os.path.join(\"loss_plots\", f\"loss_plot_exp{experience.current_experience}_lr{lr}_lambda{ewc_lambda}.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "        print(f\"Saved loss plot to {plot_filename}\")\n",
    "        \n",
    "        # Log summary metrics for this experience.\n",
    "        with open(csv_file_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([experience.current_experience, cl_strategy.train_epochs, final_train_loss, final_val_loss])\n",
    "        \n",
    "        # Evaluate on the entire test stream.\n",
    "        print(\"Testing on the entire test stream...\")\n",
    "        test_res = cl_strategy.eval(benchmark.test_stream)\n",
    "        print(\"Test results:\", test_res)\n",
    "    \n",
    "    results_summary.append({\n",
    "        \"lr\": lr,\n",
    "        \"ewc_lambda\": ewc_lambda,\n",
    "        \"final_train_loss\": final_train_loss,\n",
    "        \"final_val_loss\": final_val_loss,\n",
    "        \"test_results\": test_res    \n",
    "    })\n",
    "\n",
    "print(\"\\n=== Hyperparameter Search Summary ===\")\n",
    "for res in results_summary: \n",
    "    print(res)"
   ],
   "id": "696ecf812b74bb0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hyperparameters: lr=0.001, ewc_lambda=50 ===\n",
      "\n",
      "=== Start of Experience 0 ===\n",
      "Training Experience 0 for 50 epochs...\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 5/5 [01:10<00:00, 14.04s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1020\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3333\n",
      "Epoch 1: Train Loss = 1.0935, Val Loss = 1.0940\n",
      "100%|██████████| 5/5 [00:43<00:00,  8.66s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0886\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4058\n",
      "Epoch 2: Train Loss = 1.0703, Val Loss = 1.0741\n",
      "100%|██████████| 5/5 [00:37<00:00,  7.58s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0621\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5217\n",
      "Epoch 3: Train Loss = 1.0161, Val Loss = 1.0301\n",
      "100%|██████████| 5/5 [00:36<00:00,  7.21s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0076\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5652\n",
      "Epoch 4: Train Loss = 0.9563, Val Loss = 0.9875\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.14s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9426\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5362\n",
      "Epoch 5: Train Loss = 0.9060, Val Loss = 0.9423\n",
      "100%|██████████| 5/5 [00:38<00:00,  7.71s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9074\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4783\n",
      "Epoch 6: Train Loss = 0.8651, Val Loss = 0.8915\n",
      "100%|██████████| 5/5 [00:36<00:00,  7.22s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8796\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5072\n",
      "Epoch 7: Train Loss = 0.8394, Val Loss = 0.8852\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.65s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8940\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5362\n",
      "Epoch 8: Train Loss = 0.8318, Val Loss = 0.8553\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.48s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8590\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5797\n",
      "Epoch 9: Train Loss = 0.8960, Val Loss = 0.9972\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.58s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8943\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5652\n",
      "Epoch 10: Train Loss = 0.8169, Val Loss = 0.8356\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.41s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8421\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5942\n",
      "Epoch 11: Train Loss = 0.8020, Val Loss = 0.8542\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.49s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7897\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6232\n",
      "Epoch 12: Train Loss = 0.7829, Val Loss = 0.8251\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.38s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8125\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5797\n",
      "Epoch 13: Train Loss = 0.7886, Val Loss = 0.8932\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.47s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7819\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6087\n",
      "Epoch 14: Train Loss = 0.7581, Val Loss = 0.8133\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.98s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7767\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6232\n",
      "Epoch 15: Train Loss = 0.7469, Val Loss = 0.8151\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.80s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7418\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6522\n",
      "Epoch 16: Train Loss = 0.7443, Val Loss = 0.8174\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.42s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7343\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6812\n",
      "Epoch 17: Train Loss = 0.7316, Val Loss = 0.8322\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.80s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7174\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7101\n",
      "Epoch 18: Train Loss = 0.7298, Val Loss = 0.8396\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.94s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7154\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7101\n",
      "Epoch 19: Train Loss = 0.7277, Val Loss = 0.8416\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.44s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7162\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 20: Train Loss = 0.7262, Val Loss = 0.8443\n",
      "100%|██████████| 5/5 [00:37<00:00,  7.57s/it]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7135\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 21: Train Loss = 0.7219, Val Loss = 0.8382\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.62s/it]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7119\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7101\n",
      "Epoch 22: Train Loss = 0.7179, Val Loss = 0.8306\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.30s/it]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7072\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 23: Train Loss = 0.7152, Val Loss = 0.8215\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.40s/it]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7066\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7101\n",
      "Epoch 24: Train Loss = 0.7135, Val Loss = 0.8223\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.31s/it]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7131\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 25: Train Loss = 0.7127, Val Loss = 0.8413\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.21s/it]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6992\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 26: Train Loss = 0.7081, Val Loss = 0.8308\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.59s/it]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6973\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 27: Train Loss = 0.7055, Val Loss = 0.8173\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.13s/it]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6977\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6957\n",
      "Epoch 28: Train Loss = 0.7044, Val Loss = 0.8142\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.15s/it]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6989\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6957\n",
      "Epoch 29: Train Loss = 0.7022, Val Loss = 0.8339\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.85s/it]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6903\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 30: Train Loss = 0.7006, Val Loss = 0.8356\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.64s/it]\n",
      "Epoch 30 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6877\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 31: Train Loss = 0.7002, Val Loss = 0.8346\n",
      "100%|██████████| 5/5 [00:36<00:00,  7.36s/it]\n",
      "Epoch 31 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6873\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 32: Train Loss = 0.6996, Val Loss = 0.8330\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.11s/it]\n",
      "Epoch 32 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6870\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 33: Train Loss = 0.6990, Val Loss = 0.8311\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.42s/it]\n",
      "Epoch 33 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6864\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 34: Train Loss = 0.6985, Val Loss = 0.8294\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.61s/it]\n",
      "Epoch 34 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6862\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 35: Train Loss = 0.6982, Val Loss = 0.8281\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.41s/it]\n",
      "Epoch 35 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6859\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 36: Train Loss = 0.6979, Val Loss = 0.8279\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.34s/it]\n",
      "Epoch 36 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6858\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 37: Train Loss = 0.6976, Val Loss = 0.8266\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.33s/it]\n",
      "Epoch 37 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6855\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 38: Train Loss = 0.6973, Val Loss = 0.8263\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.36s/it]\n",
      "Epoch 38 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6855\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 39: Train Loss = 0.6971, Val Loss = 0.8256\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.47s/it]\n",
      "Epoch 39 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6850\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 40: Train Loss = 0.6969, Val Loss = 0.8261\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.72s/it]\n",
      "Epoch 40 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6852\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 41: Train Loss = 0.6968, Val Loss = 0.8281\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.62s/it]\n",
      "Epoch 41 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6846\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 42: Train Loss = 0.6966, Val Loss = 0.8288\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.51s/it]\n",
      "Epoch 42 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6844\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 43: Train Loss = 0.6965, Val Loss = 0.8291\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.81s/it]\n",
      "Epoch 43 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6840\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 44: Train Loss = 0.6963, Val Loss = 0.8297\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.41s/it]\n",
      "Epoch 44 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6839\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 45: Train Loss = 0.6960, Val Loss = 0.8292\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.40s/it]\n",
      "Epoch 45 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6835\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 46: Train Loss = 0.6960, Val Loss = 0.8292\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.42s/it]\n",
      "Epoch 46 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6835\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 47: Train Loss = 0.6960, Val Loss = 0.8292\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.60s/it]\n",
      "Epoch 47 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6835\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 48: Train Loss = 0.6960, Val Loss = 0.8293\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.87s/it]\n",
      "Epoch 48 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6834\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 49: Train Loss = 0.6959, Val Loss = 0.8292\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.71s/it]\n",
      "Epoch 49 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6834\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 50: Train Loss = 0.6959, Val Loss = 0.8292\n",
      "-- >> End of training phase << --\n",
      "Saved loss plot to loss_plots\\loss_plot_exp0_lr0.001_lambda50.png\n",
      "Testing on the entire test stream...\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 from test stream --\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n",
      "> Eval on experience 0 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp000 = 1.6658\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp000 = 0.3333\n",
      "-- Starting eval on experience 1 from test stream --\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.02s/it]\n",
      "> Eval on experience 1 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp001 = 1.3619\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp001 = 0.3333\n",
      "-- Starting eval on experience 2 from test stream --\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.51s/it]\n",
      "> Eval on experience 2 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp002 = 1.3867\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp002 = 0.3509\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 1.4151\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.3462\n",
      "Test results: {'Top1_Acc_Epoch/train_phase/train_stream': 0.7391304347826086, 'Loss_Epoch/train_phase/train_stream': 0.6834098204322483, 'Top1_Acc_Exp/eval_phase/test_stream/Exp000': 0.3333333333333333, 'Loss_Exp/eval_phase/test_stream/Exp000': 1.6657756666342418, 'Top1_Acc_Exp/eval_phase/test_stream/Exp001': 0.3333333333333333, 'Loss_Exp/eval_phase/test_stream/Exp001': 1.3619441539049149, 'Top1_Acc_Exp/eval_phase/test_stream/Exp002': 0.3508771929824561, 'Loss_Exp/eval_phase/test_stream/Exp002': 1.3866966617734808, 'Top1_Acc_Stream/eval_phase/test_stream': 0.34615384615384615, 'Loss_Stream/eval_phase/test_stream': 1.4150900072776353}\n",
      "\n",
      "=== Start of Experience 1 ===\n",
      "Training Experience 1 for 50 epochs...\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.71s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.5385\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3229\n",
      "Epoch 1: Train Loss = 1.2153, Val Loss = 1.0948\n",
      "100%|██████████| 7/7 [00:47<00:00,  6.72s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1503\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3542\n",
      "Epoch 2: Train Loss = 1.0862, Val Loss = 1.1289\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.69s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1096\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3333\n",
      "Epoch 3: Train Loss = 1.0863, Val Loss = 1.1151\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.53s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0947\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3750\n",
      "Epoch 4: Train Loss = 1.0755, Val Loss = 1.0855\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.67s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0860\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4896\n",
      "Epoch 5: Train Loss = 1.0653, Val Loss = 1.0905\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.49s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0849\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3646\n",
      "Epoch 6: Train Loss = 1.0542, Val Loss = 1.0744\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.45s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0611\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4375\n",
      "Epoch 7: Train Loss = 1.0421, Val Loss = 1.0703\n",
      "100%|██████████| 7/7 [00:48<00:00,  6.87s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0488\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5312\n",
      "Epoch 8: Train Loss = 1.0145, Val Loss = 1.0655\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.47s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0587\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4271\n",
      "Epoch 9: Train Loss = 1.0068, Val Loss = 1.0766\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.35s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0016\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5312\n",
      "Epoch 10: Train Loss = 0.9839, Val Loss = 0.9996\n",
      "100%|██████████| 7/7 [00:49<00:00,  7.00s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9794\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5833\n",
      "Epoch 11: Train Loss = 0.9368, Val Loss = 1.0025\n",
      "100%|██████████| 7/7 [00:48<00:00,  6.91s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9775\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5208\n",
      "Epoch 12: Train Loss = 0.9129, Val Loss = 0.9839\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.48s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9187\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6146\n",
      "Epoch 13: Train Loss = 0.8530, Val Loss = 0.9727\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.40s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8908\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5625\n",
      "Epoch 14: Train Loss = 0.8252, Val Loss = 0.9374\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.34s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8811\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6042\n",
      "Epoch 15: Train Loss = 0.7734, Val Loss = 0.9447\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.33s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7843\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7083\n",
      "Epoch 16: Train Loss = 0.7675, Val Loss = 0.9170\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.53s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7710\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7083\n",
      "Epoch 17: Train Loss = 0.7536, Val Loss = 0.9201\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.55s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7598\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7396\n",
      "Epoch 18: Train Loss = 0.7408, Val Loss = 0.9435\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.32s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7522\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7083\n",
      "Epoch 19: Train Loss = 0.7296, Val Loss = 0.9332\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.52s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7405\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 20: Train Loss = 0.7210, Val Loss = 0.9374\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.70s/it]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7342\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 21: Train Loss = 0.7140, Val Loss = 0.9211\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.41s/it]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7261\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7500\n",
      "Epoch 22: Train Loss = 0.7073, Val Loss = 0.9239\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.38s/it]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7227\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7604\n",
      "Epoch 23: Train Loss = 0.6983, Val Loss = 0.9280\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.31s/it]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7134\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7500\n",
      "Epoch 24: Train Loss = 0.6952, Val Loss = 0.9165\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.57s/it]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7077\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 25: Train Loss = 0.6848, Val Loss = 0.9216\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.60s/it]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6992\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7500\n",
      "Epoch 26: Train Loss = 0.6783, Val Loss = 0.9241\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.48s/it]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6919\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7500\n",
      "Epoch 27: Train Loss = 0.6712, Val Loss = 0.9333\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.49s/it]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6905\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 28: Train Loss = 0.6673, Val Loss = 0.9172\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.47s/it]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6828\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 29: Train Loss = 0.6585, Val Loss = 0.9355\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.51s/it]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6802\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7604\n",
      "Epoch 30: Train Loss = 0.6522, Val Loss = 0.9614\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.46s/it]\n",
      "Epoch 30 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6688\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7500\n",
      "Epoch 31: Train Loss = 0.6511, Val Loss = 0.9608\n",
      "100%|██████████| 7/7 [00:52<00:00,  7.45s/it]\n",
      "Epoch 31 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6674\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7604\n",
      "Epoch 32: Train Loss = 0.6503, Val Loss = 0.9589\n",
      "100%|██████████| 7/7 [00:49<00:00,  7.01s/it]\n",
      "Epoch 32 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6662\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7604\n",
      "Epoch 33: Train Loss = 0.6495, Val Loss = 0.9569\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.44s/it]\n",
      "Epoch 33 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6663\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 34: Train Loss = 0.6487, Val Loss = 0.9528\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.67s/it]\n",
      "Epoch 34 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6645\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 35: Train Loss = 0.6479, Val Loss = 0.9540\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.49s/it]\n",
      "Epoch 35 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6638\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 36: Train Loss = 0.6473, Val Loss = 0.9515\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.39s/it]\n",
      "Epoch 36 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6630\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 37: Train Loss = 0.6468, Val Loss = 0.9489\n",
      "100%|██████████| 7/7 [00:47<00:00,  6.80s/it]\n",
      "Epoch 37 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6624\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 38: Train Loss = 0.6464, Val Loss = 0.9462\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.37s/it]\n",
      "Epoch 38 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6619\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 39: Train Loss = 0.6460, Val Loss = 0.9433\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.49s/it]\n",
      "Epoch 39 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6619\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7917\n",
      "Epoch 40: Train Loss = 0.6456, Val Loss = 0.9416\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.35s/it]\n",
      "Epoch 40 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6613\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7917\n",
      "Epoch 41: Train Loss = 0.6450, Val Loss = 0.9420\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.54s/it]\n",
      "Epoch 41 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6606\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7917\n",
      "Epoch 42: Train Loss = 0.6442, Val Loss = 0.9441\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.53s/it]\n",
      "Epoch 42 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6598\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 43: Train Loss = 0.6436, Val Loss = 0.9447\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.43s/it]\n",
      "Epoch 43 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6595\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7708\n",
      "Epoch 44: Train Loss = 0.6429, Val Loss = 0.9472\n",
      "100%|██████████| 7/7 [00:48<00:00,  6.93s/it]\n",
      "Epoch 44 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6586\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 45: Train Loss = 0.6423, Val Loss = 0.9473\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.71s/it]\n",
      "Epoch 45 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6580\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 46: Train Loss = 0.6423, Val Loss = 0.9474\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.54s/it]\n",
      "Epoch 46 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6579\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 47: Train Loss = 0.6422, Val Loss = 0.9475\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.45s/it]\n",
      "Epoch 47 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6579\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 48: Train Loss = 0.6422, Val Loss = 0.9474\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.50s/it]\n",
      "Epoch 48 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6578\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 49: Train Loss = 0.6421, Val Loss = 0.9475\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.56s/it]\n",
      "Epoch 49 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6578\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7812\n",
      "Epoch 50: Train Loss = 0.6420, Val Loss = 0.9475\n",
      "-- >> End of training phase << --\n",
      "Saved loss plot to loss_plots\\loss_plot_exp1_lr0.001_lambda50.png\n",
      "Testing on the entire test stream...\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 from test stream --\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.24s/it]\n",
      "> Eval on experience 0 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp000 = 1.7389\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp000 = 0.2222\n",
      "-- Starting eval on experience 1 from test stream --\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n",
      "> Eval on experience 1 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp001 = 0.8027\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp001 = 0.6667\n",
      "-- Starting eval on experience 2 from test stream --\n",
      "100%|██████████| 8/8 [00:27<00:00,  3.42s/it]\n",
      "> Eval on experience 2 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp002 = 1.5182\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp002 = 0.3070\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 1.4336\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.3526\n",
      "Test results: {'Top1_Acc_Epoch/train_phase/train_stream': 0.78125, 'Loss_Epoch/train_phase/train_stream': 0.6577533464878798, 'Top1_Acc_Exp/eval_phase/test_stream/Exp000': 0.2222222222222222, 'Loss_Exp/eval_phase/test_stream/Exp000': 1.7389260331789653, 'Top1_Acc_Exp/eval_phase/test_stream/Exp001': 0.6666666666666666, 'Loss_Exp/eval_phase/test_stream/Exp001': 0.8026837632060051, 'Top1_Acc_Exp/eval_phase/test_stream/Exp002': 0.30701754385964913, 'Loss_Exp/eval_phase/test_stream/Exp002': 1.518238133505771, 'Top1_Acc_Stream/eval_phase/test_stream': 0.3525641025641026, 'Loss_Stream/eval_phase/test_stream': 1.4336168341911757}\n",
      "\n",
      "=== Start of Experience 2 ===\n",
      "Training Experience 2 for 50 epochs...\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 35/35 [04:14<00:00,  7.26s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.2376\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3333\n",
      "Epoch 1: Train Loss = 1.1302, Val Loss = 1.1259\n",
      "100%|██████████| 35/35 [04:12<00:00,  7.20s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1376\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3506\n",
      "Epoch 2: Train Loss = 1.0973, Val Loss = 1.0948\n",
      "100%|██████████| 35/35 [04:08<00:00,  7.10s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1243\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3352\n",
      "Epoch 3: Train Loss = 1.0923, Val Loss = 1.0921\n",
      "100%|██████████| 35/35 [04:11<00:00,  7.20s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1137\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3870\n",
      "Epoch 4: Train Loss = 1.0817, Val Loss = 1.0810\n",
      "100%|██████████| 35/35 [04:05<00:00,  7.02s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1031\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3678\n",
      "Epoch 5: Train Loss = 1.0767, Val Loss = 1.0842\n",
      "100%|██████████| 35/35 [04:02<00:00,  6.93s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0893\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4100\n",
      "Epoch 6: Train Loss = 1.0453, Val Loss = 1.0519\n",
      "100%|██████████| 35/35 [04:09<00:00,  7.12s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0466\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4693\n",
      "Epoch 7: Train Loss = 0.9664, Val Loss = 1.0114\n",
      "100%|██████████| 35/35 [04:03<00:00,  6.95s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0229\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4732\n",
      "Epoch 8: Train Loss = 0.9916, Val Loss = 1.0187\n",
      "100%|██████████| 35/35 [04:10<00:00,  7.15s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0154\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4904\n",
      "Epoch 9: Train Loss = 0.9757, Val Loss = 1.0262\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.09s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0053\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4847\n",
      "Epoch 10: Train Loss = 0.9956, Val Loss = 1.0879\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.07s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9820\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5096\n",
      "Epoch 11: Train Loss = 0.9229, Val Loss = 1.0178\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.07s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9699\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5153\n",
      "Epoch 12: Train Loss = 1.0088, Val Loss = 1.0822\n",
      "100%|██████████| 35/35 [04:11<00:00,  7.19s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9822\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5134\n",
      "Epoch 13: Train Loss = 0.9487, Val Loss = 1.0342\n",
      "100%|██████████| 35/35 [04:05<00:00,  7.02s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9648\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5211\n",
      "Epoch 14: Train Loss = 0.9896, Val Loss = 1.0690\n",
      "100%|██████████| 35/35 [04:15<00:00,  7.30s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9724\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5172\n",
      "Epoch 15: Train Loss = 0.9071, Val Loss = 0.9661\n",
      "100%|██████████| 35/35 [04:12<00:00,  7.21s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9122\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5613\n",
      "Epoch 16: Train Loss = 0.8460, Val Loss = 0.9487\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.07s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8772\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5920\n",
      "Epoch 17: Train Loss = 0.8290, Val Loss = 0.9470\n",
      "100%|██████████| 35/35 [04:13<00:00,  7.24s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8627\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5958\n",
      "Epoch 18: Train Loss = 0.8170, Val Loss = 0.9493\n",
      "100%|██████████| 35/35 [04:06<00:00,  7.04s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8529\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5900\n",
      "Epoch 19: Train Loss = 0.8151, Val Loss = 0.9526\n",
      "100%|██████████| 35/35 [04:10<00:00,  7.17s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8539\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6111\n",
      "Epoch 20: Train Loss = 0.8065, Val Loss = 0.9629\n",
      "100%|██████████| 35/35 [04:12<00:00,  7.21s/it]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8476\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5920\n",
      "Epoch 21: Train Loss = 0.7954, Val Loss = 0.9566\n",
      "100%|██████████| 35/35 [03:59<00:00,  6.83s/it]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8353\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6360\n",
      "Epoch 22: Train Loss = 0.7916, Val Loss = 0.9556\n",
      "100%|██████████| 35/35 [04:06<00:00,  7.03s/it]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8308\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6149\n",
      "Epoch 23: Train Loss = 0.7882, Val Loss = 0.9469\n",
      "100%|██████████| 35/35 [04:00<00:00,  6.88s/it]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8280\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6264\n",
      "Epoch 24: Train Loss = 0.7787, Val Loss = 0.9556\n",
      "100%|██████████| 35/35 [04:09<00:00,  7.13s/it]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8200\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6207\n",
      "Epoch 25: Train Loss = 0.7752, Val Loss = 0.9595\n",
      "100%|██████████| 35/35 [04:00<00:00,  6.86s/it]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8138\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6111\n",
      "Epoch 26: Train Loss = 0.7668, Val Loss = 0.9587\n",
      "100%|██████████| 35/35 [04:14<00:00,  7.26s/it]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8102\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6379\n",
      "Epoch 27: Train Loss = 0.7607, Val Loss = 0.9634\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.07s/it]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8058\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6245\n",
      "Epoch 28: Train Loss = 0.7569, Val Loss = 0.9580\n",
      "100%|██████████| 35/35 [04:05<00:00,  7.01s/it]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7971\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6284\n",
      "Epoch 29: Train Loss = 0.7547, Val Loss = 0.9622\n",
      "100%|██████████| 35/35 [04:10<00:00,  7.14s/it]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7961\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6437\n",
      "Epoch 30: Train Loss = 0.7443, Val Loss = 0.9583\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.08s/it]\n",
      "Epoch 30 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7798\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6456\n",
      "Epoch 31: Train Loss = 0.7433, Val Loss = 0.9580\n",
      "100%|██████████| 35/35 [03:59<00:00,  6.86s/it]\n",
      "Epoch 31 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7795\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6552\n",
      "Epoch 32: Train Loss = 0.7425, Val Loss = 0.9590\n",
      "100%|██████████| 35/35 [04:04<00:00,  6.98s/it]\n",
      "Epoch 32 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7784\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6494\n",
      "Epoch 33: Train Loss = 0.7419, Val Loss = 0.9581\n",
      "100%|██████████| 35/35 [04:17<00:00,  7.36s/it]\n",
      "Epoch 33 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7786\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6513\n",
      "Epoch 34: Train Loss = 0.7413, Val Loss = 0.9584\n",
      "100%|██████████| 35/35 [04:06<00:00,  7.04s/it]\n",
      "Epoch 34 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7768\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6552\n",
      "Epoch 35: Train Loss = 0.7406, Val Loss = 0.9584\n",
      "100%|██████████| 35/35 [04:06<00:00,  7.05s/it]\n",
      "Epoch 35 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7765\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6494\n",
      "Epoch 36: Train Loss = 0.7401, Val Loss = 0.9583\n",
      "100%|██████████| 35/35 [04:11<00:00,  7.20s/it]\n",
      "Epoch 36 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7763\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6475\n",
      "Epoch 37: Train Loss = 0.7395, Val Loss = 0.9591\n",
      "100%|██████████| 35/35 [04:08<00:00,  7.11s/it]\n",
      "Epoch 37 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7752\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6475\n",
      "Epoch 38: Train Loss = 0.7390, Val Loss = 0.9595\n",
      "100%|██████████| 35/35 [04:19<00:00,  7.41s/it]\n",
      "Epoch 38 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7767\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6590\n",
      "Epoch 39: Train Loss = 0.7384, Val Loss = 0.9588\n",
      "100%|██████████| 35/35 [04:04<00:00,  7.00s/it]\n",
      "Epoch 39 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7749\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6513\n",
      "Epoch 40: Train Loss = 0.7378, Val Loss = 0.9593\n",
      "100%|██████████| 35/35 [03:57<00:00,  6.78s/it]\n",
      "Epoch 40 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7742\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6494\n",
      "Epoch 41: Train Loss = 0.7373, Val Loss = 0.9591\n",
      "100%|██████████| 35/35 [04:21<00:00,  7.48s/it]\n",
      "Epoch 41 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7734\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6552\n",
      "Epoch 42: Train Loss = 0.7368, Val Loss = 0.9591\n",
      "100%|██████████| 35/35 [04:16<00:00,  7.33s/it]\n",
      "Epoch 42 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7726\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6533\n",
      "Epoch 43: Train Loss = 0.7362, Val Loss = 0.9587\n",
      "100%|██████████| 35/35 [04:04<00:00,  6.99s/it]\n",
      "Epoch 43 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7724\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6533\n",
      "Epoch 44: Train Loss = 0.7357, Val Loss = 0.9597\n",
      "100%|██████████| 35/35 [04:04<00:00,  6.98s/it]\n",
      "Epoch 44 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7723\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6513\n",
      "Epoch 45: Train Loss = 0.7353, Val Loss = 0.9606\n",
      "100%|██████████| 35/35 [04:02<00:00,  6.94s/it]\n",
      "Epoch 45 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7704\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6533\n",
      "Epoch 46: Train Loss = 0.7352, Val Loss = 0.9603\n",
      "100%|██████████| 35/35 [04:11<00:00,  7.18s/it]\n",
      "Epoch 46 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7705\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6533\n",
      "Epoch 47: Train Loss = 0.7351, Val Loss = 0.9602\n",
      "100%|██████████| 35/35 [04:09<00:00,  7.13s/it]\n",
      "Epoch 47 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7703\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6533\n",
      "Epoch 48: Train Loss = 0.7351, Val Loss = 0.9603\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.06s/it]\n",
      "Epoch 48 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7703\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6533\n",
      "Epoch 49: Train Loss = 0.7350, Val Loss = 0.9603\n",
      "100%|██████████| 35/35 [04:07<00:00,  7.07s/it]\n",
      "Epoch 49 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7703\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6571\n",
      "Epoch 50: Train Loss = 0.7350, Val Loss = 0.9601\n",
      "-- >> End of training phase << --\n",
      "Saved loss plot to loss_plots\\loss_plot_exp2_lr0.001_lambda50.png\n",
      "Testing on the entire test stream...\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 from test stream --\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.20s/it]\n",
      "> Eval on experience 0 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp000 = 0.9675\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp000 = 0.4444\n",
      "-- Starting eval on experience 1 from test stream --\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.85s/it]\n",
      "> Eval on experience 1 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp001 = 2.2386\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp001 = 0.2083\n",
      "-- Starting eval on experience 2 from test stream --\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.52s/it]\n",
      "> Eval on experience 2 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp002 = 0.9301\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp002 = 0.5614\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 1.1357\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.4936\n",
      "Test results: {'Top1_Acc_Epoch/train_phase/train_stream': 0.657088122605364, 'Loss_Epoch/train_phase/train_stream': 0.7702783414687233, 'Top1_Acc_Exp/eval_phase/test_stream/Exp000': 0.4444444444444444, 'Loss_Exp/eval_phase/test_stream/Exp000': 0.9675041933854421, 'Top1_Acc_Exp/eval_phase/test_stream/Exp001': 0.20833333333333334, 'Loss_Exp/eval_phase/test_stream/Exp001': 2.238552376627922, 'Top1_Acc_Exp/eval_phase/test_stream/Exp002': 0.5614035087719298, 'Loss_Exp/eval_phase/test_stream/Exp002': 0.9301147398195768, 'Top1_Acc_Stream/eval_phase/test_stream': 0.4935897435897436, 'Loss_Stream/eval_phase/test_stream': 1.13572700550923}\n",
      "\n",
      "=== Hyperparameters: lr=0.001, ewc_lambda=60 ===\n",
      "\n",
      "=== Start of Experience 0 ===\n",
      "Training Experience 0 for 50 epochs...\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.79s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1072\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3333\n",
      "Epoch 1: Train Loss = 1.0946, Val Loss = 1.0999\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.28s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1009\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3333\n",
      "Epoch 2: Train Loss = 1.0845, Val Loss = 1.0880\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.50s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0871\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3768\n",
      "Epoch 3: Train Loss = 1.0746, Val Loss = 1.0855\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.44s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0672\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3333\n",
      "Epoch 4: Train Loss = 1.0490, Val Loss = 1.0632\n",
      "100%|██████████| 5/5 [00:37<00:00,  7.47s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0378\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4928\n",
      "Epoch 5: Train Loss = 1.0082, Val Loss = 1.0215\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.45s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9965\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5072\n",
      "Epoch 6: Train Loss = 0.9555, Val Loss = 0.9745\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.30s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9502\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5072\n",
      "Epoch 7: Train Loss = 0.9159, Val Loss = 0.9246\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.73s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9466\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5507\n",
      "Epoch 8: Train Loss = 0.9032, Val Loss = 0.9204\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.57s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9021\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5652\n",
      "Epoch 9: Train Loss = 0.8763, Val Loss = 0.9042\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.68s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8530\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5652\n",
      "Epoch 10: Train Loss = 0.8605, Val Loss = 0.9154\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.31s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8858\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5797\n",
      "Epoch 11: Train Loss = 0.8498, Val Loss = 0.8952\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.25s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8148\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5797\n",
      "Epoch 12: Train Loss = 0.8278, Val Loss = 0.9277\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.32s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8324\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5942\n",
      "Epoch 13: Train Loss = 0.8118, Val Loss = 0.9279\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.51s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8557\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5507\n",
      "Epoch 14: Train Loss = 0.7809, Val Loss = 0.9035\n",
      "100%|██████████| 5/5 [00:34<00:00,  7.00s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7828\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6377\n",
      "Epoch 15: Train Loss = 0.7603, Val Loss = 0.8927\n",
      "100%|██████████| 5/5 [00:36<00:00,  7.28s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7478\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 16: Train Loss = 0.7546, Val Loss = 0.8859\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.14s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7404\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6812\n",
      "Epoch 17: Train Loss = 0.7469, Val Loss = 0.8810\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.82s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7357\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 18: Train Loss = 0.7432, Val Loss = 0.8845\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.87s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7308\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 19: Train Loss = 0.7386, Val Loss = 0.8814\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.76s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7283\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 20: Train Loss = 0.7350, Val Loss = 0.8862\n",
      "100%|██████████| 5/5 [00:33<00:00,  6.72s/it]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7237\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 21: Train Loss = 0.7302, Val Loss = 0.8824\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.04s/it]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7185\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 22: Train Loss = 0.7258, Val Loss = 0.8794\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.10s/it]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7165\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 23: Train Loss = 0.7227, Val Loss = 0.8779\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.56s/it]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7161\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7246\n",
      "Epoch 24: Train Loss = 0.7188, Val Loss = 0.8795\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.44s/it]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7080\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 25: Train Loss = 0.7165, Val Loss = 0.8837\n",
      "100%|██████████| 5/5 [00:34<00:00,  6.82s/it]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7044\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 26: Train Loss = 0.7143, Val Loss = 0.8853\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.48s/it]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7064\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7681\n",
      "Epoch 27: Train Loss = 0.7119, Val Loss = 0.8882\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.55s/it]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7032\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7391\n",
      "Epoch 28: Train Loss = 0.7110, Val Loss = 0.8937\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.33s/it]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6967\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 29: Train Loss = 0.7063, Val Loss = 0.8887\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.36s/it]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6937\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 30: Train Loss = 0.7017, Val Loss = 0.8776\n",
      "100%|██████████| 5/5 [00:37<00:00,  7.56s/it]\n",
      "Epoch 30 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6907\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 31: Train Loss = 0.7014, Val Loss = 0.8771\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.39s/it]\n",
      "Epoch 31 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6908\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 32: Train Loss = 0.7011, Val Loss = 0.8766\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.22s/it]\n",
      "Epoch 32 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6903\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 33: Train Loss = 0.7006, Val Loss = 0.8774\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.13s/it]\n",
      "Epoch 33 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6896\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 34: Train Loss = 0.7003, Val Loss = 0.8778\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.32s/it]\n",
      "Epoch 34 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6890\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 35: Train Loss = 0.6999, Val Loss = 0.8785\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.12s/it]\n",
      "Epoch 35 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6886\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 36: Train Loss = 0.6997, Val Loss = 0.8793\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.29s/it]\n",
      "Epoch 36 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6884\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 37: Train Loss = 0.6994, Val Loss = 0.8801\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.26s/it]\n",
      "Epoch 37 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6878\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 38: Train Loss = 0.6991, Val Loss = 0.8802\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.24s/it]\n",
      "Epoch 38 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6874\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 39: Train Loss = 0.6989, Val Loss = 0.8805\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.23s/it]\n",
      "Epoch 39 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6873\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 40: Train Loss = 0.6986, Val Loss = 0.8807\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.46s/it]\n",
      "Epoch 40 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6868\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 41: Train Loss = 0.6984, Val Loss = 0.8810\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.37s/it]\n",
      "Epoch 41 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6865\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 42: Train Loss = 0.6982, Val Loss = 0.8817\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.21s/it]\n",
      "Epoch 42 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6862\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 43: Train Loss = 0.6980, Val Loss = 0.8820\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.28s/it]\n",
      "Epoch 43 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6862\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 44: Train Loss = 0.6978, Val Loss = 0.8826\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.06s/it]\n",
      "Epoch 44 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6855\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 45: Train Loss = 0.6975, Val Loss = 0.8825\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.42s/it]\n",
      "Epoch 45 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6853\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 46: Train Loss = 0.6975, Val Loss = 0.8825\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.10s/it]\n",
      "Epoch 46 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6852\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 47: Train Loss = 0.6975, Val Loss = 0.8826\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.16s/it]\n",
      "Epoch 47 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6852\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 48: Train Loss = 0.6974, Val Loss = 0.8825\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.45s/it]\n",
      "Epoch 48 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6852\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 49: Train Loss = 0.6974, Val Loss = 0.8826\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.31s/it]\n",
      "Epoch 49 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6851\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7536\n",
      "Epoch 50: Train Loss = 0.6974, Val Loss = 0.8826\n",
      "-- >> End of training phase << --\n",
      "Saved loss plot to loss_plots\\loss_plot_exp0_lr0.001_lambda60.png\n",
      "Testing on the entire test stream...\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 from test stream --\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n",
      "> Eval on experience 0 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp000 = 1.8630\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp000 = 0.2778\n",
      "-- Starting eval on experience 1 from test stream --\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.29s/it]\n",
      "> Eval on experience 1 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp001 = 1.3181\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp001 = 0.3750\n",
      "-- Starting eval on experience 2 from test stream --\n",
      "100%|██████████| 8/8 [00:27<00:00,  3.49s/it]\n",
      "> Eval on experience 2 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp002 = 1.3755\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp002 = 0.3421\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 1.4229\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.3397\n",
      "Test results: {'Top1_Acc_Epoch/train_phase/train_stream': 0.7536231884057971, 'Loss_Epoch/train_phase/train_stream': 0.6851297889066779, 'Top1_Acc_Exp/eval_phase/test_stream/Exp000': 0.2777777777777778, 'Loss_Exp/eval_phase/test_stream/Exp000': 1.8630032638708751, 'Top1_Acc_Exp/eval_phase/test_stream/Exp001': 0.375, 'Loss_Exp/eval_phase/test_stream/Exp001': 1.3180713653564453, 'Top1_Acc_Exp/eval_phase/test_stream/Exp002': 0.34210526315789475, 'Loss_Exp/eval_phase/test_stream/Exp002': 1.3754608097829317, 'Top1_Acc_Stream/eval_phase/test_stream': 0.33974358974358976, 'Loss_Stream/eval_phase/test_stream': 1.422886563035158}\n",
      "\n",
      "=== Start of Experience 1 ===\n",
      "Training Experience 1 for 50 epochs...\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 7/7 [00:48<00:00,  6.98s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.4178\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3333\n",
      "Epoch 1: Train Loss = 1.1210, Val Loss = 1.0961\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.45s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.2081\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3021\n",
      "Epoch 2: Train Loss = 1.0779, Val Loss = 1.1083\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.62s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1371\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3229\n",
      "Epoch 3: Train Loss = 1.0874, Val Loss = 1.1967\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.36s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1131\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3958\n",
      "Epoch 4: Train Loss = 1.0777, Val Loss = 1.1367\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.39s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0898\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4479\n",
      "Epoch 5: Train Loss = 1.0549, Val Loss = 1.1027\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.44s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0651\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5312\n",
      "Epoch 6: Train Loss = 1.0352, Val Loss = 1.0749\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.61s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0457\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5208\n",
      "Epoch 7: Train Loss = 1.0138, Val Loss = 1.0260\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.69s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0210\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4375\n",
      "Epoch 8: Train Loss = 0.9864, Val Loss = 0.9945\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.55s/it]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9908\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5417\n",
      "Epoch 9: Train Loss = 0.9449, Val Loss = 1.0518\n",
      "100%|██████████| 7/7 [00:47<00:00,  6.76s/it]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9463\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5417\n",
      "Epoch 10: Train Loss = 0.9067, Val Loss = 0.9826\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.50s/it]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9250\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5104\n",
      "Epoch 11: Train Loss = 0.8876, Val Loss = 0.9823\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.46s/it]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.9336\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4896\n",
      "Epoch 12: Train Loss = 0.8681, Val Loss = 0.9558\n",
      "100%|██████████| 7/7 [00:47<00:00,  6.83s/it]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8821\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6042\n",
      "Epoch 13: Train Loss = 0.8262, Val Loss = 0.9004\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.45s/it]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8334\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.5521\n",
      "Epoch 14: Train Loss = 0.7844, Val Loss = 1.0011\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.34s/it]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.8139\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6146\n",
      "Epoch 15: Train Loss = 0.7535, Val Loss = 0.9278\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.71s/it]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7644\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6562\n",
      "Epoch 16: Train Loss = 0.7314, Val Loss = 0.9120\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.60s/it]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7492\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6250\n",
      "Epoch 17: Train Loss = 0.7210, Val Loss = 0.9205\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.44s/it]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7387\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6354\n",
      "Epoch 18: Train Loss = 0.7137, Val Loss = 0.9240\n",
      "100%|██████████| 7/7 [00:47<00:00,  6.77s/it]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7327\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6250\n",
      "Epoch 19: Train Loss = 0.7072, Val Loss = 0.9175\n",
      "100%|██████████| 7/7 [00:49<00:00,  7.00s/it]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7257\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6250\n",
      "Epoch 20: Train Loss = 0.7047, Val Loss = 0.9026\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.55s/it]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7218\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6562\n",
      "Epoch 21: Train Loss = 0.6996, Val Loss = 0.8915\n",
      "100%|██████████| 7/7 [00:43<00:00,  6.27s/it]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7170\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6562\n",
      "Epoch 22: Train Loss = 0.6961, Val Loss = 0.8838\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.35s/it]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7097\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6771\n",
      "Epoch 23: Train Loss = 0.6862, Val Loss = 0.9109\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.31s/it]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7027\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6562\n",
      "Epoch 24: Train Loss = 0.6808, Val Loss = 0.9324\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.43s/it]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.7016\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6667\n",
      "Epoch 25: Train Loss = 0.6776, Val Loss = 0.9213\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.31s/it]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6936\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6562\n",
      "Epoch 26: Train Loss = 0.6711, Val Loss = 0.9138\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.49s/it]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6908\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6354\n",
      "Epoch 27: Train Loss = 0.6662, Val Loss = 0.9325\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.40s/it]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6872\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6771\n",
      "Epoch 28: Train Loss = 0.6603, Val Loss = 0.9338\n",
      "100%|██████████| 7/7 [00:49<00:00,  7.01s/it]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6810\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6771\n",
      "Epoch 29: Train Loss = 0.6573, Val Loss = 0.9172\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.54s/it]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6802\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6771\n",
      "Epoch 30: Train Loss = 0.6518, Val Loss = 0.9383\n",
      "100%|██████████| 7/7 [00:47<00:00,  6.74s/it]\n",
      "Epoch 30 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6710\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6667\n",
      "Epoch 31: Train Loss = 0.6509, Val Loss = 0.9377\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.35s/it]\n",
      "Epoch 31 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6695\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6667\n",
      "Epoch 32: Train Loss = 0.6502, Val Loss = 0.9363\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.39s/it]\n",
      "Epoch 32 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6685\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6771\n",
      "Epoch 33: Train Loss = 0.6495, Val Loss = 0.9323\n",
      "100%|██████████| 7/7 [00:43<00:00,  6.24s/it]\n",
      "Epoch 33 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6675\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6875\n",
      "Epoch 34: Train Loss = 0.6490, Val Loss = 0.9256\n",
      "100%|██████████| 7/7 [00:43<00:00,  6.23s/it]\n",
      "Epoch 34 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6670\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6875\n",
      "Epoch 35: Train Loss = 0.6486, Val Loss = 0.9222\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.35s/it]\n",
      "Epoch 35 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6662\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7083\n",
      "Epoch 36: Train Loss = 0.6482, Val Loss = 0.9204\n",
      "100%|██████████| 7/7 [00:43<00:00,  6.26s/it]\n",
      "Epoch 36 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6657\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.6979\n",
      "Epoch 37: Train Loss = 0.6476, Val Loss = 0.9204\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.47s/it]\n",
      "Epoch 37 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6649\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7083\n",
      "Epoch 38: Train Loss = 0.6473, Val Loss = 0.9176\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.40s/it]\n",
      "Epoch 38 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6647\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7083\n",
      "Epoch 39: Train Loss = 0.6470, Val Loss = 0.9162\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.43s/it]\n",
      "Epoch 39 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6643\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7188\n",
      "Epoch 40: Train Loss = 0.6465, Val Loss = 0.9176\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.34s/it]\n",
      "Epoch 40 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6640\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7188\n",
      "Epoch 41: Train Loss = 0.6462, Val Loss = 0.9155\n",
      "100%|██████████| 7/7 [00:50<00:00,  7.24s/it]\n",
      "Epoch 41 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6636\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7188\n",
      "Epoch 42: Train Loss = 0.6454, Val Loss = 0.9179\n",
      "100%|██████████| 7/7 [00:45<00:00,  6.51s/it]\n",
      "Epoch 42 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6627\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7188\n",
      "Epoch 43: Train Loss = 0.6449, Val Loss = 0.9176\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.38s/it]\n",
      "Epoch 43 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6626\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7188\n",
      "Epoch 44: Train Loss = 0.6447, Val Loss = 0.9157\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.59s/it]\n",
      "Epoch 44 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6621\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 45: Train Loss = 0.6444, Val Loss = 0.9139\n",
      "100%|██████████| 7/7 [00:47<00:00,  6.75s/it]\n",
      "Epoch 45 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6614\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 46: Train Loss = 0.6444, Val Loss = 0.9139\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.42s/it]\n",
      "Epoch 46 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6613\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 47: Train Loss = 0.6443, Val Loss = 0.9140\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.32s/it]\n",
      "Epoch 47 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6613\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 48: Train Loss = 0.6443, Val Loss = 0.9140\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.29s/it]\n",
      "Epoch 48 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6612\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 49: Train Loss = 0.6442, Val Loss = 0.9142\n",
      "100%|██████████| 7/7 [00:44<00:00,  6.39s/it]\n",
      "Epoch 49 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 0.6612\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.7292\n",
      "Epoch 50: Train Loss = 0.6442, Val Loss = 0.9142\n",
      "-- >> End of training phase << --\n",
      "Saved loss plot to loss_plots\\loss_plot_exp1_lr0.001_lambda60.png\n",
      "Testing on the entire test stream...\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 from test stream --\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.24s/it]\n",
      "> Eval on experience 0 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp000 = 1.9468\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp000 = 0.2222\n",
      "-- Starting eval on experience 1 from test stream --\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n",
      "> Eval on experience 1 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp001 = 0.7380\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp001 = 0.5833\n",
      "-- Starting eval on experience 2 from test stream --\n",
      "100%|██████████| 8/8 [00:27<00:00,  3.41s/it]\n",
      "> Eval on experience 2 from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Exp002 = 1.6555\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Exp002 = 0.3509\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream = 1.5480\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream = 0.3718\n",
      "Test results: {'Top1_Acc_Epoch/train_phase/train_stream': 0.7291666666666666, 'Loss_Epoch/train_phase/train_stream': 0.6611629854887724, 'Top1_Acc_Exp/eval_phase/test_stream/Exp000': 0.2222222222222222, 'Loss_Exp/eval_phase/test_stream/Exp000': 1.946841061115265, 'Top1_Acc_Exp/eval_phase/test_stream/Exp001': 0.5833333333333334, 'Loss_Exp/eval_phase/test_stream/Exp001': 0.7380191162228584, 'Top1_Acc_Exp/eval_phase/test_stream/Exp002': 0.3508771929824561, 'Loss_Exp/eval_phase/test_stream/Exp002': 1.655501531927209, 'Top1_Acc_Stream/eval_phase/test_stream': 0.3717948717948718, 'Loss_Stream/eval_phase/test_stream': 1.5479664905713155}\n",
      "\n",
      "=== Start of Experience 2 ===\n",
      "Training Experience 2 for 50 epochs...\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 35/35 [04:13<00:00,  7.23s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.2428\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3621\n",
      "Epoch 1: Train Loss = 1.0941, Val Loss = 1.1072\n",
      "100%|██████████| 35/35 [04:08<00:00,  7.11s/it]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1322\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.3602\n",
      "Epoch 2: Train Loss = 1.0758, Val Loss = 1.0895\n",
      "100%|██████████| 35/35 [04:05<00:00,  7.01s/it]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.1108\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4234\n",
      "Epoch 3: Train Loss = 1.0472, Val Loss = 1.0615\n",
      "100%|██████████| 35/35 [04:15<00:00,  7.31s/it]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0698\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4559\n",
      "Epoch 4: Train Loss = 1.0051, Val Loss = 1.0387\n",
      "100%|██████████| 35/35 [04:06<00:00,  7.05s/it]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0476\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4617\n",
      "Epoch 5: Train Loss = 1.0054, Val Loss = 1.0318\n",
      "100%|██████████| 35/35 [04:13<00:00,  7.24s/it]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0548\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4598\n",
      "Epoch 6: Train Loss = 0.9824, Val Loss = 1.0151\n",
      "100%|██████████| 35/35 [04:12<00:00,  7.21s/it]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0214\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4847\n",
      "Epoch 7: Train Loss = 0.9733, Val Loss = 1.0249\n",
      "100%|██████████| 35/35 [04:09<00:00,  7.13s/it]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream = 1.0351\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream = 0.4923\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ceba184e038ccd0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
