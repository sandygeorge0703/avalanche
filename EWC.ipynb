{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries",
   "id": "cf83250ee7f63eeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Sampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm  \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from avalanche.models import SimpleCNN\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,\\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,\\\n",
    "    disk_usage_metrics\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training import EWC\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "from models.cnn_models import SimpleCNN\n",
    "\n",
    "\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)  # If using a GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ],
   "id": "b8061d6902ab803b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define filepaths as constant",
   "id": "41b9047ff69b1348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define file paths as constants\n",
    "CSV_FILE_PATH = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'\n",
    "ROOT_DIR_PATH = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\\print24'\n",
    "\n",
    "csv_file = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'  # Path to the CSV file\n",
    "root_dir = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\\print24'  # Path to the image directory"
   ],
   "id": "1abad502d1ece263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data into DataFrame and filter print24",
   "id": "cb52d1ca4b474151"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data into a DataFrame for easier processing\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Limit dataset to the images between row indices 454 and 7058 (inclusive)\n",
    "#data_limited = data.iloc[454:7059].reset_index(drop=True)\n",
    "\n",
    "# Filter the dataset to only include images containing \"print24\"\n",
    "data_filtered = data[data.iloc[:, 0].str.contains('print24', na=False)]\n",
    "\n",
    "# Update the first column to contain only the image filenames\n",
    "data_filtered.iloc[:, 0] = data_filtered.iloc[:, 0].str.replace(r'.*?/(image-\\d+\\.jpg)', r'\\1', regex=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"First rows of filtered DataFrame:\")\n",
    "print(data_filtered.head())\n",
    "\n",
    "# Display the last few rows of the updated DataFrame\n",
    "print(\"\\nLast rows of filtered DataFrame:\")\n",
    "print(data_filtered.tail())"
   ],
   "id": "a798b6f5b4adbf65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysing the target hotend temperature column",
   "id": "5d0423ac0a8bc90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract unique temperatures in the 'target_hotend' column and sort them\n",
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())  # Sort temperatures in ascending order\n",
    "\n",
    "# Calculate the full range of temperatures (min and max)\n",
    "temperature_min = data_filtered['target_hotend'].min()\n",
    "temperature_max = data_filtered['target_hotend'].max()\n",
    "\n",
    "# Print the unique temperatures (sorted), count, and full range\n",
    "print(\"\\nUnique target hotend temperatures in the dataset (sorted):\")\n",
    "print(unique_temperatures)\n",
    "print(f\"\\nNumber of unique target hotend temperatures: {len(unique_temperatures)}\")\n",
    "print(f\"Temperature range: {temperature_min} to {temperature_max}\")"
   ],
   "id": "f29c405cf341b9ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a random temperature sub list and new dataframes with equal class distribution",
   "id": "a22e7cd8879fcbeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract unique temperatures and sort them\n",
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())  # Sort temperatures in ascending order\n",
    "\n",
    "# Check if we have enough unique temperatures to select from\n",
    "if len(unique_temperatures) >= 50:\n",
    "    # Select the lowest and highest temperatures\n",
    "    temperature_min = unique_temperatures[0]\n",
    "    temperature_max = unique_temperatures[-1]\n",
    "\n",
    "    # Remove the lowest and highest temperatures from the unique temperatures list\n",
    "    remaining_temperatures = [temp for temp in unique_temperatures if temp != temperature_min and temp != temperature_max]\n",
    "\n",
    "    # Randomly select 40 other temperatures from the remaining ones\n",
    "    random_temperatures = random.sample(remaining_temperatures, 40)\n",
    "\n",
    "    # Add the random temperatures to the temperature_sublist\n",
    "    temperature_sublist = [temperature_min, temperature_max] + random_temperatures\n",
    "    \n",
    "    # Sort from lowest to highest hotend temperature\n",
    "    temperature_sublist = sorted(temperature_sublist)\n",
    "\n",
    "    # Print the temperature sublist\n",
    "    print(\"\\nTemperature sublist:\")\n",
    "    print(temperature_sublist)\n",
    "    \n",
    "    # Split into three experience groups\n",
    "    split_size = len(temperature_sublist) // 3\n",
    "    experience_1 = temperature_sublist[:split_size]  # First third\n",
    "    experience_2 = temperature_sublist[split_size:2*split_size]  # Second third\n",
    "    experience_3 = temperature_sublist[2*split_size:]  # Last third\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nExperience Group 1:\", experience_1)\n",
    "    print(\"\\nExperience Group 2:\", experience_2)\n",
    "    print(\"\\nExperience Group 3:\", experience_3)\n",
    "else:\n",
    "    print(\"Not enough unique temperatures to select from. At least 50 unique temperatures are required.\")\n",
    "    experience_1 = experience_2 = experience_3 = []\n",
    "\n",
    "# Initialize a dictionary to store DataFrames for each class per experience\n",
    "experience_datasets = {1: {}, 2: {}, 3: {}}\n",
    "\n",
    "# Iterate through the three experience groups\n",
    "for exp_id, experience_temps in enumerate([experience_1, experience_2, experience_3], start=1):\n",
    "    if not experience_temps:\n",
    "        print(f\"Skipping Experience {exp_id} due to insufficient temperatures.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing Experience {exp_id} with temperatures: {experience_temps}...\")\n",
    "\n",
    "    # Filter the dataset based on the current experience's temperature range\n",
    "    exp_data = data_filtered[data_filtered['target_hotend'].isin(experience_temps)]\n",
    "    \n",
    "    # Check if exp_data is empty after filtering\n",
    "    if exp_data.empty:\n",
    "        print(f\"No data found for Experience {exp_id} with temperatures {experience_temps}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create a dictionary to store class-wise data for this experience\n",
    "    class_datasets = {}\n",
    "\n",
    "    # Iterate through each class (0, 1, 2) and filter data\n",
    "    for class_id in [0, 1, 2]:\n",
    "        class_data = exp_data[exp_data['hotend_class'] == class_id]\n",
    "        \n",
    "        if class_data.empty:\n",
    "            print(f\"Warning: Class {class_id} in Experience {exp_id} has no data!\")\n",
    "        else:\n",
    "            class_datasets[class_id] = class_data\n",
    "            print(f\"Class {class_id} dataset size in Experience {exp_id}: {len(class_data)}\")\n",
    "\n",
    "    # Ensure that all classes have data before proceeding to balance\n",
    "    if len(class_datasets) != 3:\n",
    "        print(f\"Skipping Experience {exp_id} because one or more classes are missing data!\")\n",
    "        continue  # Skip processing this experience if any class has no data\n",
    "\n",
    "    # Find the smallest class size in this experience\n",
    "    min_class_size = min(len(class_datasets[class_id]) for class_id in class_datasets)\n",
    "    print(f\"Smallest class size in Experience {exp_id}: {min_class_size}\")\n",
    "\n",
    "    # Balance the dataset for this experience\n",
    "    balanced_data = []\n",
    "\n",
    "    for class_id in class_datasets:\n",
    "        class_data = class_datasets[class_id]\n",
    "        # Randomly sample 'min_class_size' images from the class data to balance class distribution\n",
    "        sampled_class_data = class_data.sample(n=min_class_size, random_state=42)  # Sample equally\n",
    "        balanced_data.append(sampled_class_data)\n",
    "\n",
    "    # Combine all class data for this experience into one balanced dataset\n",
    "    balanced_dataset = pd.concat(balanced_data).reset_index(drop=True)\n",
    "\n",
    "    # Shuffle the final balanced dataset\n",
    "    balanced_dataset = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Store the balanced dataset in the experience_datasets dictionary\n",
    "    experience_datasets[exp_id] = balanced_dataset\n",
    "\n",
    "    # Print summary for this experience\n",
    "    print(f\"\\nBalanced dataset size for Experience {exp_id}: {len(balanced_dataset)}\")\n",
    "    print(\"Number of images in each class after balancing:\")\n",
    "\n",
    "    for class_id in [0, 1, 2]:\n",
    "        class_count = len(balanced_dataset[balanced_dataset['hotend_class'] == class_id])\n",
    "        print(f\"Class {class_id}: {class_count} images\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "for exp_id in [1, 2, 3]:\n",
    "    if exp_id in experience_datasets:\n",
    "        print(f\"\\nFirst five rows of Experience {exp_id} dataset:\")\n",
    "        print(experience_datasets[exp_id].head())"
   ],
   "id": "47acde363e257d88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking the class distribution of all the experience datasets",
   "id": "32f7aecc5666b507"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Check if the experience dataset exists (in case an experience was skipped)\n",
    "    if exp_id in experience_datasets:\n",
    "        # Select only the 'img_path' and 'hotend_class' columns\n",
    "        balanced_dataset_filtered = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "\n",
    "        # Check the class distribution in the filtered dataset\n",
    "        class_distribution = balanced_dataset_filtered['hotend_class'].value_counts()\n",
    "        \n",
    "        # Print the class distribution for the current experience\n",
    "        print(f\"\\nClass distribution for Experience {exp_id}:\")\n",
    "        print(class_distribution)"
   ],
   "id": "84535699bd125895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Printing the indices, the classes, and the number of images in each class",
   "id": "77f6b191f590e6f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Check if the experience dataset exists (in case an experience was skipped)\n",
    "    if exp_id in experience_datasets:\n",
    "        # Select only the 'img_path' and 'hotend_class' columns for the current experience dataset\n",
    "        balanced_dataset_filtered = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "\n",
    "        # Get the class distribution for the current experience dataset\n",
    "        class_distribution = balanced_dataset_filtered['hotend_class'].value_counts()\n",
    "        \n",
    "        # Step 1: Print the indices, the classes, and the number of images in each class\n",
    "        print(f\"\\n--- Experience {exp_id} ---\")\n",
    "        for class_label in class_distribution.index:\n",
    "            # Get all indices for the current class\n",
    "            class_indices = balanced_dataset_filtered[balanced_dataset_filtered['hotend_class'] == class_label].index.tolist()\n",
    "\n",
    "            # Count the number of images for the current class\n",
    "            num_images_in_class = len(class_indices)\n",
    "\n",
    "            # Print the details for this class\n",
    "            print(f\"\\nClass: {class_label} (Total images: {num_images_in_class})\")\n",
    "            print(\"Indices: \", class_indices)\n",
    "            print(f\"Number of images in class {class_label}: {num_images_in_class}\")\n",
    "\n",
    "        # Step 2: Get the number of unique classes\n",
    "        num_classes = len(class_distribution)\n",
    "\n",
    "        # Step 3: Set a small batch size\n",
    "        small_batch_size = 15  # You can change this to a value like 32, 64, etc.\n",
    "\n",
    "        # Step 4: Calculate the number of samples per class per batch\n",
    "        samples_per_class = small_batch_size // num_classes  # Ensure it's divisible\n",
    "\n",
    "        # Make sure we don't ask for more samples than available in the smallest class\n",
    "        samples_per_class = min(samples_per_class, class_distribution.min())\n",
    "\n",
    "        # Step 5: Calculate the total batch size\n",
    "        batch_size = samples_per_class * num_classes\n",
    "\n",
    "        print(f\"\\nRecommended Small Batch Size for Experience {exp_id}: {batch_size}\")\n",
    "        print(f\"Samples per class in Experience {exp_id}: {samples_per_class}\")\n",
    "        print(\"-\" * 50)  # To separate each experience's results"
   ],
   "id": "49fc331070b94ce1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## At this point a balanced dataset for each experience has been created",
   "id": "b6f62e66e01398b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create training, validation, and testing datasets",
   "id": "1be9c1eebf9fd709"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Check if the experience dataset exists (in case an experience was skipped)\n",
    "    if exp_id in experience_datasets:\n",
    "        # Select only the 'img_path' and 'hotend_class' columns for the current experience dataset\n",
    "        balanced_dataset_filtered = experience_datasets[exp_id][['img_path', 'hotend_class']]\n",
    "\n",
    "        # Number of images per class (this will be the same after balancing)\n",
    "        num_images_per_class = len(balanced_dataset_filtered) // 3  # Assuming there are 3 classes (0, 1, 2)\n",
    "\n",
    "        # Calculate the number of samples per class for train, validation, and test sets\n",
    "        train_size = int(0.8 * num_images_per_class)\n",
    "        valid_size = int(0.1 * num_images_per_class)\n",
    "        test_size = num_images_per_class - train_size - valid_size\n",
    "\n",
    "        # Lists to hold indices for each class's dataset (train, validation, test)\n",
    "        train_indices, valid_indices, test_indices = [], [], []\n",
    "\n",
    "        # Split the data by class (assuming classes are 0, 1, 2)\n",
    "        for class_label in [0, 1, 2]:\n",
    "            class_data = balanced_dataset_filtered[balanced_dataset_filtered['hotend_class'] == class_label].index.tolist()\n",
    "\n",
    "            # Shuffle the indices of the current class\n",
    "            random.shuffle(class_data)\n",
    "\n",
    "            # Split the indices for each class into train, validation, and test\n",
    "            train_indices.extend(class_data[:train_size])\n",
    "            valid_indices.extend(class_data[train_size:train_size + valid_size])\n",
    "            test_indices.extend(class_data[train_size + valid_size:])\n",
    "\n",
    "        # Sort the indices to ensure consistent processing\n",
    "        train_indices, valid_indices, test_indices = sorted(train_indices), sorted(valid_indices), sorted(test_indices)\n",
    "\n",
    "        # Create DataFrames for train, validation, and test sets based on the indices\n",
    "        globals()[f'train_{exp_id}'] = balanced_dataset_filtered.loc[train_indices].reset_index(drop=True)\n",
    "        globals()[f'valid_{exp_id}'] = balanced_dataset_filtered.loc[valid_indices].reset_index(drop=True)\n",
    "        globals()[f'test_{exp_id}'] = balanced_dataset_filtered.loc[test_indices].reset_index(drop=True)\n",
    "\n",
    "        # Count class distribution for each of the datasets\n",
    "        def count_class_distribution(indices):\n",
    "            class_counts = [0, 0, 0]  # Assuming 3 classes (0, 1, 2)\n",
    "            for index in indices:\n",
    "                class_label = balanced_dataset_filtered.loc[index, 'hotend_class']\n",
    "                class_counts[class_label] += 1\n",
    "            return class_counts\n",
    "\n",
    "        # Count class distribution for each of the datasets\n",
    "        train_class_distribution = count_class_distribution(train_indices)\n",
    "        valid_class_distribution = count_class_distribution(valid_indices)\n",
    "        test_class_distribution = count_class_distribution(test_indices)\n",
    "\n",
    "        # Print the class distribution and dataset sizes\n",
    "        print(f\"\\n--- Experience {exp_id} ---\")\n",
    "        print(f\"Train set size: {len(train_indices)} | Class distribution: {train_class_distribution}\")\n",
    "        print(f\"Validation set size: {len(valid_indices)} | Class distribution: {valid_class_distribution}\")\n",
    "        print(f\"Test set size: {len(test_indices)} | Class distribution: {test_class_distribution}\")\n",
    "\n",
    "        print(f\"Experience {exp_id} datasets created successfully!\\n\")\n",
    "\n",
    "# Now, the datasets are directly available as:\n",
    "# train_1, valid_1, test_1, train_2, valid_2, test_2, train_3, valid_3, test_3"
   ],
   "id": "cb6e91a94bc66fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check for Missing or Invalid Labels in Training, Validation, and Test Data",
   "id": "9c6d9800330d9cf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for any missing labels or invalid labels\n",
    "print(train_1['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(train_1['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(train_2['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(train_2['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(train_3['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(train_3['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(valid_1['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(valid_1['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(valid_2['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(valid_2['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(valid_3['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(valid_3['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(test_1['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(test_1['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(test_2['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(test_2['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values\n",
    "\n",
    "print(test_3['hotend_class'].isnull().sum())  # Count missing labels\n",
    "print(test_3['hotend_class'].unique())  # Check unique labels to ensure there are no unexpected values"
   ],
   "id": "9013740cba3d35b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Balanced Dataset class",
   "id": "8cd987e605164710"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the dataset class\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None):\n",
    "        self.data = data_frame\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Validate that the images exist in the directory\n",
    "        self.valid_indices = self.get_valid_indices()\n",
    "\n",
    "    def get_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "            img_name = self.data.iloc[idx, 0].strip()\n",
    "            img_name = img_name.split('/')[-1]  # Extract file name\n",
    "            \n",
    "            if img_name.startswith(\"image-\"):\n",
    "                try:\n",
    "                    # Ensure we only include images in the valid range\n",
    "                    image_number = int(img_name.split('-')[1].split('.')[0])\n",
    "                    if 4 <= image_number <= 26637:\n",
    "                        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "                        if os.path.exists(full_img_path):\n",
    "                            valid_indices.append(idx)\n",
    "                        else:\n",
    "                            print(f\"Image does not exist: {full_img_path}\")\n",
    "                except ValueError:\n",
    "                    print(f\"Invalid filename format for {img_name}. Skipping...\")\n",
    "        \n",
    "        print(f\"Total valid indices found: {len(valid_indices)}\")  # Debugging output\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Wrap around the index if it exceeds the length of valid indices\n",
    "        idx = idx % len(self.valid_indices)\n",
    "        \n",
    "        # Get the actual index from valid indices\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        img_name = self.data.iloc[actual_idx, 0].strip()\n",
    "        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "    \n",
    "        try:\n",
    "            # Attempt to open the image and convert to RGB\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "    \n",
    "            # Fetch the label and convert it to an integer\n",
    "            label_str = self.data.iloc[actual_idx]['hotend_class']  # Use column name 'hotend_class'\n",
    "            label = int(label_str)  # Ensure label is integer\n",
    "    \n",
    "            # Apply transformations if defined\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "    \n",
    "            return image, label, actual_idx\n",
    "        except (OSError, IOError, ValueError) as e:\n",
    "            # Print error message for debugging\n",
    "            print(f\"Error loading image {full_img_path}: {e}\")\n",
    "    \n",
    "            # Handle gracefully by skipping the corrupted/missing file\n",
    "            # Fetch the next valid index (recursively handle until a valid image is found)\n",
    "            return self.__getitem__((idx + 1) % len(self.valid_indices))"
   ],
   "id": "f9b871fcd654111f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Balanced Batch Sampler class",
   "id": "f288ab510f111651"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, data_frame, batch_size=15, samples_per_class=5):\n",
    "        \"\"\"\n",
    "        data_frame: Pandas DataFrame with image paths and their respective class labels.\n",
    "        batch_size: Total batch size.\n",
    "        samples_per_class: Number of samples to draw from each class per batch.\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.batch_size = batch_size\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.num_classes = len(data_frame['hotend_class'].unique())\n",
    "        \n",
    "        if self.batch_size % self.num_classes != 0:\n",
    "            raise ValueError(\"Batch size must be divisible by the number of classes.\")\n",
    "\n",
    "        self.class_indices = {\n",
    "            class_id: self.data_frame[self.data_frame['hotend_class'] == class_id].index.tolist()\n",
    "            for class_id in self.data_frame['hotend_class'].unique()\n",
    "        }\n",
    "        \n",
    "        # Shuffle class indices initially\n",
    "        for class_id in self.class_indices:\n",
    "            random.shuffle(self.class_indices[class_id])\n",
    "\n",
    "        self.num_samples_per_epoch = sum(len(indices) for indices in self.class_indices.values())\n",
    "        self.indices_used = {class_id: [] for class_id in self.class_indices}\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = []\n",
    "\n",
    "        # Replenish indices for each class\n",
    "        for class_id in self.class_indices:\n",
    "            if not self.class_indices[class_id]:\n",
    "                raise ValueError(f\"Class {class_id} has no samples. Cannot form balanced batches.\")\n",
    "\n",
    "            # Shuffle and use all indices from this class\n",
    "            self.indices_used[class_id] = self.class_indices[class_id].copy()\n",
    "            random.shuffle(self.indices_used[class_id])\n",
    "\n",
    "        # Generate balanced batches\n",
    "        while len(batches) * self.batch_size < self.num_samples_per_epoch:\n",
    "            batch = []\n",
    "            for class_id in self.indices_used:\n",
    "                if len(self.indices_used[class_id]) < self.samples_per_class:\n",
    "                    # If a class runs out of samples, reshuffle and replenish\n",
    "                    self.indices_used[class_id] = self.class_indices[class_id].copy()\n",
    "                    random.shuffle(self.indices_used[class_id])\n",
    "\n",
    "                # Take `samples_per_class` indices from the current class\n",
    "                batch.extend(self.indices_used[class_id][:self.samples_per_class])\n",
    "                self.indices_used[class_id] = self.indices_used[class_id][self.samples_per_class:]\n",
    "\n",
    "            # Shuffle the batch and append\n",
    "            random.shuffle(batch)\n",
    "            batches.append(batch)\n",
    "\n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of batches per epoch\n",
    "        return self.num_samples_per_epoch // self.batch_size"
   ],
   "id": "d5d08879cfc9d799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a dictionary to store datasets and DataLoaders\n",
    "datasets = {}\n",
    "dataloaders = {}\n",
    "\n",
    "# Iterate over all experience datasets (1, 2, 3)\n",
    "for exp_id in [1, 2, 3]:\n",
    "    # Ensure the dataset exists\n",
    "    if f\"train_{exp_id}\" in globals():\n",
    "        train_data = globals()[f\"train_{exp_id}\"]\n",
    "        val_data = globals()[f\"valid_{exp_id}\"]\n",
    "        test_data = globals()[f\"test_{exp_id}\"]\n",
    "\n",
    "        # Create dataset instances\n",
    "        datasets[f\"train_{exp_id}\"] = BalancedDataset(data_frame=train_data, root_dir=root_dir)\n",
    "        datasets[f\"valid_{exp_id}\"] = BalancedDataset(data_frame=val_data, root_dir=root_dir)\n",
    "        datasets[f\"test_{exp_id}\"] = BalancedDataset(data_frame=test_data, root_dir=root_dir)\n",
    "\n",
    "        # Create batch samplers for balanced training\n",
    "        train_sampler = BalancedBatchSampler(data_frame=train_data, batch_size=15, samples_per_class=5)\n",
    "        val_sampler = BalancedBatchSampler(data_frame=val_data, batch_size=15, samples_per_class=5)\n",
    "        test_sampler = BalancedBatchSampler(data_frame=test_data, batch_size=15, samples_per_class=5)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        dataloaders[f\"train_{exp_id}\"] = DataLoader(datasets[f\"train_{exp_id}\"], batch_sampler=train_sampler, shuffle=False)\n",
    "        dataloaders[f\"valid_{exp_id}\"] = DataLoader(datasets[f\"valid_{exp_id}\"], batch_sampler=val_sampler, shuffle=False)\n",
    "        dataloaders[f\"test_{exp_id}\"] = DataLoader(datasets[f\"test_{exp_id}\"], batch_sampler=test_sampler)\n",
    "\n",
    "        # Print dataset lengths\n",
    "        print(f\"   Experience {exp_id} datasets and DataLoaders created successfully!\")\n",
    "        print(f\"   Train dataset length: {len(datasets[f'train_{exp_id}'])}\")\n",
    "        print(f\"   Validation dataset length: {len(datasets[f'valid_{exp_id}'])}\")\n",
    "        print(f\"   Test dataset length: {len(datasets[f'test_{exp_id}'])}\")\n"
   ],
   "id": "d19ddec286b9fbfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check the class distribution of randomly selected batches in train loader (for each experience)",
   "id": "a307e150ad2306b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to print random batches and their class distribution for a given experience group\n",
    "def print_random_batches(exp_id, num_batches=5):\n",
    "    # Ensure the experience group exists\n",
    "    if f\"train_{exp_id}\" not in dataloaders:\n",
    "        print(f\"Experience {exp_id} train loader not found!\")\n",
    "        return\n",
    "\n",
    "    train_loader = dataloaders[f\"train_{exp_id}\"]  # Get the correct DataLoader\n",
    "\n",
    "    print(f\"\\n  Random Batches for Experience {exp_id}  \")\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        # Get the next batch from the loader\n",
    "        batch_images, batch_labels, _ = next(iter(train_loader))  # Get images, labels, and indices\n",
    "\n",
    "        # Calculate the class distribution in this batch\n",
    "        class_distribution = Counter(batch_labels.tolist())  # Convert tensor to list for counting\n",
    "\n",
    "        # Print details\n",
    "        print(f\"\\nBatch Class Distribution: {dict(class_distribution)}\")\n",
    "        print(f\"Actual Labels: {batch_labels.tolist()}\")\n",
    "        print(f\"Image Tensor Shape: {batch_images.shape}\")\n",
    "        print(f\"Min Pixel Value: {batch_images.min()}, Max Pixel Value: {batch_images.max()}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example Usage:\n",
    "# Print batches for Experience 1\n",
    "print_random_batches(exp_id=1, num_batches=5)\n",
    "\n",
    "# Print batches for Experience 2\n",
    "print_random_batches(exp_id=2, num_batches=5)\n",
    "\n",
    "# Print batches for Experience 3\n",
    "print_random_batches(exp_id=3, num_batches=5)"
   ],
   "id": "e3abc023e3310c69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check class distribution of random batches from training, validation and testing data",
   "id": "92c59fde6c535c1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_label_batch_from_loader(exp_id, dataset_type):\n",
    "    \"\"\"\n",
    "    Fetch and print a batch of labels from the data loader for a specific experience group.\n",
    "\n",
    "    Args:\n",
    "        exp_id (int): The experience group number (1, 2, or 3).\n",
    "        dataset_type (str): The dataset type - 'train', 'valid', or 'test'.\n",
    "    \"\"\"\n",
    "    loader_key = f\"{dataset_type}_{exp_id}\"  # Example: 'train_1', 'valid_2', 'test_3'\n",
    "\n",
    "    # Ensure the requested DataLoader exists\n",
    "    if loader_key not in dataloaders:\n",
    "        print(f\"DataLoader for {loader_key} not found!\")\n",
    "        return\n",
    "\n",
    "    loader = dataloaders[loader_key]  # Get the correct DataLoader\n",
    "\n",
    "    # Fetch one batch\n",
    "    data_iter = iter(loader)\n",
    "    batch_images, batch_labels, _ = next(data_iter)  # Extract batch data\n",
    "\n",
    "    # Print batch label details\n",
    "    print(f\"\\nExperience {exp_id} - {dataset_type.capitalize()} Set - Sample Label Batch:\")\n",
    "    print(f\"Labels Tensor: {batch_labels}\")\n",
    "    print(f\"Labels as List: {batch_labels.tolist()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Example Usage:\n",
    "print_label_batch_from_loader(exp_id=1, dataset_type='train')  # Training batch from Experience 1\n",
    "print_label_batch_from_loader(exp_id=2, dataset_type='valid')  # Validation batch from Experience 2\n",
    "print_label_batch_from_loader(exp_id=3, dataset_type='test')   # Test batch from Experience 3"
   ],
   "id": "3c0144382105e22e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting up a new folder for each experiment",
   "id": "6781c72ea237b19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set base directory\n",
    "base_dir = \"experiments\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Function to get the next experiment folder\n",
    "def get_experiment_folder(exp_num):\n",
    "    return os.path.join(base_dir, f\"Experiment_{exp_num:02d}\")  # Keeps two-digit format (01, 02, ..., 10)\n",
    "\n",
    "# Set initial experiment number\n",
    "experiment_num = 1\n",
    "experiment_folder = get_experiment_folder(experiment_num)\n",
    "\n",
    "# Create the main experiment directory if it doesn't exist\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "# Set model path inside experiment folder\n",
    "model_path = os.path.join(experiment_folder, \"best_model.pth\")\n",
    "\n",
    "# Create subdirectories for training, validation, and test confusion matrices\n",
    "train_folder = os.path.join(experiment_folder, \"training_confusion_matrices\")\n",
    "val_folder = os.path.join(experiment_folder, \"validation_confusion_matrices\")\n",
    "test_folder = os.path.join(experiment_folder, \"test_confusion_matrices\")\n",
    "\n",
    "# Ensure that the subdirectories exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Print the directory where results will be saved\n",
    "print(f\"Saving results to: {experiment_folder}\")"
   ],
   "id": "1a67c960943dfe8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Display a Random Image from the Dataset with Its Label",
   "id": "a021cfa5257c953e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_random_image_from_experiment(exp_id, dataset_type):\n",
    "    \"\"\"\n",
    "    Selects a random image from the specified dataset (train, valid, or test) for a given experience ID,\n",
    "    loads it, displays it, and saves it to the corresponding experiment folder.\n",
    "\n",
    "    Args:\n",
    "        exp_id (int): The experience group number (1, 2, or 3).\n",
    "        dataset_type (str): The dataset type - 'train', 'valid', or 'test'.\n",
    "    \"\"\"\n",
    "    # Ensure the dataset exists\n",
    "    dataset_key = f\"{dataset_type}_{exp_id}\"  # Example: 'train_1', 'valid_2', 'test_3'\n",
    "    if dataset_key not in datasets:\n",
    "        print(f\"Dataset {dataset_key} not found!\")\n",
    "        return\n",
    "\n",
    "    dataset = datasets[dataset_key]  # Retrieve the dataset\n",
    "    data_frame = dataset.data  # Get the underlying DataFrame\n",
    "\n",
    "    # Ensure the dataset is not empty\n",
    "    if data_frame.empty:\n",
    "        print(f\"Dataset {dataset_key} is empty!\")\n",
    "        return\n",
    "\n",
    "    # Select a random index\n",
    "    random_index = random.choice(data_frame.index)\n",
    "    img_path = os.path.join(root_dir, data_frame.iloc[random_index, 0].strip())\n",
    "    label = data_frame.loc[random_index, 'hotend_class']\n",
    "\n",
    "    # Load and display the image\n",
    "    img = plt.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Label: {label}\")\n",
    "\n",
    "    # Define the path to save the image inside the current experiment folder\n",
    "    experiment_folder = os.path.join(\"experiments\", f\"experiment_{exp_id}\")\n",
    "    os.makedirs(experiment_folder, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "    output_path = os.path.join(experiment_folder, f\"random_{dataset_type}.png\")\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(output_path)\n",
    "    plt.clf()  # Clear the plot to avoid overlaps\n",
    "\n",
    "    print(f\"Image saved to: {output_path}\")\n",
    "\n",
    "# Example Usage:\n",
    "save_random_image_from_experiment(exp_id=1, dataset_type='train')  # Random training image from Experience 1\n",
    "save_random_image_from_experiment(exp_id=2, dataset_type='valid')  # Random validation image from Experience 2\n",
    "save_random_image_from_experiment(exp_id=3, dataset_type='test')   # Random test image from Experience 3"
   ],
   "id": "e1236742af8c2373",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over all experience groups\n",
    "for exp_id in [1, 2, 3]:  \n",
    "    dataset_key = f\"train_{exp_id}\"  # e.g., 'train_1', 'train_2', 'train_3'\n",
    "    \n",
    "    # Ensure the dataset exists\n",
    "    if dataset_key in datasets:\n",
    "        data_frame = datasets[dataset_key].data  # Access the DataFrame from BalancedDataset\n",
    "\n",
    "        # Ensure the dataset is not empty\n",
    "        if not data_frame.empty:\n",
    "            # First image\n",
    "            first_index = data_frame.index[0]\n",
    "            first_image = data_frame.loc[first_index, 'img_path']\n",
    "            first_label = data_frame.loc[first_index, 'hotend_class']\n",
    "            print(f\"Experience {exp_id} - First Image Path: {first_image}, First Label: {first_label}\")\n",
    "\n",
    "            # Last image\n",
    "            last_index = data_frame.index[-1]\n",
    "            last_image = data_frame.loc[last_index, 'img_path']\n",
    "            last_label = data_frame.loc[last_index, 'hotend_class']\n",
    "            print(f\"Experience {exp_id} - Last Image Path: {last_image}, Last Label: {last_label}\\n\")\n",
    "        else:\n",
    "            print(f\"Experience {exp_id} - Training dataset is empty!\\n\")\n",
    "    else:\n",
    "        print(f\"Experience {exp_id} - Training dataset not found!\\n\")"
   ],
   "id": "791d1d863779a6a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating the datasets for the EWC Strategy",
   "id": "ab22d53115398805"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementing a Continual Learning Strategy (EWC) using Avalanche",
   "id": "ef63a9c0fbec3354"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EWCCompatibleBalancedDataset(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (pd.DataFrame): DataFrame with at least two columns: 'image_path' and 'hotend_class'\n",
    "            root_dir (str): Directory where images are stored.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data_frame  # Stores the dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Assuming 'hotend_class' is the target column and 'image_path' is the column for image paths\n",
    "        if 'hotend_class' in self.data.columns:\n",
    "            self.targets = torch.tensor(self.data['hotend_class'].values)  # Labels from the 'hotend_class' column\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame must have a column 'hotend_class' for labels.\")\n",
    "\n",
    "        if 'image_path' not in self.data.columns:\n",
    "            raise ValueError(\"DataFrame must have a column 'image_path' to reference image files.\")\n",
    "\n",
    "        # Validate that the images exist in the directory\n",
    "        self.valid_indices = self.get_valid_indices()\n",
    "\n",
    "    def get_valid_indices(self):\n",
    "        \"\"\"Validates image paths and ensures they exist.\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "            img_name = self.data.iloc[idx, 0].strip()  # Assuming first column is image path\n",
    "            img_name = img_name.split('/')[-1]  # Extract file name\n",
    "            \n",
    "            if img_name.startswith(\"image-\"):\n",
    "                try:\n",
    "                    # Ensure we only include images in the valid range\n",
    "                    image_number = int(img_name.split('-')[1].split('.')[0])\n",
    "                    if 4 <= image_number <= 26637:\n",
    "                        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "                        if os.path.exists(full_img_path):\n",
    "                            valid_indices.append(idx)\n",
    "                        else:\n",
    "                            print(f\"Image does not exist: {full_img_path}\")\n",
    "                except ValueError:\n",
    "                    print(f\"Invalid filename format for {img_name}. Skipping...\")\n",
    "        \n",
    "        print(f\"Total valid indices found: {len(valid_indices)}\")  # Debugging output\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Wrap around the index if it exceeds the length of valid indices\n",
    "        idx = idx % len(self.valid_indices)\n",
    "        \n",
    "        # Get the actual index from valid indices\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        img_name = self.data.iloc[actual_idx, 0].strip()\n",
    "        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "        label = self.targets[actual_idx]  # Get the label from the targets tensor\n",
    "    \n",
    "        try:\n",
    "            # Attempt to open the image and convert to RGB\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "    \n",
    "            # Apply transformations if defined\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "    \n",
    "            return image, label\n",
    "        except (OSError, IOError, ValueError) as e:\n",
    "            # Print error message for debugging\n",
    "            print(f\"Error loading image {full_img_path}: {e}\")\n",
    "    \n",
    "            # Handle gracefully by skipping the corrupted/missing file\n",
    "            return self.__getitem__((idx + 1) % len(self.valid_indices))  # Try next valid index\n",
    "\n",
    "    def store_model_parameters(self, model):\n",
    "        \"\"\"Store the model's parameters after training a task\"\"\"\n",
    "        self.model_parameters = {name: param.clone() for name, param in model.named_parameters()}\n",
    "    \n",
    "    def compute_fisher_information(self, model, dataloader, criterion):\n",
    "        \"\"\"Compute Fisher Information for EWC loss calculation\"\"\"\n",
    "        fisher_information = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(model.device), labels.to(model.device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Calculate Fisher Information for each parameter\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher_information[name] += param.grad ** 2 / len(dataloader)\n",
    "        \n",
    "        # Store Fisher Information\n",
    "        self.fisher_information = fisher_information\n",
    "        return fisher_information\n",
    "    \n",
    "    def compute_ewc_loss(self, model, lambda_ewc=1000):\n",
    "        \"\"\"Compute the EWC loss based on stored parameters and Fisher Information\"\"\"\n",
    "        ewc_loss = 0\n",
    "        if hasattr(self, 'model_parameters') and hasattr(self, 'fisher_information'):\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in self.model_parameters:\n",
    "                    param_diff = param - self.model_parameters[name]\n",
    "                    ewc_loss += (self.fisher_information[name] * param_diff ** 2).sum()\n",
    "        \n",
    "        return lambda_ewc * ewc_loss"
   ],
   "id": "8a080a958d713300",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating training, validation and testing datasets to implement EWC",
   "id": "1b3bca2f633ddf40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the transformation (e.g., normalization)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Experience 1\n",
    "train_data_exp1 = train_1.rename(columns={'img_path': 'image_path'})\n",
    "val_data_exp1 = valid_1.rename(columns={'img_path': 'image_path'})\n",
    "test_data_exp1 = test_1.rename(columns={'img_path': 'image_path'})\n",
    "\n",
    "# Wrap the datasets using EWCCompatibleBalancedDataset\n",
    "train_dataset_exp1 = EWCCompatibleBalancedDataset(data_frame=train_data_exp1, root_dir=root_dir, transform=transform)\n",
    "val_dataset_exp1 = EWCCompatibleBalancedDataset(data_frame=val_data_exp1, root_dir=root_dir, transform=transform)\n",
    "test_dataset_exp1 = EWCCompatibleBalancedDataset(data_frame=test_data_exp1, root_dir=root_dir, transform=transform)\n",
    "\n",
    "# Create the BalancedBatchSampler instances for Experience 1\n",
    "train_sampler_exp1 = BalancedBatchSampler(data_frame=train_data_exp1, batch_size=15, samples_per_class=5)\n",
    "val_sampler_exp1 = BalancedBatchSampler(data_frame=val_data_exp1, batch_size=15, samples_per_class=5)\n",
    "test_sampler_exp1 = BalancedBatchSampler(data_frame=test_data_exp1, batch_size=15, samples_per_class=5)\n",
    "\n",
    "# Create DataLoaders for Experience 1\n",
    "train_loader1 = DataLoader(train_dataset_exp1, batch_sampler=train_sampler_exp1, shuffle=False)\n",
    "val_loader1 = DataLoader(val_dataset_exp1, batch_sampler=val_sampler_exp1, shuffle=False)\n",
    "test_loader1 = DataLoader(test_dataset_exp1, batch_sampler=test_sampler_exp1, shuffle=False)\n",
    "\n",
    "# Optionally print a message indicating successful creation of the DataLoader for Experience 1\n",
    "print(\"Experience 1 DataLoaders created successfully!\")\n",
    "\n",
    "# Experience 2\n",
    "train_data_exp2 = train_2.rename(columns={'img_path': 'image_path'})\n",
    "val_data_exp2 = valid_2.rename(columns={'img_path': 'image_path'})\n",
    "test_data_exp2 = test_2.rename(columns={'img_path': 'image_path'})\n",
    "\n",
    "# Wrap the datasets using EWCCompatibleBalancedDataset\n",
    "train_dataset_exp2 = EWCCompatibleBalancedDataset(data_frame=train_data_exp2, root_dir=root_dir, transform=transform)\n",
    "val_dataset_exp2 = EWCCompatibleBalancedDataset(data_frame=val_data_exp2, root_dir=root_dir, transform=transform)\n",
    "test_dataset_exp2 = EWCCompatibleBalancedDataset(data_frame=test_data_exp2, root_dir=root_dir, transform=transform)\n",
    "\n",
    "# Create the BalancedBatchSampler instances for Experience 2\n",
    "train_sampler_exp2 = BalancedBatchSampler(data_frame=train_data_exp2, batch_size=15, samples_per_class=5)\n",
    "val_sampler_exp2 = BalancedBatchSampler(data_frame=val_data_exp2, batch_size=15, samples_per_class=5)\n",
    "test_sampler_exp2 = BalancedBatchSampler(data_frame=test_data_exp2, batch_size=15, samples_per_class=5)\n",
    "\n",
    "# Create DataLoaders for Experience 2\n",
    "train_loader2 = DataLoader(train_dataset_exp2, batch_sampler=train_sampler_exp2, shuffle=False)\n",
    "val_loader2 = DataLoader(val_dataset_exp2, batch_sampler=val_sampler_exp2, shuffle=False)\n",
    "test_loader2 = DataLoader(test_dataset_exp2, batch_sampler=test_sampler_exp2, shuffle=False)\n",
    "\n",
    "# Optionally print a message indicating successful creation of the DataLoader for Experience 2\n",
    "print(\"Experience 2 DataLoaders created successfully!\")\n",
    "\n",
    "# Experience 3\n",
    "train_data_exp3 = train_3.rename(columns={'img_path': 'image_path'})\n",
    "val_data_exp3 = valid_3.rename(columns={'img_path': 'image_path'})\n",
    "test_data_exp3 = test_3.rename(columns={'img_path': 'image_path'})\n",
    "\n",
    "# Wrap the datasets using EWCCompatibleBalancedDataset\n",
    "train_dataset_exp3 = EWCCompatibleBalancedDataset(data_frame=train_data_exp3, root_dir=root_dir, transform=transform)\n",
    "val_dataset_exp3 = EWCCompatibleBalancedDataset(data_frame=val_data_exp3, root_dir=root_dir, transform=transform)\n",
    "test_dataset_exp3 = EWCCompatibleBalancedDataset(data_frame=test_data_exp3, root_dir=root_dir, transform=transform)\n",
    "\n",
    "# Create the BalancedBatchSampler instances for Experience 3\n",
    "train_sampler_exp3 = BalancedBatchSampler(data_frame=train_data_exp3, batch_size=15, samples_per_class=5)\n",
    "val_sampler_exp3 = BalancedBatchSampler(data_frame=val_data_exp3, batch_size=15, samples_per_class=5)\n",
    "test_sampler_exp3 = BalancedBatchSampler(data_frame=test_data_exp3, batch_size=15, samples_per_class=5)\n",
    "\n",
    "# Create DataLoaders for Experience 3\n",
    "train_loader3 = DataLoader(train_dataset_exp3, batch_sampler=train_sampler_exp3, shuffle=False)\n",
    "val_loader3 = DataLoader(val_dataset_exp3, batch_sampler=val_sampler_exp3, shuffle=False)\n",
    "test_loader3 = DataLoader(test_dataset_exp3, batch_sampler=test_sampler_exp3, shuffle=False)\n",
    "\n",
    "# Optionally print a message indicating successful creation of the DataLoader for Experience 3\n",
    "print(\"Experience 3 DataLoaders created successfully!\")"
   ],
   "id": "45de73a63067e725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3629db99517ffcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking the class distribution of random batches from each experience group",
   "id": "12a48882ae9d33ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to print random batches and their class distribution\n",
    "def print_random_batches(train_loader1, num_batches=5):\n",
    "    for _ in range(num_batches):\n",
    "        # Get the next batch from the loader\n",
    "        batch_images, batch_labels = next(iter(train_loader1))  # Get the images, labels, and indices (if needed)\n",
    "        \n",
    "        # Calculate the class distribution in this batch\n",
    "        class_distribution = Counter(batch_labels.tolist())  # Convert tensor to list for counting\n",
    "        \n",
    "        # Print the class distribution for the current batch\n",
    "        print(f\"Class distribution for this batch: {dict(class_distribution)}\")\n",
    "        \n",
    "        # Print the actual labels for the batch (as a list or tensor)\n",
    "        print(\"Actual labels for this batch:\")\n",
    "        print(batch_labels.tolist())  # Converts tensor to list for readability\n",
    "        \n",
    "        # Print the image tensor shape for the batch\n",
    "        print(\"Image tensor shape for the batch:\")\n",
    "        print(batch_images.shape)  # This prints the shape of the image tensor\n",
    "        \n",
    "        # Optionally, print a few details of the image tensors (e.g., min and max values) to understand them\n",
    "        print(\"Min and max values of the image tensors:\")\n",
    "        print(f\"Min: {batch_images.min()}, Max: {batch_images.max()}\")\n",
    "        \n",
    "        # If you want to print the image itself (assuming it's a small size, for visualization)\n",
    "        # You can use something like matplotlib to visualize the images, for example:\n",
    "        # from matplotlib import pyplot as plt\n",
    "        # plt.imshow(batch_images[0].permute(1, 2, 0).numpy())  # assuming 3 channel images\n",
    "        # plt.show()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Print random batches and their class distribution\n",
    "print_random_batches(train_loader1, num_batches=5)\n",
    "print_random_batches(val_loader1, num_batches=5)\n",
    "print_random_batches(test_loader1, num_batches=5)"
   ],
   "id": "732f735662954035",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking datasets attributes",
   "id": "2a3c98b230b13a19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if the dataset has the 'data' and 'targets' attributes\n",
    "def check_dataset(train_dataset_exp1):\n",
    "    # Check the 'data' (images) and 'targets' (labels) attributes\n",
    "    try:\n",
    "        print(f\"Data attribute type: {type(train_dataset_exp1.data)}\")\n",
    "        print(f\"Targets attribute type: {type(train_dataset_exp1.targets)}\")\n",
    "        \n",
    "        # Print the shapes of the data and targets to confirm dimensions\n",
    "        print(f\"Shape of data (images): {train_dataset_exp1.data.shape}\")\n",
    "        print(f\"Shape of targets (labels): {train_dataset_exp1.targets.shape}\")\n",
    "        \n",
    "        # Check the type and a few elements from the dataset\n",
    "        print(\"Sample data and target:\", train_dataset_exp1[0])  # Check the first sample (image, label)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error accessing dataset attributes: {e}\")\n",
    "\n",
    "# Running the function on my dataset\n",
    "check_dataset(train_dataset_exp1)"
   ],
   "id": "20d58bc78cd886cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Debugging statements",
   "id": "ba116c59b279275"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Total training samples: {len(train_dataset_exp1)}\")\n",
    "print(f\"Total test samples: {len(test_dataset_exp1)}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}, CUDA available: {torch.cuda.is_available()}\")"
   ],
   "id": "5a9501fb6a926f9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Debugging function to inspect class distribution across the datasets\n",
    "from collections import Counter\n",
    "\n",
    "def print_class_distribution(dataset_list, dataset_name, sample_size=1000):\n",
    "    \"\"\"Efficiently prints dataset class distributions using a sample.\"\"\"\n",
    "    print(f\"\\nChecking {dataset_name} Dataset Class Distributions...\\n\")\n",
    "\n",
    "    for i, dataset in enumerate(dataset_list):\n",
    "        labels = []\n",
    "        \n",
    "        # Sample a subset of data for efficiency\n",
    "        sample_indices = range(min(sample_size, len(dataset)))  # Limit to `sample_size`\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            labels.append(int(dataset[idx][1].item()))  # Convert tensor label to int\n",
    "        \n",
    "        # Count occurrences of each class\n",
    "        label_counts = Counter(labels)\n",
    "        print(f\"{dataset_name} Dataset {i}: Unique Classes -> {set(label_counts.keys())}, Counts -> {dict(label_counts)}\")\n",
    "\n",
    "# Print class distributions before creating the benchmark\n",
    "print_class_distribution([train_dataset_exp1, train_dataset_exp2, train_dataset_exp3], \"Train\")\n",
    "print_class_distribution([test_dataset_exp1, test_dataset_exp2, test_dataset_exp3], \"Test\")"
   ],
   "id": "7315d9aa9648951f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check if the datasets are in the correct format for using Avalanche",
   "id": "82d062704541d814"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if your dataset is a subclass of torch.utils.data.Dataset\n",
    "isinstance(train_dataset_exp1, torch.utils.data.Dataset)  # This should return True"
   ],
   "id": "2da1b144fe0a7f08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(train_dataset_exp1)  # This should return the total number of samples in the dataset.",
   "id": "ced609bc0cefd4db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image, label = train_dataset_exp1[0]  # Try accessing the first element\n",
    "print(image.shape, label)  # This should return a tensor (image) and an integer (label)"
   ],
   "id": "22630a26b2f54dcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image, label = train_dataset_exp1[0]  # Access the first sample\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")"
   ],
   "id": "4773ffaffb8f7ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for your dataset\n",
    "train_loader = DataLoader(train_dataset_exp1, batch_size=15, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader and check batches\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}: Images shape {images.shape}, Labels: {labels}\")\n",
    "    break  # Check the first batch only"
   ],
   "id": "304193cd55f100d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Classes in train_dataset_exp1:\", set(train_dataset_exp1.targets.numpy()))\n",
    "print(\"Classes in train_dataset_exp2:\", set(train_dataset_exp2.targets.numpy()))\n",
    "print(\"Classes in train_dataset_exp3:\", set(train_dataset_exp3.targets.numpy()))\n",
    "print(train_dataset_exp1)"
   ],
   "id": "b1d3e897a8029224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from avalanche.benchmarks import ni_benchmark\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "\n",
    "# Create a simple benchmark using your dataset\n",
    "benchmark = ni_benchmark(\n",
    "    train_dataset=[train_dataset_exp1, train_dataset_exp2, train_dataset_exp3],\n",
    "    test_dataset=[test_dataset_exp1, test_dataset_exp2, test_dataset_exp3],\n",
    "    n_experiences=3,\n",
    ")\n",
    "\n",
    "# Print the benchmark experiences\n",
    "for experience in benchmark.train_stream:\n",
    "    print(f\"Start of experience: {experience.current_experience}\")\n",
    "    print(f\"Classes in this experience: {experience.classes_in_this_experience}\")"
   ],
   "id": "ee6dc8b1384d6dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementing EWC using Avalanche",
   "id": "ccd760b7b5776e8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##### THIS IS THE VERSION I AM CURRENTLY USING ########\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleCNN\n",
    "from avalanche.logging import TensorboardLogger, TextLogger, InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, forgetting_metrics, StreamConfusionMatrix\n",
    "from torch.utils.data import DataLoader\n",
    "from avalanche.training import EWC\n",
    "from avalanche.benchmarks import ni_benchmark\n",
    "\n",
    "# Generate the benchmark with explicit fixed experience assignment\n",
    "benchmark = ni_benchmark(\n",
    "    train_dataset=[train_dataset_exp1, train_dataset_exp2, train_dataset_exp3],  # List of training datasets\n",
    "    test_dataset=[test_dataset_exp1, test_dataset_exp2, test_dataset_exp3],  # List of test datasets\n",
    "    n_experiences=3,  # 3 experiences, one for each dataset\n",
    "    task_labels=[0,0,0],  # We don't need separate task labels in domain-incremental learning\n",
    "    shuffle=False,  # No shuffling to maintain domain consistency across experiences\n",
    "    seed=1234,  # Reproducibility seed\n",
    "    balance_experiences=True,  # Optional: Ensure balanced experiences if needed\n",
    ")\n",
    "\n",
    "# Print the classes in each experience to verify correctness\n",
    "for experience in benchmark.train_stream:\n",
    "    print(f\"Start of experience {experience.current_experience}\")\n",
    "    print(f\"Classes in this experience: {experience.classes_in_this_experience}\")\n",
    "\n",
    "\n",
    "# Model creation\n",
    "model = SimpleCNN(num_classes=3).to(device)\n",
    "\n",
    "# Define the evaluation plugin and loggers\n",
    "tb_logger = TensorboardLogger()\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    StreamConfusionMatrix(num_classes=3, save_image=False),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger]\n",
    ")\n",
    "\n",
    "# Create the strategy instance (EWC)\n",
    "cl_strategy = EWC(\n",
    "    model=model,  \n",
    "    optimizer=SGD(model.parameters(), lr=0.001, momentum=0.9),  \n",
    "    criterion=CrossEntropyLoss(),  \n",
    "    train_mb_size=4,\n",
    "    train_epochs=1,\n",
    "    eval_mb_size=16,\n",
    "    ewc_lambda=0.4,\n",
    "    evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "# Starting experiment...\n",
    "print(\"Starting experiment...\")\n",
    "results = []\n",
    "\n",
    "# Run the training strategy with your corrected benchmark\n",
    "for experience in benchmark.train_stream:\n",
    "    print(f\"Start of experience: {experience.current_experience}\")\n",
    "    print(f\"Current Classes: {experience.classes_in_this_experience}\")\n",
    "    \n",
    "    # Ensure `start_time` is initialized before timing\n",
    "    start_time = time.time() \n",
    "\n",
    "    # Check if the experience is correctly loaded\n",
    "    print(f\"Experience {experience.current_experience}: {len(experience.dataset)} samples\")\n",
    "\n",
    "    # Create DataLoader for the current experience\n",
    "    if experience.current_experience == 0:\n",
    "        train_loader = train_loader1\n",
    "        test_loader = test_loader1\n",
    "    elif experience.current_experience == 1:\n",
    "        train_loader = train_loader2\n",
    "        test_loader = test_loader2\n",
    "    else:\n",
    "        train_loader = train_loader3\n",
    "        test_loader = test_loader3\n",
    "\n",
    "    # Print batch sizes and input/output shapes in the train_loader\n",
    "    print(f\"Number of batches in this experience: {len(train_loader)}\")\n",
    "    \n",
    "    for i, (train_batch_data, train_batch_labels) in enumerate(train_loader):\n",
    "        print(f\"Batch {i}: Input shape = {train_batch_data.shape}, Labels shape = {train_batch_labels.shape}\")\n",
    "        if i > 5:  # Check for the first 6 batches to avoid printing too much\n",
    "            break\n",
    "    \n",
    "    # Get number of batches for progress bar\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for epoch in range(cl_strategy.train_epochs):  # Manually loop over the epochs\n",
    "        print(f\"Starting epoch {epoch+1}/{cl_strategy.train_epochs}...\")\n",
    "\n",
    "        with tqdm(total=num_batches, desc=f\"Training Experience {experience.current_experience}\") as pbar:\n",
    "            for i, (train_batch_data, train_batch_labels) in enumerate(train_loader):\n",
    "                # Ensure data is on the same device as the model\n",
    "                train_batch_data, train_batch_labels = train_batch_data.to(device), train_batch_labels.to(device)\n",
    "                \n",
    "                # Print the batch shapes at each iteration for further debugging\n",
    "                print(f\"Training Batch {i}: x shape = {train_batch_data.shape}, y shape = {train_batch_labels.shape}\")\n",
    "                \n",
    "                # Pack into a batch (data, labels)\n",
    "                train_batch = (train_batch_data, train_batch_labels)\n",
    "    \n",
    "                # Train the model on the minibatch\n",
    "                try:\n",
    "                    res = cl_strategy.train(experience, num_workers=0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during training in experience {experience.current_experience}, batch {i}: {e}\")\n",
    "                    break\n",
    "    \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Log epoch-level metrics at the end of each epoch\n",
    "        print(f\"End of epoch {epoch+1}: \"\n",
    "              f\"Loss: {res['Loss_Epoch/train_phase/train_stream/Task000']:.4f}, \"\n",
    "              f\"Accuracy: {res['Top1_Acc_Epoch/train_phase/train_stream/Task000']:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "    # Evaluate the model after training the experience\n",
    "    print(\"Evaluating after training...\")\n",
    "    start_eval_time = time.time()\n",
    "    \n",
    "    # Ensure we pass the correct experience object, not a DataLoader\n",
    "    with tqdm(total=1, desc=f\"Evaluating Experience {experience.current_experience}\") as eval_pbar:\n",
    "        try:\n",
    "            res_eval = cl_strategy.eval([experience])  # Pass the experience, not test_loader\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation in experience {experience.current_experience}: {e}\")\n",
    "            continue\n",
    "        eval_pbar.update(1)  # Update progress bar after evaluation\n",
    "    \n",
    "    eval_time = time.time() - start_eval_time\n",
    "    print(f\"Evaluation completed in {eval_time:.2f} seconds\")\n",
    "    print(f\"Evaluation results: {res_eval}\")\n",
    "\n",
    "    results.append(res_eval)\n",
    "    print(\"-\" * 50)  # Add separator between experiences"
   ],
   "id": "cb4041a3cada9648",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b048e975da5737a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
