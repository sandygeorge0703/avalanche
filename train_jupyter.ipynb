{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import libraries ",
   "id": "36671eacce8cb8a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:19:56.480048Z",
     "start_time": "2024-11-14T13:19:46.920318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import labels\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Sampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm  # Import tqdm for progress visualization\n",
    "from models.cnn_models import SimpleCNN\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)  # If using a GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ],
   "id": "afcfee75be6eeefc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define filepaths as constant",
   "id": "71e62fe7b8ae8d44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:19:56.485702Z",
     "start_time": "2024-11-14T13:19:56.481050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define file paths as constants\n",
    "CSV_FILE_PATH = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'\n",
    "ROOT_DIR_PATH = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\\print24'\n",
    "\n",
    "csv_file = r'C:\\Users\\Sandhra George\\avalanche\\data\\dataset.csv'  # Path to the CSV file\n",
    "root_dir = r'C:\\Users\\Sandhra George\\avalanche\\caxton_dataset\\print24'  # Path to the image directory"
   ],
   "id": "f7422c446985ea38",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data into DataFrame",
   "id": "569bddba04d6df1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:20:00.064205Z",
     "start_time": "2024-11-14T13:19:56.487834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data into a DataFrame for easier processing\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Limit dataset to the images between row indices 454 and 7058 (inclusive)\n",
    "#data_limited = data.iloc[454:7059].reset_index(drop=True)\n",
    "\n",
    "# Filter the dataset to only include images containing \"print24\"\n",
    "data_filtered = data[data.iloc[:, 0].str.contains('print24', na=False)]\n",
    "\n",
    "# Update the first column to contain only the image filenames\n",
    "data_filtered.iloc[:, 0] = data_filtered.iloc[:, 0].str.replace(r'.*?/(image-\\d+\\.jpg)', r'\\1', regex=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"First rows of filtered DataFrame:\")\n",
    "print(data_filtered.head())\n",
    "\n",
    "# Display the last few rows of the updated DataFrame\n",
    "print(\"\\nLast rows of filtered DataFrame:\")\n",
    "print(data_filtered.tail())"
   ],
   "id": "ba8aa2adc060c334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of filtered DataFrame:\n",
      "          img_path               timestamp  flow_rate  feed_rate  z_offset  \\\n",
      "99496  image-4.jpg  2020-10-07T11:45:35-86        100        100       0.0   \n",
      "99497  image-5.jpg  2020-10-07T11:45:36-32        100        100       0.0   \n",
      "99498  image-6.jpg  2020-10-07T11:45:36-79        100        100       0.0   \n",
      "99499  image-7.jpg  2020-10-07T11:45:37-26        100        100       0.0   \n",
      "99500  image-8.jpg  2020-10-07T11:45:37-72        100        100       0.0   \n",
      "\n",
      "       target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  img_num  \\\n",
      "99496          205.0  204.86  64.83           654           560        3   \n",
      "99497          205.0  204.62  65.08           654           560        4   \n",
      "99498          205.0  204.62  65.08           654           560        5   \n",
      "99499          205.0  204.62  65.08           654           560        6   \n",
      "99500          205.0  204.62  65.08           654           560        7   \n",
      "\n",
      "       print_id  flow_rate_class  feed_rate_class  z_offset_class  \\\n",
      "99496        24                1                1               1   \n",
      "99497        24                1                1               1   \n",
      "99498        24                1                1               1   \n",
      "99499        24                1                1               1   \n",
      "99500        24                1                1               1   \n",
      "\n",
      "       hotend_class   img_mean    img_std  \n",
      "99496             1  21.047311  31.160557  \n",
      "99497             1  23.239277  32.133393  \n",
      "99498             1  23.686270  31.702140  \n",
      "99499             1  21.645111  31.329910  \n",
      "99500             1  20.883776  32.322521  \n",
      "\n",
      "Last rows of filtered DataFrame:\n",
      "               img_path               timestamp  flow_rate  feed_rate  \\\n",
      "120639  image-26633.jpg  2020-10-07T15:11:15-03        167         39   \n",
      "120640  image-26634.jpg  2020-10-07T15:11:15-48        167         39   \n",
      "120641  image-26635.jpg  2020-10-07T15:11:15-94        167         39   \n",
      "120642  image-26636.jpg  2020-10-07T15:11:16-40        167         39   \n",
      "120643  image-26637.jpg  2020-10-07T15:11:16-85        167         39   \n",
      "\n",
      "        z_offset  target_hotend  hotend    bed  nozzle_tip_x  nozzle_tip_y  \\\n",
      "120639     -0.02          226.0  226.00  65.05           654           560   \n",
      "120640     -0.02          226.0  226.00  65.05           654           560   \n",
      "120641     -0.02          226.0  226.00  65.05           654           560   \n",
      "120642     -0.02          226.0  226.00  65.05           654           560   \n",
      "120643     -0.02          226.0  226.25  64.94           654           560   \n",
      "\n",
      "        img_num  print_id  flow_rate_class  feed_rate_class  z_offset_class  \\\n",
      "120639    26632        24                2                0               1   \n",
      "120640    26633        24                2                0               1   \n",
      "120641    26634        24                2                0               1   \n",
      "120642    26635        24                2                0               1   \n",
      "120643    26636        24                2                0               1   \n",
      "\n",
      "        hotend_class    img_mean    img_std  \n",
      "120639             2   99.738226  71.471386  \n",
      "120640             2  102.395052  72.259859  \n",
      "120641             2  105.231595  74.336885  \n",
      "120642             2   97.592887  67.624012  \n",
      "120643             2   97.459404  67.672459  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysing the hotend temperature column",
   "id": "af909128331dfa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:20:00.079355Z",
     "start_time": "2024-11-14T13:20:00.067708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract unique temperatures in the 'target_hotend' column and sort them\n",
    "unique_temperatures = sorted(data_filtered['target_hotend'].unique())  # Sort temperatures in ascending order\n",
    "\n",
    "# Calculate the full range of temperatures (min and max)\n",
    "temperature_min = data_filtered['target_hotend'].min()\n",
    "temperature_max = data_filtered['target_hotend'].max()\n",
    "\n",
    "# Print the unique temperatures (sorted), count, and full range\n",
    "print(\"\\nUnique target hotend temperatures in the dataset (sorted):\")\n",
    "print(unique_temperatures)\n",
    "print(f\"\\nNumber of unique target hotend temperatures: {len(unique_temperatures)}\")\n",
    "print(f\"Temperature range: {temperature_min}° to {temperature_max}°\")"
   ],
   "id": "5a2e601b8c1a2b3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique target hotend temperatures in the dataset (sorted):\n",
      "[180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0]\n",
      "\n",
      "Number of unique target hotend temperatures: 51\n",
      "Temperature range: 180.0° to 230.0°\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a \"random\" temperature sub list",
   "id": "cc36722b6f015211"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:20:00.092976Z",
     "start_time": "2024-11-14T13:20:00.081388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if we have enough unique temperatures to select from\n",
    "if len(unique_temperatures) >= 50:\n",
    "    # Select the lowest and highest temperatures\n",
    "    temperature_sublist = [temperature_min, temperature_max]\n",
    "\n",
    "    # Remove the lowest and highest temperatures from the unique temperatures list\n",
    "    remaining_temperatures = [temp for temp in unique_temperatures if temp != temperature_min and temp != temperature_max]\n",
    "\n",
    "    # Randomly select 11 other temperatures from the remaining ones\n",
    "    random_temperatures = random.sample(remaining_temperatures, 40)\n",
    "\n",
    "    # Add the random temperatures to the temperature_sublist\n",
    "    temperature_sublist.extend(random_temperatures)\n",
    "    \n",
    "    # Sort from lowest to highest hotend temperature\n",
    "    temperature_sublist = sorted(temperature_sublist)\n",
    "\n",
    "    # Print the temperature sublist\n",
    "    print(\"\\nTemperature sublist:\")\n",
    "    print(temperature_sublist)\n",
    "else:\n",
    "    print(\"Not enough unique temperatures to select from. At least 13 unique temperatures are required.\")"
   ],
   "id": "3d1ee38b740b9aa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature sublist:\n",
      "[180.0, 181.0, 182.0, 183.0, 184.0, 186.0, 187.0, 188.0, 189.0, 191.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 201.0, 202.0, 203.0, 205.0, 208.0, 209.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 226.0, 227.0, 228.0, 229.0, 230.0]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split the dataset into separate DataFrames for each class",
   "id": "6dda5270ad2ca419"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:23:59.501084Z",
     "start_time": "2024-11-14T13:23:59.294612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialise a dictionary to store DataFrames for each class and temperature combination\n",
    "class_temperature_datasets = {}\n",
    "\n",
    "# Iterate over all temperatures in the temperature_sublist\n",
    "for temp in temperature_sublist:\n",
    "    print(f\"Processing temperature: {temp}°\")\n",
    "    \n",
    "    # Filter the dataset for the current temperature\n",
    "    temp_filtered = data_filtered[data_filtered['target_hotend'] == temp]\n",
    "    \n",
    "    # Now, iterate over all classes (0, 1, 2)\n",
    "    for class_id in [0, 1, 2]:  # Ensure we process all classes: 0, 1, 2\n",
    "        # Filter the data for the current class\n",
    "        class_temp_data = temp_filtered[temp_filtered['hotend_class'] == class_id]\n",
    "        \n",
    "        if class_temp_data.empty:\n",
    "            # If there are no images for this class at the current temperature, print a message\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: 0\")\n",
    "        else:\n",
    "            # Shuffle the data (if needed) and store it in the dictionary\n",
    "            class_temperature_datasets[(class_id, temp)] = class_temp_data.sample(frac=1, random_state=42)\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: {len(class_temp_data)}\")\n",
    "    \n",
    "# Print the size of each class-temperature dataset (even if the size is 0)\n",
    "for temp in temperature_sublist:\n",
    "    print(f\"\\nSummary for Temperature: {temp}°\")\n",
    "    for class_id in [0, 1, 2]:\n",
    "        # Retrieve the data for the current temperature and class from the dictionary\n",
    "        if (class_id, temp) in class_temperature_datasets:\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: {len(class_temperature_datasets[(class_id, temp)])}\")\n",
    "        else:\n",
    "            # If no data for this class-temperature combination, print 0\n",
    "            print(f\"Class {class_id} at {temp}° dataset size: 0\")\n",
    "\n",
    "# OPTIONAL: Process the minimum class sizes where no class has zero data, and minimum size is 10 or more\n",
    "min_class_size = float('inf')  # Start with infinity as a comparison baseline\n",
    "min_combinations = []  # List to store class-temperature combinations with the minimum size\n",
    "\n",
    "# Iterate over each temperature in the temperature sublist\n",
    "for temp in temperature_sublist:\n",
    "    \n",
    "    # Check if all classes (0, 1, 2) have non-zero data for the current temperature\n",
    "    class_sizes = []  # List to hold class sizes for the current temperature\n",
    "    valid_temperature = True  # Flag to check if all classes have non-zero data for this temperature\n",
    "    \n",
    "    # Iterate over all classes (0, 1, 2)\n",
    "    for class_id in [0, 1, 2]:\n",
    "        if (class_id, temp) in class_temperature_datasets:\n",
    "            class_size = len(class_temperature_datasets[(class_id, temp)])\n",
    "            class_sizes.append(class_size)\n",
    "            if class_size == 0:\n",
    "                valid_temperature = False\n",
    "                break  # No need to check further, this temperature is invalid\n",
    "        else:\n",
    "            valid_temperature = False\n",
    "            break  # No data for this temperature-class combination, so it's invalid\n",
    "\n",
    "    # If all classes have non-zero data for this temperature, calculate the minimum class size\n",
    "    if valid_temperature:\n",
    "        min_temp_class_size = min(class_sizes)  # Get the minimum class size for this temperature\n",
    "        \n",
    "        # Ensure the minimum class size is at least 10 before considering it\n",
    "        if min_temp_class_size >= 15:\n",
    "            if min_temp_class_size < min_class_size:\n",
    "                # If we find a new minimum, reset the combinations list\n",
    "                min_class_size = min_temp_class_size\n",
    "                min_combinations = [(class_sizes.index(min_class_size), temp)]  # Store the combination\n",
    "            elif min_temp_class_size == min_class_size:\n",
    "                # If it's the same as the current minimum, append to the list\n",
    "                min_combinations.append((class_sizes.index(min_class_size), temp))\n",
    "\n",
    "# Print the minimum class size and the corresponding class ID and temperature\n",
    "if min_class_size != float('inf'):\n",
    "    print(f\"\\nMinimum class size: {min_class_size} (with size >= 10) occurs for the following class-temperature combinations:\")\n",
    "    for class_id, temp in min_combinations:\n",
    "        print(f\"Class {class_id} at {temp}°\")\n",
    "else:\n",
    "    print(\"\\nNo valid temperature with all classes having non-zero data and class size >= 10.\")"
   ],
   "id": "597436ab2068d25a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing temperature: 180.0°\n",
      "Class 0 at 180.0° dataset size: 618\n",
      "Class 1 at 180.0° dataset size: 53\n",
      "Class 2 at 180.0° dataset size: 5\n",
      "Processing temperature: 181.0°\n",
      "Class 0 at 181.0° dataset size: 393\n",
      "Class 1 at 181.0° dataset size: 12\n",
      "Class 2 at 181.0° dataset size: 0\n",
      "Processing temperature: 182.0°\n",
      "Class 0 at 182.0° dataset size: 243\n",
      "Class 1 at 182.0° dataset size: 27\n",
      "Class 2 at 182.0° dataset size: 0\n",
      "Processing temperature: 183.0°\n",
      "Class 0 at 183.0° dataset size: 262\n",
      "Class 1 at 183.0° dataset size: 8\n",
      "Class 2 at 183.0° dataset size: 0\n",
      "Processing temperature: 184.0°\n",
      "Class 0 at 184.0° dataset size: 272\n",
      "Class 1 at 184.0° dataset size: 0\n",
      "Class 2 at 184.0° dataset size: 0\n",
      "Processing temperature: 186.0°\n",
      "Class 0 at 186.0° dataset size: 385\n",
      "Class 1 at 186.0° dataset size: 21\n",
      "Class 2 at 186.0° dataset size: 0\n",
      "Processing temperature: 187.0°\n",
      "Class 0 at 187.0° dataset size: 110\n",
      "Class 1 at 187.0° dataset size: 26\n",
      "Class 2 at 187.0° dataset size: 0\n",
      "Processing temperature: 188.0°\n",
      "Class 0 at 188.0° dataset size: 241\n",
      "Class 1 at 188.0° dataset size: 30\n",
      "Class 2 at 188.0° dataset size: 0\n",
      "Processing temperature: 189.0°\n",
      "Class 0 at 189.0° dataset size: 238\n",
      "Class 1 at 189.0° dataset size: 34\n",
      "Class 2 at 189.0° dataset size: 0\n",
      "Processing temperature: 191.0°\n",
      "Class 0 at 191.0° dataset size: 486\n",
      "Class 1 at 191.0° dataset size: 55\n",
      "Class 2 at 191.0° dataset size: 0\n",
      "Processing temperature: 193.0°\n",
      "Class 0 at 193.0° dataset size: 599\n",
      "Class 1 at 193.0° dataset size: 92\n",
      "Class 2 at 193.0° dataset size: 0\n",
      "Processing temperature: 194.0°\n",
      "Class 0 at 194.0° dataset size: 501\n",
      "Class 1 at 194.0° dataset size: 40\n",
      "Class 2 at 194.0° dataset size: 0\n",
      "Processing temperature: 195.0°\n",
      "Class 0 at 195.0° dataset size: 332\n",
      "Class 1 at 195.0° dataset size: 201\n",
      "Class 2 at 195.0° dataset size: 8\n",
      "Processing temperature: 196.0°\n",
      "Class 0 at 196.0° dataset size: 21\n",
      "Class 1 at 196.0° dataset size: 115\n",
      "Class 2 at 196.0° dataset size: 0\n",
      "Processing temperature: 197.0°\n",
      "Class 0 at 197.0° dataset size: 85\n",
      "Class 1 at 197.0° dataset size: 182\n",
      "Class 2 at 197.0° dataset size: 3\n",
      "Processing temperature: 198.0°\n",
      "Class 0 at 198.0° dataset size: 92\n",
      "Class 1 at 198.0° dataset size: 179\n",
      "Class 2 at 198.0° dataset size: 0\n",
      "Processing temperature: 199.0°\n",
      "Class 0 at 199.0° dataset size: 170\n",
      "Class 1 at 199.0° dataset size: 643\n",
      "Class 2 at 199.0° dataset size: 0\n",
      "Processing temperature: 201.0°\n",
      "Class 0 at 201.0° dataset size: 56\n",
      "Class 1 at 201.0° dataset size: 485\n",
      "Class 2 at 201.0° dataset size: 0\n",
      "Processing temperature: 202.0°\n",
      "Class 0 at 202.0° dataset size: 64\n",
      "Class 1 at 202.0° dataset size: 207\n",
      "Class 2 at 202.0° dataset size: 0\n",
      "Processing temperature: 203.0°\n",
      "Class 0 at 203.0° dataset size: 7\n",
      "Class 1 at 203.0° dataset size: 128\n",
      "Class 2 at 203.0° dataset size: 0\n",
      "Processing temperature: 205.0°\n",
      "Class 0 at 205.0° dataset size: 25\n",
      "Class 1 at 205.0° dataset size: 554\n",
      "Class 2 at 205.0° dataset size: 8\n",
      "Processing temperature: 208.0°\n",
      "Class 0 at 208.0° dataset size: 28\n",
      "Class 1 at 208.0° dataset size: 512\n",
      "Class 2 at 208.0° dataset size: 2\n",
      "Processing temperature: 209.0°\n",
      "Class 0 at 209.0° dataset size: 1\n",
      "Class 1 at 209.0° dataset size: 260\n",
      "Class 2 at 209.0° dataset size: 9\n",
      "Processing temperature: 211.0°\n",
      "Class 0 at 211.0° dataset size: 0\n",
      "Class 1 at 211.0° dataset size: 125\n",
      "Class 2 at 211.0° dataset size: 10\n",
      "Processing temperature: 212.0°\n",
      "Class 0 at 212.0° dataset size: 8\n",
      "Class 1 at 212.0° dataset size: 520\n",
      "Class 2 at 212.0° dataset size: 13\n",
      "Processing temperature: 213.0°\n",
      "Class 0 at 213.0° dataset size: 27\n",
      "Class 1 at 213.0° dataset size: 244\n",
      "Class 2 at 213.0° dataset size: 0\n",
      "Processing temperature: 214.0°\n",
      "Class 0 at 214.0° dataset size: 9\n",
      "Class 1 at 214.0° dataset size: 534\n",
      "Class 2 at 214.0° dataset size: 0\n",
      "Processing temperature: 215.0°\n",
      "Class 0 at 215.0° dataset size: 0\n",
      "Class 1 at 215.0° dataset size: 271\n",
      "Class 2 at 215.0° dataset size: 0\n",
      "Processing temperature: 216.0°\n",
      "Class 0 at 216.0° dataset size: 0\n",
      "Class 1 at 216.0° dataset size: 271\n",
      "Class 2 at 216.0° dataset size: 0\n",
      "Processing temperature: 217.0°\n",
      "Class 0 at 217.0° dataset size: 9\n",
      "Class 1 at 217.0° dataset size: 533\n",
      "Class 2 at 217.0° dataset size: 0\n",
      "Processing temperature: 218.0°\n",
      "Class 0 at 218.0° dataset size: 72\n",
      "Class 1 at 218.0° dataset size: 605\n",
      "Class 2 at 218.0° dataset size: 0\n",
      "Processing temperature: 219.0°\n",
      "Class 0 at 219.0° dataset size: 30\n",
      "Class 1 at 219.0° dataset size: 105\n",
      "Class 2 at 219.0° dataset size: 0\n",
      "Processing temperature: 220.0°\n",
      "Class 0 at 220.0° dataset size: 9\n",
      "Class 1 at 220.0° dataset size: 299\n",
      "Class 2 at 220.0° dataset size: 97\n",
      "Processing temperature: 221.0°\n",
      "Class 0 at 221.0° dataset size: 11\n",
      "Class 1 at 221.0° dataset size: 389\n",
      "Class 2 at 221.0° dataset size: 276\n",
      "Processing temperature: 222.0°\n",
      "Class 0 at 222.0° dataset size: 19\n",
      "Class 1 at 222.0° dataset size: 161\n",
      "Class 2 at 222.0° dataset size: 225\n",
      "Processing temperature: 223.0°\n",
      "Class 0 at 223.0° dataset size: 0\n",
      "Class 1 at 223.0° dataset size: 74\n",
      "Class 2 at 223.0° dataset size: 332\n",
      "Processing temperature: 224.0°\n",
      "Class 0 at 224.0° dataset size: 0\n",
      "Class 1 at 224.0° dataset size: 208\n",
      "Class 2 at 224.0° dataset size: 468\n",
      "Processing temperature: 226.0°\n",
      "Class 0 at 226.0° dataset size: 18\n",
      "Class 1 at 226.0° dataset size: 140\n",
      "Class 2 at 226.0° dataset size: 226\n",
      "Processing temperature: 227.0°\n",
      "Class 0 at 227.0° dataset size: 0\n",
      "Class 1 at 227.0° dataset size: 24\n",
      "Class 2 at 227.0° dataset size: 246\n",
      "Processing temperature: 228.0°\n",
      "Class 0 at 228.0° dataset size: 27\n",
      "Class 1 at 228.0° dataset size: 120\n",
      "Class 2 at 228.0° dataset size: 123\n",
      "Processing temperature: 229.0°\n",
      "Class 0 at 229.0° dataset size: 65\n",
      "Class 1 at 229.0° dataset size: 116\n",
      "Class 2 at 229.0° dataset size: 89\n",
      "Processing temperature: 230.0°\n",
      "Class 0 at 230.0° dataset size: 16\n",
      "Class 1 at 230.0° dataset size: 137\n",
      "Class 2 at 230.0° dataset size: 252\n",
      "\n",
      "Summary for Temperature: 180.0°\n",
      "Class 0 at 180.0° dataset size: 618\n",
      "Class 1 at 180.0° dataset size: 53\n",
      "Class 2 at 180.0° dataset size: 5\n",
      "\n",
      "Summary for Temperature: 181.0°\n",
      "Class 0 at 181.0° dataset size: 393\n",
      "Class 1 at 181.0° dataset size: 12\n",
      "Class 2 at 181.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 182.0°\n",
      "Class 0 at 182.0° dataset size: 243\n",
      "Class 1 at 182.0° dataset size: 27\n",
      "Class 2 at 182.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 183.0°\n",
      "Class 0 at 183.0° dataset size: 262\n",
      "Class 1 at 183.0° dataset size: 8\n",
      "Class 2 at 183.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 184.0°\n",
      "Class 0 at 184.0° dataset size: 272\n",
      "Class 1 at 184.0° dataset size: 0\n",
      "Class 2 at 184.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 186.0°\n",
      "Class 0 at 186.0° dataset size: 385\n",
      "Class 1 at 186.0° dataset size: 21\n",
      "Class 2 at 186.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 187.0°\n",
      "Class 0 at 187.0° dataset size: 110\n",
      "Class 1 at 187.0° dataset size: 26\n",
      "Class 2 at 187.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 188.0°\n",
      "Class 0 at 188.0° dataset size: 241\n",
      "Class 1 at 188.0° dataset size: 30\n",
      "Class 2 at 188.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 189.0°\n",
      "Class 0 at 189.0° dataset size: 238\n",
      "Class 1 at 189.0° dataset size: 34\n",
      "Class 2 at 189.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 191.0°\n",
      "Class 0 at 191.0° dataset size: 486\n",
      "Class 1 at 191.0° dataset size: 55\n",
      "Class 2 at 191.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 193.0°\n",
      "Class 0 at 193.0° dataset size: 599\n",
      "Class 1 at 193.0° dataset size: 92\n",
      "Class 2 at 193.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 194.0°\n",
      "Class 0 at 194.0° dataset size: 501\n",
      "Class 1 at 194.0° dataset size: 40\n",
      "Class 2 at 194.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 195.0°\n",
      "Class 0 at 195.0° dataset size: 332\n",
      "Class 1 at 195.0° dataset size: 201\n",
      "Class 2 at 195.0° dataset size: 8\n",
      "\n",
      "Summary for Temperature: 196.0°\n",
      "Class 0 at 196.0° dataset size: 21\n",
      "Class 1 at 196.0° dataset size: 115\n",
      "Class 2 at 196.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 197.0°\n",
      "Class 0 at 197.0° dataset size: 85\n",
      "Class 1 at 197.0° dataset size: 182\n",
      "Class 2 at 197.0° dataset size: 3\n",
      "\n",
      "Summary for Temperature: 198.0°\n",
      "Class 0 at 198.0° dataset size: 92\n",
      "Class 1 at 198.0° dataset size: 179\n",
      "Class 2 at 198.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 199.0°\n",
      "Class 0 at 199.0° dataset size: 170\n",
      "Class 1 at 199.0° dataset size: 643\n",
      "Class 2 at 199.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 201.0°\n",
      "Class 0 at 201.0° dataset size: 56\n",
      "Class 1 at 201.0° dataset size: 485\n",
      "Class 2 at 201.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 202.0°\n",
      "Class 0 at 202.0° dataset size: 64\n",
      "Class 1 at 202.0° dataset size: 207\n",
      "Class 2 at 202.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 203.0°\n",
      "Class 0 at 203.0° dataset size: 7\n",
      "Class 1 at 203.0° dataset size: 128\n",
      "Class 2 at 203.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 205.0°\n",
      "Class 0 at 205.0° dataset size: 25\n",
      "Class 1 at 205.0° dataset size: 554\n",
      "Class 2 at 205.0° dataset size: 8\n",
      "\n",
      "Summary for Temperature: 208.0°\n",
      "Class 0 at 208.0° dataset size: 28\n",
      "Class 1 at 208.0° dataset size: 512\n",
      "Class 2 at 208.0° dataset size: 2\n",
      "\n",
      "Summary for Temperature: 209.0°\n",
      "Class 0 at 209.0° dataset size: 1\n",
      "Class 1 at 209.0° dataset size: 260\n",
      "Class 2 at 209.0° dataset size: 9\n",
      "\n",
      "Summary for Temperature: 211.0°\n",
      "Class 0 at 211.0° dataset size: 0\n",
      "Class 1 at 211.0° dataset size: 125\n",
      "Class 2 at 211.0° dataset size: 10\n",
      "\n",
      "Summary for Temperature: 212.0°\n",
      "Class 0 at 212.0° dataset size: 8\n",
      "Class 1 at 212.0° dataset size: 520\n",
      "Class 2 at 212.0° dataset size: 13\n",
      "\n",
      "Summary for Temperature: 213.0°\n",
      "Class 0 at 213.0° dataset size: 27\n",
      "Class 1 at 213.0° dataset size: 244\n",
      "Class 2 at 213.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 214.0°\n",
      "Class 0 at 214.0° dataset size: 9\n",
      "Class 1 at 214.0° dataset size: 534\n",
      "Class 2 at 214.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 215.0°\n",
      "Class 0 at 215.0° dataset size: 0\n",
      "Class 1 at 215.0° dataset size: 271\n",
      "Class 2 at 215.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 216.0°\n",
      "Class 0 at 216.0° dataset size: 0\n",
      "Class 1 at 216.0° dataset size: 271\n",
      "Class 2 at 216.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 217.0°\n",
      "Class 0 at 217.0° dataset size: 9\n",
      "Class 1 at 217.0° dataset size: 533\n",
      "Class 2 at 217.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 218.0°\n",
      "Class 0 at 218.0° dataset size: 72\n",
      "Class 1 at 218.0° dataset size: 605\n",
      "Class 2 at 218.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 219.0°\n",
      "Class 0 at 219.0° dataset size: 30\n",
      "Class 1 at 219.0° dataset size: 105\n",
      "Class 2 at 219.0° dataset size: 0\n",
      "\n",
      "Summary for Temperature: 220.0°\n",
      "Class 0 at 220.0° dataset size: 9\n",
      "Class 1 at 220.0° dataset size: 299\n",
      "Class 2 at 220.0° dataset size: 97\n",
      "\n",
      "Summary for Temperature: 221.0°\n",
      "Class 0 at 221.0° dataset size: 11\n",
      "Class 1 at 221.0° dataset size: 389\n",
      "Class 2 at 221.0° dataset size: 276\n",
      "\n",
      "Summary for Temperature: 222.0°\n",
      "Class 0 at 222.0° dataset size: 19\n",
      "Class 1 at 222.0° dataset size: 161\n",
      "Class 2 at 222.0° dataset size: 225\n",
      "\n",
      "Summary for Temperature: 223.0°\n",
      "Class 0 at 223.0° dataset size: 0\n",
      "Class 1 at 223.0° dataset size: 74\n",
      "Class 2 at 223.0° dataset size: 332\n",
      "\n",
      "Summary for Temperature: 224.0°\n",
      "Class 0 at 224.0° dataset size: 0\n",
      "Class 1 at 224.0° dataset size: 208\n",
      "Class 2 at 224.0° dataset size: 468\n",
      "\n",
      "Summary for Temperature: 226.0°\n",
      "Class 0 at 226.0° dataset size: 18\n",
      "Class 1 at 226.0° dataset size: 140\n",
      "Class 2 at 226.0° dataset size: 226\n",
      "\n",
      "Summary for Temperature: 227.0°\n",
      "Class 0 at 227.0° dataset size: 0\n",
      "Class 1 at 227.0° dataset size: 24\n",
      "Class 2 at 227.0° dataset size: 246\n",
      "\n",
      "Summary for Temperature: 228.0°\n",
      "Class 0 at 228.0° dataset size: 27\n",
      "Class 1 at 228.0° dataset size: 120\n",
      "Class 2 at 228.0° dataset size: 123\n",
      "\n",
      "Summary for Temperature: 229.0°\n",
      "Class 0 at 229.0° dataset size: 65\n",
      "Class 1 at 229.0° dataset size: 116\n",
      "Class 2 at 229.0° dataset size: 89\n",
      "\n",
      "Summary for Temperature: 230.0°\n",
      "Class 0 at 230.0° dataset size: 16\n",
      "Class 1 at 230.0° dataset size: 137\n",
      "Class 2 at 230.0° dataset size: 252\n",
      "\n",
      "Minimum class size: 16 (with size >= 10) occurs for the following class-temperature combinations:\n",
      "Class 0 at 230.0°\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a balanced dataset",
   "id": "d96b4ed4148ae5d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:24:04.273646Z",
     "start_time": "2024-11-14T13:24:04.122872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialise a list to store valid datasets for each temperature\n",
    "valid_class_temperature_datasets = []\n",
    "\n",
    "# Process each temperature in the temperature sublist\n",
    "for temp in temperature_sublist:\n",
    "    print(f\"\\nProcessing temperature: {temp}°\")\n",
    "    \n",
    "    # Filter the dataset for the current temperature\n",
    "    temp_filtered = data_filtered[data_filtered['target_hotend'] == temp]\n",
    "    \n",
    "    # Dictionary to store class-specific data for the current temperature\n",
    "    temp_class_data = {}\n",
    "    meets_criteria = True  # Assume the temperature meets criteria until proven otherwise\n",
    "\n",
    "    # Iterate through each class (0, 1, 2)\n",
    "    for class_id in [0, 1, 2]:\n",
    "        # Filter by both class and temperature\n",
    "        class_temp_data = temp_filtered[temp_filtered['hotend_class'] == class_id]\n",
    "        \n",
    "        # Check and print actual dataset size for verification\n",
    "        actual_class_size = len(class_temp_data)\n",
    "        print(f\"Class {class_id} at {temp}° actual dataset size: {actual_class_size}\")\n",
    "\n",
    "        # Only add if the dataset size for this class meets the minimum requirement\n",
    "        if actual_class_size >= min_class_size:\n",
    "            # Sample exactly min_class_size images\n",
    "            temp_class_data[class_id] = class_temp_data.sample(n=min_class_size, random_state=42)\n",
    "        else:\n",
    "            print(f\"Class {class_id} at {temp}° does not have enough images ({actual_class_size}). Skipping this temperature.\")\n",
    "            meets_criteria = False\n",
    "            break  # Stop processing this temperature if any class fails to meet min_class_size\n",
    "\n",
    "    # If all classes at this temperature meet the criteria, add to valid datasets\n",
    "    if meets_criteria:\n",
    "        combined_data_for_temp = pd.concat(temp_class_data.values(), ignore_index=True)\n",
    "        valid_class_temperature_datasets.append(combined_data_for_temp)\n",
    "        print(f\"Temperature {temp}° included with {min_class_size} images per class.\")\n",
    "\n",
    "# Combine all valid datasets for all temperatures into one DataFrame\n",
    "balanced_data = pd.concat(valid_class_temperature_datasets, ignore_index=True) if valid_class_temperature_datasets else pd.DataFrame()\n",
    "\n",
    "# Shuffle the balanced dataset if it’s not empty\n",
    "if not balanced_data.empty:\n",
    "    balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"\\nTotal number of images in the balanced dataset: {len(balanced_data)}\")\n",
    "else:\n",
    "    print(\"No valid data left after filtering temperatures with insufficient class sizes.\")\n",
    "\n",
    "# Print the final class and temperature counts in the balanced dataset\n",
    "if not balanced_data.empty:\n",
    "    print(\"\\nClass and Temperature counts in the balanced dataset:\")\n",
    "    for temp in balanced_data['target_hotend'].unique():\n",
    "        print(f\"\\nTemperature: {temp}°\")\n",
    "        for class_id in [0, 1, 2]:\n",
    "            count = len(balanced_data[(balanced_data['hotend_class'] == class_id) & (balanced_data['target_hotend'] == temp)])\n",
    "            print(f\"Class {class_id}: {count} images\")\n",
    "else:\n",
    "    print(\"Balanced dataset is empty.\")"
   ],
   "id": "455b4c712af60cc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing temperature: 180.0°\n",
      "Class 0 at 180.0° actual dataset size: 618\n",
      "Class 1 at 180.0° actual dataset size: 53\n",
      "Class 2 at 180.0° actual dataset size: 5\n",
      "Class 2 at 180.0° does not have enough images (5). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 181.0°\n",
      "Class 0 at 181.0° actual dataset size: 393\n",
      "Class 1 at 181.0° actual dataset size: 12\n",
      "Class 1 at 181.0° does not have enough images (12). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 182.0°\n",
      "Class 0 at 182.0° actual dataset size: 243\n",
      "Class 1 at 182.0° actual dataset size: 27\n",
      "Class 2 at 182.0° actual dataset size: 0\n",
      "Class 2 at 182.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 183.0°\n",
      "Class 0 at 183.0° actual dataset size: 262\n",
      "Class 1 at 183.0° actual dataset size: 8\n",
      "Class 1 at 183.0° does not have enough images (8). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 184.0°\n",
      "Class 0 at 184.0° actual dataset size: 272\n",
      "Class 1 at 184.0° actual dataset size: 0\n",
      "Class 1 at 184.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 186.0°\n",
      "Class 0 at 186.0° actual dataset size: 385\n",
      "Class 1 at 186.0° actual dataset size: 21\n",
      "Class 2 at 186.0° actual dataset size: 0\n",
      "Class 2 at 186.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 187.0°\n",
      "Class 0 at 187.0° actual dataset size: 110\n",
      "Class 1 at 187.0° actual dataset size: 26\n",
      "Class 2 at 187.0° actual dataset size: 0\n",
      "Class 2 at 187.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 188.0°\n",
      "Class 0 at 188.0° actual dataset size: 241\n",
      "Class 1 at 188.0° actual dataset size: 30\n",
      "Class 2 at 188.0° actual dataset size: 0\n",
      "Class 2 at 188.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 189.0°\n",
      "Class 0 at 189.0° actual dataset size: 238\n",
      "Class 1 at 189.0° actual dataset size: 34\n",
      "Class 2 at 189.0° actual dataset size: 0\n",
      "Class 2 at 189.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 191.0°\n",
      "Class 0 at 191.0° actual dataset size: 486\n",
      "Class 1 at 191.0° actual dataset size: 55\n",
      "Class 2 at 191.0° actual dataset size: 0\n",
      "Class 2 at 191.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 193.0°\n",
      "Class 0 at 193.0° actual dataset size: 599\n",
      "Class 1 at 193.0° actual dataset size: 92\n",
      "Class 2 at 193.0° actual dataset size: 0\n",
      "Class 2 at 193.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 194.0°\n",
      "Class 0 at 194.0° actual dataset size: 501\n",
      "Class 1 at 194.0° actual dataset size: 40\n",
      "Class 2 at 194.0° actual dataset size: 0\n",
      "Class 2 at 194.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 195.0°\n",
      "Class 0 at 195.0° actual dataset size: 332\n",
      "Class 1 at 195.0° actual dataset size: 201\n",
      "Class 2 at 195.0° actual dataset size: 8\n",
      "Class 2 at 195.0° does not have enough images (8). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 196.0°\n",
      "Class 0 at 196.0° actual dataset size: 21\n",
      "Class 1 at 196.0° actual dataset size: 115\n",
      "Class 2 at 196.0° actual dataset size: 0\n",
      "Class 2 at 196.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 197.0°\n",
      "Class 0 at 197.0° actual dataset size: 85\n",
      "Class 1 at 197.0° actual dataset size: 182\n",
      "Class 2 at 197.0° actual dataset size: 3\n",
      "Class 2 at 197.0° does not have enough images (3). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 198.0°\n",
      "Class 0 at 198.0° actual dataset size: 92\n",
      "Class 1 at 198.0° actual dataset size: 179\n",
      "Class 2 at 198.0° actual dataset size: 0\n",
      "Class 2 at 198.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 199.0°\n",
      "Class 0 at 199.0° actual dataset size: 170\n",
      "Class 1 at 199.0° actual dataset size: 643\n",
      "Class 2 at 199.0° actual dataset size: 0\n",
      "Class 2 at 199.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 201.0°\n",
      "Class 0 at 201.0° actual dataset size: 56\n",
      "Class 1 at 201.0° actual dataset size: 485\n",
      "Class 2 at 201.0° actual dataset size: 0\n",
      "Class 2 at 201.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 202.0°\n",
      "Class 0 at 202.0° actual dataset size: 64\n",
      "Class 1 at 202.0° actual dataset size: 207\n",
      "Class 2 at 202.0° actual dataset size: 0\n",
      "Class 2 at 202.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 203.0°\n",
      "Class 0 at 203.0° actual dataset size: 7\n",
      "Class 0 at 203.0° does not have enough images (7). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 205.0°\n",
      "Class 0 at 205.0° actual dataset size: 25\n",
      "Class 1 at 205.0° actual dataset size: 554\n",
      "Class 2 at 205.0° actual dataset size: 8\n",
      "Class 2 at 205.0° does not have enough images (8). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 208.0°\n",
      "Class 0 at 208.0° actual dataset size: 28\n",
      "Class 1 at 208.0° actual dataset size: 512\n",
      "Class 2 at 208.0° actual dataset size: 2\n",
      "Class 2 at 208.0° does not have enough images (2). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 209.0°\n",
      "Class 0 at 209.0° actual dataset size: 1\n",
      "Class 0 at 209.0° does not have enough images (1). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 211.0°\n",
      "Class 0 at 211.0° actual dataset size: 0\n",
      "Class 0 at 211.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 212.0°\n",
      "Class 0 at 212.0° actual dataset size: 8\n",
      "Class 0 at 212.0° does not have enough images (8). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 213.0°\n",
      "Class 0 at 213.0° actual dataset size: 27\n",
      "Class 1 at 213.0° actual dataset size: 244\n",
      "Class 2 at 213.0° actual dataset size: 0\n",
      "Class 2 at 213.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 214.0°\n",
      "Class 0 at 214.0° actual dataset size: 9\n",
      "Class 0 at 214.0° does not have enough images (9). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 215.0°\n",
      "Class 0 at 215.0° actual dataset size: 0\n",
      "Class 0 at 215.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 216.0°\n",
      "Class 0 at 216.0° actual dataset size: 0\n",
      "Class 0 at 216.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 217.0°\n",
      "Class 0 at 217.0° actual dataset size: 9\n",
      "Class 0 at 217.0° does not have enough images (9). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 218.0°\n",
      "Class 0 at 218.0° actual dataset size: 72\n",
      "Class 1 at 218.0° actual dataset size: 605\n",
      "Class 2 at 218.0° actual dataset size: 0\n",
      "Class 2 at 218.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 219.0°\n",
      "Class 0 at 219.0° actual dataset size: 30\n",
      "Class 1 at 219.0° actual dataset size: 105\n",
      "Class 2 at 219.0° actual dataset size: 0\n",
      "Class 2 at 219.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 220.0°\n",
      "Class 0 at 220.0° actual dataset size: 9\n",
      "Class 0 at 220.0° does not have enough images (9). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 221.0°\n",
      "Class 0 at 221.0° actual dataset size: 11\n",
      "Class 0 at 221.0° does not have enough images (11). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 222.0°\n",
      "Class 0 at 222.0° actual dataset size: 19\n",
      "Class 1 at 222.0° actual dataset size: 161\n",
      "Class 2 at 222.0° actual dataset size: 225\n",
      "Temperature 222.0° included with 16 images per class.\n",
      "\n",
      "Processing temperature: 223.0°\n",
      "Class 0 at 223.0° actual dataset size: 0\n",
      "Class 0 at 223.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 224.0°\n",
      "Class 0 at 224.0° actual dataset size: 0\n",
      "Class 0 at 224.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 226.0°\n",
      "Class 0 at 226.0° actual dataset size: 18\n",
      "Class 1 at 226.0° actual dataset size: 140\n",
      "Class 2 at 226.0° actual dataset size: 226\n",
      "Temperature 226.0° included with 16 images per class.\n",
      "\n",
      "Processing temperature: 227.0°\n",
      "Class 0 at 227.0° actual dataset size: 0\n",
      "Class 0 at 227.0° does not have enough images (0). Skipping this temperature.\n",
      "\n",
      "Processing temperature: 228.0°\n",
      "Class 0 at 228.0° actual dataset size: 27\n",
      "Class 1 at 228.0° actual dataset size: 120\n",
      "Class 2 at 228.0° actual dataset size: 123\n",
      "Temperature 228.0° included with 16 images per class.\n",
      "\n",
      "Processing temperature: 229.0°\n",
      "Class 0 at 229.0° actual dataset size: 65\n",
      "Class 1 at 229.0° actual dataset size: 116\n",
      "Class 2 at 229.0° actual dataset size: 89\n",
      "Temperature 229.0° included with 16 images per class.\n",
      "\n",
      "Processing temperature: 230.0°\n",
      "Class 0 at 230.0° actual dataset size: 16\n",
      "Class 1 at 230.0° actual dataset size: 137\n",
      "Class 2 at 230.0° actual dataset size: 252\n",
      "Temperature 230.0° included with 16 images per class.\n",
      "\n",
      "Total number of images in the balanced dataset: 240\n",
      "\n",
      "Class and Temperature counts in the balanced dataset:\n",
      "\n",
      "Temperature: 222.0°\n",
      "Class 0: 16 images\n",
      "Class 1: 16 images\n",
      "Class 2: 16 images\n",
      "\n",
      "Temperature: 226.0°\n",
      "Class 0: 16 images\n",
      "Class 1: 16 images\n",
      "Class 2: 16 images\n",
      "\n",
      "Temperature: 228.0°\n",
      "Class 0: 16 images\n",
      "Class 1: 16 images\n",
      "Class 2: 16 images\n",
      "\n",
      "Temperature: 229.0°\n",
      "Class 0: 16 images\n",
      "Class 1: 16 images\n",
      "Class 2: 16 images\n",
      "\n",
      "Temperature: 230.0°\n",
      "Class 0: 16 images\n",
      "Class 1: 16 images\n",
      "Class 2: 16 images\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Converting balanced_dataset into a dataframe that contains only the img_path and hotend_class",
   "id": "7211a40d7515bee6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now select only the columns you want\n",
    "balanced_data = balanced_data[['img_path', 'hotend_class']]\n",
    "\n",
    "# Check the modified dataset\n",
    "print(balanced_data)"
   ],
   "id": "f8be5db355cffacd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AT THIS POINT THE BALANCED DATASET IS CREATED",
   "id": "b73d376bd78e50cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create training, validation, and testing datasets",
   "id": "a9d13161d1fe7599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class BalancedDataset(Dataset):\n",
    "#     def __init__(self, data_frame, root_dir, transform=None):\n",
    "#         self.data = data_frame\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform or transforms.Compose([transforms.Resize((224, 224)),\n",
    "#                                                           transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "#                 \n",
    "#         # Validate that the images exist in the directory\n",
    "#         self.valid_indices = self.get_valid_indices()\n",
    "# \n",
    "#     def get_valid_indices(self):\n",
    "#         valid_indices = []\n",
    "#         for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "#             img_name = self.data.iloc[idx, 0].strip()\n",
    "#             img_name = img_name.split('/')[-1]  # Extract file name\n",
    "#             \n",
    "#             if img_name.startswith(\"image-\"):\n",
    "#                 try:\n",
    "#                     # Ensure we only include images in print24\n",
    "#                     image_number = int(img_name.split('-')[1].split('.')[0])\n",
    "#                     if 4 <= image_number <= 26637:\n",
    "#                         full_img_path = os.path.join(self.root_dir, img_name)\n",
    "#                         if os.path.exists(full_img_path):\n",
    "#                             valid_indices.append(idx)\n",
    "#                         else:\n",
    "#                             print(f\"Image does not exist: {full_img_path}\")\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Invalid filename format for {img_name}. Skipping...\")\n",
    "#         \n",
    "#         print(f\"Total valid indices found: {len(valid_indices)}\")  # Debugging output\n",
    "#         return valid_indices\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return len(self.valid_indices)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Get the actual index from valid indices\n",
    "#         actual_idx = self.valid_indices[idx]\n",
    "#         img_name = self.data.iloc[actual_idx, 0].strip()\n",
    "#         full_img_path = os.path.join(self.root_dir, img_name)\n",
    "#         \n",
    "#         try:\n",
    "#             image = Image.open(full_img_path).convert('RGB')  # Ensure image is RGB\n",
    "#             label_str = self.data.iloc[actual_idx]['hotend_class']  # Use column name 'hotend_class'\n",
    "#             label = int(label_str)  # Convert label to integer (ensure it's valid)\n",
    "#             \n",
    "#             # Apply transformation if any\n",
    "#             image = self.transform(image)  # Apply transformation\n",
    "#     \n",
    "#             return image, label, actual_idx\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading image {full_img_path}: {e}\")\n",
    "#             return None  # Handle error gracefully\n",
    "# \n",
    "# # Assuming the data and labels are already prepared in the dataframe\n",
    "# # balanced_data should be a pandas DataFrame that contains image paths and labels\n",
    "# dataset = BalancedDataset(balanced_data, ROOT_DIR_PATH)\n",
    "# \n",
    "# # Step 3: Stratified Split (Ensure each class is represented proportionally in the splits)\n",
    "# labels = dataset.data['hotend_class'].values\n",
    "# \n",
    "# # Get the number of samples for each class\n",
    "# class_counts = np.bincount(labels)\n",
    "# \n",
    "# # Calculate the number of samples to allocate for each class in each split (train, val, test)\n",
    "# def calculate_split_indices(class_counts, split_ratios):\n",
    "#     train_indices = []\n",
    "#     val_indices = []\n",
    "#     test_indices = []\n",
    "# \n",
    "#     # For each class, calculate how many samples go into each split\n",
    "#     for class_label, count in enumerate(class_counts):\n",
    "#         # Calculate how many samples per split for this class\n",
    "#         num_train = int(count * split_ratios[0])\n",
    "#         num_val = int(count * split_ratios[1])\n",
    "#         num_test = count - num_train - num_val  # The remaining samples go to test\n",
    "#         \n",
    "#         # Get the indices for the class\n",
    "#         class_indices = np.where(labels == class_label)[0]\n",
    "#         \n",
    "#         # Shuffle indices for randomness\n",
    "#         np.random.shuffle(class_indices)\n",
    "#         \n",
    "#         # Split the indices based on the calculated numbers\n",
    "#         train_indices.extend(class_indices[:num_train])\n",
    "#         val_indices.extend(class_indices[num_train:num_train+num_val])\n",
    "#         test_indices.extend(class_indices[num_train+num_val:])\n",
    "#     \n",
    "#     return train_indices, val_indices, test_indices\n",
    "# \n",
    "# # Define the split ratios (train, val, test)\n",
    "# split_ratios = (0.8, 0.1, 0.1)  # 70% training, 20% validation, 10% testing\n",
    "# train_indices, val_indices, test_indices = calculate_split_indices(class_counts, split_ratios)\n",
    "# \n",
    "# # Print sizes to confirm the split\n",
    "# print(f\"Training set size: {len(train_indices)}\")\n",
    "# print(f\"Validation set size: {len(val_indices)}\")\n",
    "# print(f\"Test set size: {len(test_indices)}\")\n",
    "# \n",
    "# # Step 4: Create DataLoaders with SubsetRandomSampler\n",
    "# batch_size = 33  # Adjust batch size as needed\n",
    "# \n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "# val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_indices))\n",
    "# test_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices))\n",
    "# \n",
    "# # Optionally: You can also print the class distribution in each set\n",
    "# def print_class_distribution(loader, name):\n",
    "#     class_counts = np.zeros(3)  # Assuming 3 classes, adjust for your number of classes\n",
    "#     for _, labels, _ in loader:\n",
    "#         for label in labels:\n",
    "#             class_counts[label] += 1\n",
    "#     print(f\"{name} Class Distribution: {class_counts}\")\n",
    "# \n",
    "# print_class_distribution(train_loader, \"Training\")\n",
    "# print_class_distribution(val_loader, \"Validation\")\n",
    "# print_class_distribution(test_loader, \"Test\")\n",
    "# \n",
    "# # Optionally: You can print details of the indices in the DataLoader as follows:\n",
    "# def print_loader_info(loader, name):\n",
    "#     print(f\"\\n{name} set:\")\n",
    "#     for images, labels, indices in loader:\n",
    "#         for img, label, idx in zip(images, labels, indices):\n",
    "#             print(f\"Index: {idx}, Label: {label}\")\n",
    "# \n",
    "# print_loader_info(train_loader, \"Train\")\n",
    "# print_loader_info(val_loader, \"Validation\")\n",
    "# print_loader_info(test_loader, \"Test\")"
   ],
   "id": "f6ff87496c6d75a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a worker function for deterministic DataLoader behavior\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(seed + worker_id)\n",
    "    random.seed(seed + worker_id)\n",
    "\n",
    "# Define the BalancedDataset class\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None):\n",
    "        self.data = data_frame\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Validate that the images exist in the directory\n",
    "        self.valid_indices = self.get_valid_indices()\n",
    "\n",
    "    def get_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "            img_name = self.data.iloc[idx, 0].strip()\n",
    "            img_name = img_name.split('/')[-1]  # Extract file name\n",
    "            \n",
    "            if img_name.startswith(\"image-\"):\n",
    "                try:\n",
    "                    # Ensure we only include images in the valid range\n",
    "                    image_number = int(img_name.split('-')[1].split('.')[0])\n",
    "                    if 4 <= image_number <= 26637:\n",
    "                        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "                        if os.path.exists(full_img_path):\n",
    "                            valid_indices.append(idx)\n",
    "                        else:\n",
    "                            print(f\"Image does not exist: {full_img_path}\")\n",
    "                except ValueError:\n",
    "                    print(f\"Invalid filename format for {img_name}. Skipping...\")\n",
    "        \n",
    "        print(f\"Total valid indices found: {len(valid_indices)}\")  # Debugging output\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the actual index from valid indices\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        img_name = self.data.iloc[actual_idx, 0].strip()\n",
    "        full_img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')  # Ensure image is RGB\n",
    "            label_str = self.data.iloc[actual_idx]['hotend_class']  # Use column name 'hotend_class'\n",
    "            label = int(label_str)  # Convert label to integer (ensure it's valid)\n",
    "            \n",
    "            # Apply transformation if any\n",
    "            image = self.transform(image)  # Apply transformation\n",
    "    \n",
    "            return image, label, actual_idx\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {full_img_path}: {e}\")\n",
    "            return None  # Handle error gracefully\n",
    "\n",
    "# Assuming the data and labels are already prepared in the dataframe\n",
    "# balanced_data should be a pandas DataFrame that contains image paths and labels\n",
    "dataset = BalancedDataset(balanced_data, ROOT_DIR_PATH)\n",
    "\n",
    "# Stratified Split to ensure proportional representation in splits\n",
    "labels = dataset.data['hotend_class'].values\n",
    "\n",
    "# Calculate the number of samples for each class in each split (train, val, test)\n",
    "def calculate_split_indices(class_counts, split_ratios, seed=42):\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Set the seed for consistent behavior\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # For each class, calculate how many samples go into each split\n",
    "    for class_label, count in enumerate(class_counts):\n",
    "        num_train = int(count * split_ratios[0])\n",
    "        num_val = int(count * split_ratios[1])\n",
    "        num_test = count - num_train - num_val  # The remaining samples go to test\n",
    "        \n",
    "        # Get the indices for the class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "        \n",
    "        # Shuffle indices for randomness\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Split the indices based on the calculated numbers\n",
    "        train_indices.extend(class_indices[:num_train])\n",
    "        val_indices.extend(class_indices[num_train:num_train+num_val])\n",
    "        test_indices.extend(class_indices[num_train+num_val:])\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "# Define the split ratios (train, val, test)\n",
    "split_ratios = (0.8, 0.1, 0.1)  # 80% training, 10% validation, 10% testing\n",
    "class_counts = np.bincount(labels)  # Get the count of each class in the labels\n",
    "train_indices, val_indices, test_indices = calculate_split_indices(class_counts, split_ratios, seed=seed)\n",
    "\n",
    "# Print sizes to confirm the split\n",
    "print(f\"Training set size: {len(train_indices)}\")\n",
    "print(f\"Validation set size: {len(val_indices)}\")\n",
    "print(f\"Test set size: {len(test_indices)}\")\n",
    "\n",
    "# Step 4: Create DataLoaders with SubsetRandomSampler, applying worker_init_fn=seed_worker for reproducibility\n",
    "batch_size = 15  # Adjust batch size as needed\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices), worker_init_fn=seed_worker)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_indices), worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices), worker_init_fn=seed_worker)\n",
    "\n",
    "# Modify the function to accept num_classes as an argument\n",
    "def print_class_distribution(loader, name, num_classes):\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    for _, labels, _ in loader:\n",
    "        for label in labels:\n",
    "            class_counts[label] += 1\n",
    "    print(f\"{name} Class Distribution: {class_counts}\")\n",
    "\n",
    "# Get the number of unique classes in the dataset\n",
    "num_classes = len(np.unique(labels))  # Calculate it once for the dataset\n",
    "\n",
    "# Call the function with the number of classes as an additional argument\n",
    "print_class_distribution(train_loader, \"Training\", num_classes)\n",
    "print_class_distribution(val_loader, \"Validation\", num_classes)\n",
    "print_class_distribution(test_loader, \"Test\", num_classes)\n",
    "\n",
    "\n",
    "# Optionally: Print details of the indices in each DataLoader\n",
    "def print_loader_info(loader, name):\n",
    "    print(f\"\\n{name} set:\")\n",
    "    for images, labels, indices in loader:\n",
    "        for img, label, idx in zip(images, labels, indices):\n",
    "            print(f\"Index: {idx}, Label: {label}\")\n",
    "\n",
    "print_loader_info(train_loader, \"Train\")\n",
    "print_loader_info(val_loader, \"Validation\")\n",
    "print_loader_info(test_loader, \"Test\")"
   ],
   "id": "e4d04ad55d89d1d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AT THIS POINT I AM READY TO BEGIN TRAINING MY MODEL",
   "id": "62020c8940624217"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to device\n",
    "model = SimpleCNN(num_classes=3).to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100 # Adjust as needed\n",
    "class_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)  # Update these based on your class distribution\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)  # Adjust learning rate if needed\n",
    "# **Add the learning rate scheduler here**\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Decrease LR every 10 epochs by a factor of 0.1\n",
    "\n",
    "best_val_accuracy = 0.0  # Track the best validation accuracy to save the best model\n",
    "\n",
    "# Store losses for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    class_counts = [0] * 3  # Assuming 3 classes, update if needed\n",
    "\n",
    "    # Training phase with tqdm progress bar\n",
    "    for images, labels, _ in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training loss and accuracy\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Update class counts\n",
    "        for label in labels:\n",
    "            class_counts[label.item()] += 1\n",
    "        \n",
    "        # Print predicted vs actual labels for each batch\n",
    "        for i in range(len(labels)):\n",
    "            print(f\"Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}\")\n",
    "    \n",
    "    # Print class distribution during training\n",
    "    print(f\"Training Class Distribution: {class_counts}\")\n",
    "    \n",
    "    # **Call the scheduler here at the end of each epoch to update the learning rate**\n",
    "    scheduler.step()\n",
    "\n",
    "    # Store training loss for plotting\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase with tqdm progress bar\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    val_class_counts = [0] * 3  # Assuming 3 classes, update if needed\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for images, labels, _ in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Track validation loss and accuracy\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_samples += labels.size(0)\n",
    "\n",
    "            # Update class counts for validation\n",
    "            for label in labels:\n",
    "                val_class_counts[label.item()] += 1\n",
    "\n",
    "            # Print predicted vs actual labels for each batch\n",
    "            for i in range(len(labels)):\n",
    "                print(f\"Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "\n",
    "    val_epoch_loss = val_loss / val_total_samples\n",
    "    val_epoch_accuracy = val_correct_predictions / val_total_samples\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}\")\n",
    "    \n",
    "    # Print class distribution during validation\n",
    "    print(f\"Validation Class Distribution: {val_class_counts}\")\n",
    "\n",
    "    # Store validation loss for plotting\n",
    "    val_losses.append(val_epoch_loss)\n",
    "\n",
    "    # Save the model if it achieves better validation accuracy\n",
    "    if val_epoch_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_epoch_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        print(\"Saved the model with improved validation accuracy.\")\n",
    "\n",
    "# End of training loop\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Plotting the training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Test model function with tqdm progress bar\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    test_class_counts = [0] * 3  # Assuming 3 classes, update if needed\n",
    "    with torch.no_grad():  # Disable gradients for testing\n",
    "        for images, labels, _ in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Update class counts for testing\n",
    "            for label in labels:\n",
    "                test_class_counts[label.item()] += 1\n",
    "\n",
    "            # Print predicted vs actual labels for each batch\n",
    "            for i in range(len(labels)):\n",
    "                print(f\"Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "\n",
    "    avg_accuracy = correct_predictions / total_samples\n",
    "    print(f\"Test Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Print class distribution during testing\n",
    "    print(f\"Test Class Distribution: {test_class_counts}\")\n",
    "\n",
    "# Run the test phase after training\n",
    "test_model(model, test_loader)"
   ],
   "id": "f837d05c28c4299d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, csv_file=None, root_dir=None, transform=None, data_frame=None):\n",
    "#         if data_frame is not None:\n",
    "#             self.data = data_frame\n",
    "#         elif csv_file is not None:\n",
    "#             self.data = pd.read_csv(csv_file, header=0, dtype=str)\n",
    "#         else:\n",
    "#             raise ValueError(\"Either csv_file or data_frame must be provided.\")\n",
    "# \n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform or self.default_transform()\n",
    "#         self.valid_indices = self.get_valid_indices()\n",
    "# \n",
    "#     def default_transform(self):\n",
    "#         return transforms.Compose([\n",
    "#             transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),\n",
    "#         ])\n",
    "# \n",
    "#     def get_valid_indices(self):\n",
    "#         valid_indices = []\n",
    "#         for idx in tqdm(range(len(self.data)), desc=\"Validating images\"):\n",
    "#             img_name = self.data.iloc[idx, 0].strip()\n",
    "#             img_name = img_name.split('/')[-1]\n",
    "#         \n",
    "#             if img_name.startswith(\"image-\"):\n",
    "#                 try:\n",
    "#                     image_number = int(img_name.split('-')[1].split('.')[0])\n",
    "#                     if image_number <= 3084:\n",
    "#                         full_img_path = os.path.join(self.root_dir, img_name)\n",
    "#                         if os.path.exists(full_img_path):\n",
    "#                             valid_indices.append(idx)\n",
    "#                             label = self.data.iloc[idx, 15]  # Assuming label is in the 15th column\n",
    "#                             print(f\"Valid image: {img_name}, Label: {label}\")\n",
    "#                         else:\n",
    "#                             print(f\"Image does not exist: {full_img_path}\")\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Invalid filename format for {img_name}. Skipping...\")\n",
    "#         \n",
    "#         print(f\"Total valid indices found: {len(valid_indices)}\")  # Debugging output\n",
    "#         return valid_indices\n",
    "# \n",
    "# \n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return len(self.valid_indices)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         if isinstance(idx, list):\n",
    "#             items = [self._load_sample(i) for i in idx if self._load_sample(i) is not None]\n",
    "#             if not items:\n",
    "#                 raise RuntimeError(\"No valid items found in the batch.\")\n",
    "#             images, labels = zip(*items)\n",
    "#             return torch.stack(images), torch.tensor(labels)\n",
    "#         else:\n",
    "#             return self._load_sample(idx)\n",
    "# \n",
    "# \n",
    "#     def _load_sample(self, idx):\n",
    "#         # Get the actual index from valid indices\n",
    "#         actual_idx = self.valid_indices[idx]\n",
    "#         img_name = self.data.iloc[actual_idx, 0].strip()\n",
    "#         full_img_path = os.path.join(self.root_dir, img_name)\n",
    "#     \n",
    "#         try:\n",
    "#             image = Image.open(full_img_path).convert('RGB')  # Ensure image is RGB\n",
    "#             label_str = self.data.iloc[actual_idx, 15]  # Assuming label is in the 15th column\n",
    "#             \n",
    "#             # Attempt to convert label to integer; handle exceptions\n",
    "#             try:\n",
    "#                 label = int(label_str)  # Try converting to int\n",
    "#             except ValueError:\n",
    "#                 print(f\"Warning: Non-integer label found for image {img_name}: {label_str}\")\n",
    "#                 print()\n",
    "#                 return None  # Skip this sample if label conversion fails\n",
    "#     \n",
    "#             # Print image and label info when loading a sample\n",
    "#             print(f\"Loading sample: {img_name}, Label: {label}\")\n",
    "#     \n",
    "#             image = self.transform(image)  # Apply transformation\n",
    "#     \n",
    "#             return image, label\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading image {full_img_path}: {e}\")\n",
    "#             return None  # Handle error gracefully"
   ],
   "id": "6e407596bd51a9e5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
